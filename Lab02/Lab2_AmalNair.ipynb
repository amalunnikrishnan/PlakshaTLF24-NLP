{"cells":[{"cell_type":"markdown","id":"19c247ef","metadata":{"deletable":false,"editable":false,"id":"19c247ef","nbgrader":{"cell_type":"markdown","checksum":"a55d3ed06eee4d3a7da8b7d095623f91","grade":false,"grade_id":"cell-23a7f8d3de35de0b","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Lab 2: Fine-tuning BERT To Perform Common Sense Reasoning\n","\n","## May 13, 2024\n","\n","Welcome to the Lab 2 of our course on Natural Language Processing. As the name suggests in this lab you will learn how to fine-tune a pretrained model like BERT on a downstream task to improve much more superior performance compared to the methods discussed so far. We will be working with the [SocialIQA](https://arxiv.org/abs/1904.09728) dataset this week, which is a multiple choice classification dataset designed to learn and measure social and emotional intelligence in NLP models.\n","\n","\n","This assignment will also make heavy use of the [ðŸ¤— Transformers Library](https://huggingface.co/docs/transformers/index). Don't worry if you are not familiar with the library, we will discuss its usage in detail.\n","\n","Note: Access to a GPU will be crucial for working on this assignment. So do select a GPU runtime in Colab before you start working.\n","\n","Learning Outcomes from this Lab:\n","- Learn how to use ðŸ¤— Transformer library to load and fine-tune pre-trained langauge models\n","- Learn how to solve common sense reasoning problems using Masked Language Models like BERT\n","\n","Suggested Reading:\n","- [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)\n","- [Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense Reasoning about Social Interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463â€“4473, Hong Kong, China. Association for Computational Linguistics.] (https://arxiv.org/pdf/1810.04805.pdf)"]},{"cell_type":"code","execution_count":1,"id":"26c077e7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26c077e7","outputId":"06d7c380-2548-4d21-b835-ff33250cc3f7","executionInfo":{"status":"ok","timestamp":1716675478643,"user_tz":-330,"elapsed":24678,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","siqa_data_dir = \"gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/\" #renamed basis directory organization on my drive."]},{"cell_type":"code","execution_count":2,"id":"0ah77NB3QsU9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ah77NB3QsU9","outputId":"14a5ef35-51f2-4b87-af70-cd6e3968dbce","executionInfo":{"status":"ok","timestamp":1716675479138,"user_tz":-330,"elapsed":502,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total 8479\n","-rw------- 1 root root  476394 May 13 09:13 dev.jsonl\n","-rw------- 1 root root    5862 May 13 09:13 dev-labels.lst\n","-rw------- 1 root root 8098489 May 13 09:13 train.jsonl\n","-rw------- 1 root root  100230 May 13 09:13 train-labels.lst\n"]}],"source":["!ls -l gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/"]},{"cell_type":"code","execution_count":3,"id":"3242992d","metadata":{"deletable":false,"editable":false,"id":"3242992d","nbgrader":{"cell_type":"code","checksum":"9b746d7065e47851c69e7bd3f33db670","grade":false,"grade_id":"cell-00e4313e5ec6522c","locked":true,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1716675479139,"user_tz":-330,"elapsed":6,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["# If using Colab, NO NEED TO INSTALL ANYTHING\n","# Install required libraries\n","# !pip install numpy\n","# !pip install pandas\n","# !pip install torch\n","# !pip install tqdm\n","# !pip install matplotlib\n","# !pip install transformers\n","# !pip install scikit-learn\n","# !pip install tqdm"]},{"cell_type":"code","execution_count":4,"id":"9e7b0e64","metadata":{"deletable":false,"editable":false,"id":"9e7b0e64","nbgrader":{"cell_type":"code","checksum":"6f6521e81b123c3d00a56e3fca57d658","grade":false,"grade_id":"cell-eb2f90d7f62cad18","locked":true,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1716675484076,"user_tz":-330,"elapsed":4942,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["# We start by importing libraries that we will be making use of in the assignment.\n","import os\n","from functools import partial\n","import json\n","from pprint import pprint\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import copy\n","from tqdm.notebook import tqdm\n","\n","from transformers.utils import logging\n","logging.set_verbosity(40) # to avoid warnings from transformers"]},{"cell_type":"markdown","id":"3c04bc11","metadata":{"id":"3c04bc11"},"source":["## SocialIQA Dataset\n","\n","We start by discussing the dataset that we will making use of in today's Lab. As described above SocialIQA was designed to learn and measure social and emotional intelligence in NLP models. It is a multiple choice classification task, where you are given a context of some social situation, a question about the context and then three possible answers to the questions. The task is to predict which of the three options answers the question given the context.\n","\n","![siqa dataset](https://i.ibb.co/s5tMpY8/siqa.png)\n","\n","Below we load the dataset in memory"]},{"cell_type":"code","execution_count":5,"id":"b3f82e9d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"b3f82e9d","nbgrader":{"cell_type":"code","checksum":"5d3586a8ea681ba874073f19efd8d10d","grade":false,"grade_id":"cell-61ae8095088b1abc","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"21ea2dea-1b52-41e2-d0bc-cfcafcb6c456","executionInfo":{"status":"ok","timestamp":1716675486235,"user_tz":-330,"elapsed":2183,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Training Examples: 33410\n","Number of Validation Examples: 1954\n"]}],"source":["def load_siqa_data(split):\n","\n","    # We first load the file containing context, question and answers\n","    with open(f\"gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/{split}.jsonl\") as f:\n","        data = [json.loads(jline) for jline in f.read().splitlines()]\n","\n","    # We then load the file containing the correct answer for each question\n","    with open(f\"gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/{split}-labels.lst\") as f:\n","        labels = f.read().splitlines()\n","\n","    labels_dict = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\"}\n","    labels = [labels_dict[label] for label in labels]\n","\n","    return data, labels\n","\n","\n","train_data, train_labels = load_siqa_data(\"train\")\n","dev_data, dev_labels = load_siqa_data(\"dev\")\n","\n","print(f\"Number of Training Examples: {len(train_data)}\")\n","print(f\"Number of Validation Examples: {len(dev_data)}\")"]},{"cell_type":"code","execution_count":6,"id":"3dc04307","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"3dc04307","outputId":"15e18863-5a6e-4d9f-b600-751f3cb7d64a","executionInfo":{"status":"ok","timestamp":1716675486780,"user_tz":-330,"elapsed":549,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Axes: ylabel='count'>"]},"metadata":{},"execution_count":6},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["sns.countplot(x = train_labels)"]},{"cell_type":"code","execution_count":7,"id":"adf24a61","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"adf24a61","nbgrader":{"cell_type":"code","checksum":"70e3ecb51abfd2ca15b0b8f5f24de0e7","grade":false,"grade_id":"cell-602fe165abcf4503","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"d2226a83-66f4-42fd-8463-d1d2d8d944f7","executionInfo":{"status":"ok","timestamp":1716675486781,"user_tz":-330,"elapsed":12,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Example from dataset\n","{   'context': \"Jordan's dog peed on the couch they were selling and Jordan \"\n","               'removed the odor as soon as possible.',\n","    'question': 'How would Jordan feel afterwards?',\n","    'answerA': 'selling a couch',\n","    'answerB': 'Disgusted',\n","    'answerC': 'Relieved'}\n","Label: B\n"]}],"source":["# View a sample of the dataset\n","print(\"Example from dataset\")\n","pprint(train_data[100], sort_dicts=False, indent=4)\n","print(f\"Label: {train_labels[100]}\")"]},{"cell_type":"code","execution_count":8,"id":"271a12e6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"271a12e6","outputId":"80ac1273-92b6-4d38-87bb-638dc520bdad","executionInfo":{"status":"ok","timestamp":1716675486781,"user_tz":-330,"elapsed":8,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': 'kendall was a person who kept her word so she got my money the other day.',\n"," 'question': 'What will Others want to do next?',\n"," 'answerA': 'resent kendall',\n"," 'answerB': 'support kendall',\n"," 'answerC': 'hate kendall'}"]},"metadata":{},"execution_count":8}],"source":["train_data[500]"]},{"cell_type":"markdown","id":"99a6b904","metadata":{"deletable":false,"editable":false,"id":"99a6b904","nbgrader":{"cell_type":"markdown","checksum":"e77cbee4bcad7c097c84525407a83d89","grade":false,"grade_id":"cell-23562a8e437add81","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Task 1: Tokenization and Data Preperation (1 hour)"]},{"cell_type":"markdown","id":"0af7f506","metadata":{"deletable":false,"editable":false,"id":"0af7f506","nbgrader":{"cell_type":"markdown","checksum":"9634252b2664ab163c234b45f7cac77f","grade":false,"grade_id":"cell-ee0e1d7bb6a346a4","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["As discussed in the lectures, BERT and other pretrained language models use sub-word tokenization i.e. individual words can also be split into constituent subwords to reduce the vocabulary size. The Transformer library provides tokenizer for all the popular language models. Below we demonstrate how to create and use these tokenizers."]},{"cell_type":"code","execution_count":9,"id":"4e8aa458","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304,"referenced_widgets":["1a763aed81e04a11a24a99f7a6e8b55d","b22a80dbefac4657aca9803024c8a94f","2929637c3dab402cb1d1b0f66ea8e7bc","fb39ab4250cb451d8638193aef35e7a2","cff7ca0d469747a1b4723428bd93fa18","21eec649413b4b38866a196bf5f5a4db","80103da6c6174393aeaf7e6e949cb5d7","09eace6487f24f35bb4daca9ed5c41fa","9ab10cb9c08b4cf7ae6335b960e2c78b","28400d62c4274c3eb7bf5573ed381533","5ceb9aed2fe64767aca207f04cec9620","ca58e55fa96b45c8a98b02133b99a33c","a09991a4389449ac998bfc25c4de0dbb","1bb291abccea43b9844cb16ec80f5726","7de3e6a9737643d48b5f89fbf98b5198","24958d54d0df496b940b1c6bd53385ae","c28a85c393754c39b799dd8f9162d244","477317aedf3345b29f6f47b3f1bb3057","6c0cfa87698f400c974bbe0128d39e7d","a2e14bd2934e4a6e9108df9507ef9c35","a25c01c593f847f89e62666b896e75b8","a38c4ece31844358bc41743a32f0b2cb","be69add35cb741db9d5d1a59ddfd5acc","fc9d024b4a1143778c7e1f0aa65d57f1","8b77293e1e4b4fdc85cdfdb1a047cf09","4fddb337d3934c058c0c4211318eb17f","9660205e7d614a5f872cba5ba5531519","ad3d65e902524f47954daa8f7f6a19b0","da5437b79cbd4d4e928f3e30243f7559","88c6926293164ca9afe1d5d38ba13edf","811af9d2d6b94f6caf441259472b4e05","c08155b450a448a69fd79e39791e5571","9e280f0408ce4eda9a3de2854f4507e8","3d0e0a9d1341482db08155f398f9c861","c5cc22b4b84e4430a31b11b4af8046ad","fb6a51efc60241be88a973d72ef4e57f","8692f10c36d642448767b7f10aab1de2","1bd3e0576a5741fdaee8158f6ff9cfd4","c8b9abe0f6274fff87c2f3b65db3b330","22a51b8b12e045649e97447b388ff9dd","2e84acaa3f3c4be78c6d7673fa5e1802","492b31fb2cb34aaaa1105476a154801e","95430f19bf5b4fe6bf7fdc19a4632321","c785f86062824e30a9239e886ef8b2ab"]},"deletable":false,"editable":false,"id":"4e8aa458","nbgrader":{"cell_type":"code","checksum":"1791efdd95ba6e41368fcf3d0e742e0b","grade":false,"grade_id":"cell-cf4d278c7b6855d7","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"657edbfa-ff80-4e9d-fb86-742f6005aab2","executionInfo":{"status":"ok","timestamp":1716675490973,"user_tz":-330,"elapsed":4197,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a763aed81e04a11a24a99f7a6e8b55d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca58e55fa96b45c8a98b02133b99a33c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be69add35cb741db9d5d1a59ddfd5acc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0e0a9d1341482db08155f398f9c861"}},"metadata":{}}],"source":["# Import the BertTokenizer from the library\n","from transformers import BertTokenizer\n","\n","# Load a pre-trained BERT Tokenizer\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"markdown","id":"30724776","metadata":{"deletable":false,"editable":false,"id":"30724776","nbgrader":{"cell_type":"markdown","checksum":"ec587bee5c51d19bfe0135af7f8514b4","grade":false,"grade_id":"cell-e5d5e6b75df21d75","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["`BertTokenizer.from_pretrained` is used to load a pre-trained tokenizer. Notice that we provide the argument `\"bert-base-uncased\"` to the method. This refers to the variant of BERT that we want to use. The term \"base\" means we want to use the smaller BERT variant i.e. the one with 12 layers, and \"uncased\" refers to the fact that it treats upper-case and lower-case characters identically. There are 4 variants available for BERT which are:\n","    - `bert-base-uncased`\n","    - `bert-base-cased`\n","    - `bert-large-uncased`\n","    - `bert-large-cased`\n","Now that we have loaded the tokenizer, let's see how to use it."]},{"cell_type":"markdown","id":"13523cc5","metadata":{"deletable":false,"editable":false,"id":"13523cc5","nbgrader":{"cell_type":"markdown","checksum":"4cce224a12854b52864b4a47c8cf6afc","grade":false,"grade_id":"cell-64c955b108dc7281","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["`tokenize` method can be used to split the text into sequence of tokens"]},{"cell_type":"code","execution_count":10,"id":"5ccbad09","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"5ccbad09","nbgrader":{"cell_type":"code","checksum":"b3bbc88079b2d0531a7be74670e9edb6","grade":false,"grade_id":"cell-d232c469d8bd9d8b","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"0c44a2a0-2c73-48ee-e308-d85cb36febfa","executionInfo":{"status":"ok","timestamp":1716675490974,"user_tz":-330,"elapsed":23,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['kendall',\n"," 'was',\n"," 'a',\n"," 'person',\n"," 'who',\n"," 'kept',\n"," 'her',\n"," 'word',\n"," 'exquisite',\n"," '##ly',\n"," ',',\n"," 'so',\n"," 'she',\n"," 'got',\n"," 'my',\n"," 'money',\n"," 'the',\n"," 'other',\n"," 'day']"]},"metadata":{},"execution_count":10}],"source":["bert_tokenizer.tokenize(\"kendall was a person who kept her word exquisitely, so she got my money the other day\")"]},{"cell_type":"markdown","id":"daced828","metadata":{"deletable":false,"editable":false,"id":"daced828","nbgrader":{"cell_type":"markdown","checksum":"0b2c517ab6b6cf3d2e479b89ce221764","grade":false,"grade_id":"cell-20733e8cd4cc9db8","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Notice how the tokenizer not only splits the text into words but also subwords like \"exquisitely\" is split into \"exquisite\" and \"ly\"."]},{"cell_type":"markdown","id":"d7260263","metadata":{"deletable":false,"editable":false,"id":"d7260263","nbgrader":{"cell_type":"markdown","checksum":"e24402cff8fc0e8a3dcfda8fe8eead78","grade":false,"grade_id":"cell-c2120e5f1d50ce3e","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Another use case of the tokenizer is to convert the tokens into indices. This is important because BERT and almost all language models takes as the inputs a sequence of token ids, which they use to map into embeddings. `convert_tokens_to_ids` method can be used to do this"]},{"cell_type":"code","execution_count":11,"id":"dbee421b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"dbee421b","nbgrader":{"cell_type":"code","checksum":"7d55893b443134e6f92a1f6620c0f751","grade":false,"grade_id":"cell-3a05f425316cea54","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"e547acf7-7bda-48d7-dcca-5275ac2b0316","executionInfo":{"status":"ok","timestamp":1716675490974,"user_tz":-330,"elapsed":20,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[14509, 2001, 1037, 2711, 2040, 2921, 2014, 2773, 19401, 2135, 1010, 2061, 2016, 2288, 2026, 2769, 1996, 2060, 2154]\n"]}],"source":["sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n","tokens = bert_tokenizer.tokenize(sentence)\n","token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n","print(token_ids)"]},{"cell_type":"markdown","id":"efd54a05","metadata":{"deletable":false,"editable":false,"id":"efd54a05","nbgrader":{"cell_type":"markdown","checksum":"1e4e23bc6201fae04c7d4fd982288e98","grade":false,"grade_id":"cell-4ee63c56c4ce3e10","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["The two steps can also be combined by simply calling the tokenizer object"]},{"cell_type":"code","execution_count":12,"id":"bfcbb696","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"bfcbb696","nbgrader":{"cell_type":"code","checksum":"f1887b57da3c78d52ecf03ce6e405a29","grade":false,"grade_id":"cell-0e587013451e87df","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"7725199b-0d2a-427e-9f17-220a2455b959","executionInfo":{"status":"ok","timestamp":1716675490974,"user_tz":-330,"elapsed":16,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{   'input_ids': [   101,\n","                     14509,\n","                     2001,\n","                     1037,\n","                     2711,\n","                     2040,\n","                     2921,\n","                     2014,\n","                     2773,\n","                     19401,\n","                     2135,\n","                     1010,\n","                     2061,\n","                     2016,\n","                     2288,\n","                     2026,\n","                     2769,\n","                     1996,\n","                     2060,\n","                     2154,\n","                     102],\n","    'token_type_ids': [   0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0],\n","    'attention_mask': [   1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1]}\n"]}],"source":["pprint(bert_tokenizer(sentence), sort_dicts=False, indent=4)"]},{"cell_type":"markdown","id":"2ab48ddf","metadata":{"deletable":false,"editable":false,"id":"2ab48ddf","nbgrader":{"cell_type":"markdown","checksum":"7b1a833f0f503757099493ad4e6bad0f","grade":false,"grade_id":"cell-19c0f71b49a7786a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Notice that it returns a bunch of things in addition to the ids. The `\"input_ids\"` are just the token ids that we obtained in the previous cell. However you will notice that it has a few additional ids, it starts with 101 and ends with 102. These are what we call special tokens and correspond the \\[CLS\\] and \\[SEP\\] tokens used by BERT. \\[CLS\\] token is mainly added to beginning of each sequence, and its representations are used to perform sequence classification. More on \\[SEP\\] token later.\n","\n","`\"token_type_ids\"` contains which sequence does a particular token belongs to.\n","\n","`\"attention_mask`\" is a mask vector that indicates if a particular token corresponds to padding. Padding is extremely important when we are dealing with variable length sequences, which is almost always the case. Through padding we can ensure that all the sequences in a batch are of same size. However, while processing the sequence we need ignore these padding tokens, hence a mask is required to identify such tokens.\n","\n","We can tokenize a batch of sequences by just providing a list instead of a string while calling the tokenizer and later pad them using the `.pad` method."]},{"cell_type":"code","execution_count":13,"id":"1d2c0925","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"1d2c0925","nbgrader":{"cell_type":"code","checksum":"d4a48370450acebae8f9a4b7953eff16","grade":false,"grade_id":"cell-14fe0d9b9608e600","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"571eebb0-8205-4ef7-9989-abf2b495d486","executionInfo":{"status":"ok","timestamp":1716675490974,"user_tz":-330,"elapsed":14,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Input Ids shape: torch.Size([4, 23])\n","Attention Mask shape: torch.Size([4, 23])\n","('Input Ids:\\n'\n"," ' tensor([[  101,  7232,  2787,  2000,  2031,  1037, 26375,  1998,  5935,  '\n"," '2014,\\n'\n"," '          2814,  2362,  1012,   102,     0,     0,     0,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0],\\n'\n"," '        [  101,  5553,  2734,  2000,  2507,  2041,  5841,  2005,  2019,  '\n"," '9046,\\n'\n"," '          2622,  2012,  2147,  1012,   102,     0,     0,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0],\\n'\n"," '        [  101, 22712,  2001,  2019,  6739, 19949,  1998,  2001,  2006,  '\n"," '1996,\\n'\n"," '          2300,  2007, 11928,  1012, 22712, 17395,  2098, 11928,  1005,  '\n"," '1055,\\n'\n"," '          8103,  1012,   102],\\n'\n"," '        [  101, 18403,  2435,  1037,  8549,  2000, 27970,  1005,  1055,  '\n"," '2365,\\n'\n"," '          2043,  2027,  2020,  3110,  2091,  1012,   102,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0]])\\n')\n","('Attention Mask:\\n'\n"," ' tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, '\n"," '0],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, '\n"," '0],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '\n"," '1],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, '\n"," '0]])\\n')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n"]}],"source":["batch_size = 4\n","sentence_batch = [train_data[i][\"context\"] for i in range(batch_size)]\n","\n","#Tokenize the batch of sequences\n","tokenized_batch = bert_tokenizer(sentence_batch)\n","\n","# Pad the tokenized batch\n","tokenized_batch_padded = bert_tokenizer.pad(tokenized_batch, padding=True, max_length=32, return_tensors=\"pt\")\n","\n","input_ids = tokenized_batch_padded[\"input_ids\"]\n","attn_mask = tokenized_batch_padded[\"attention_mask\"]\n","print(f\"Input Ids shape: {input_ids.shape}\")\n","print(f\"Attention Mask shape: {attn_mask.shape}\")\n","\n","pprint(f\"Input Ids:\\n {input_ids}\\n\")\n","pprint(f\"Attention Mask:\\n {attn_mask}\\n\")"]},{"cell_type":"markdown","id":"fc12c5d0","metadata":{"deletable":false,"editable":false,"id":"fc12c5d0","nbgrader":{"cell_type":"markdown","checksum":"fce3fa2c6deedb93b575d9cb4e28d20f","grade":false,"grade_id":"cell-c42c7d946429adc4","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Notice how 0s get appended to the input ids sequence, and the same is also reflected in the output of `attn_mask` where `0` indicates that the particular token was padded and `1` means otherwise. Setting `return_tensors=\"pt\"` results in the outputs as torch tensors"]},{"cell_type":"markdown","id":"c4e65595","metadata":{"deletable":false,"editable":false,"id":"c4e65595","nbgrader":{"cell_type":"markdown","checksum":"db13d05cd5920e98eeaf5bb761b1075c","grade":false,"grade_id":"cell-673d0a006e2f027a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Finally, for tasks involving reasoning over multiple sentences (like what we have for the SocialIQA dataset), it is common to seperate out each sentence using a \\[SEP\\] token:\n","\n","<img src=\"https://i.ibb.co/Nx8mK1P/bert-sentence-pair.jpg\" alt=\"bert-sentence-pair\" border=\"0\">\n","\n","We can achieve this by adding concatenating all sentences with the `[SEP]` token before calling the tokenizer"]},{"cell_type":"code","execution_count":14,"id":"19f0e7bd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"19f0e7bd","nbgrader":{"cell_type":"code","checksum":"6cd951f70f83b450715fb2af85c023fe","grade":false,"grade_id":"cell-a7fcef5940c3feb8","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"68a4fffb-c8a9-4ef2-c437-cba5bf324ace","executionInfo":{"status":"ok","timestamp":1716675490974,"user_tz":-330,"elapsed":11,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n","{   'input_ids': [   101,\n","                     5207,\n","                     1005,\n","                     1055,\n","                     3899,\n","                     21392,\n","                     2094,\n","                     2006,\n","                     1996,\n","                     6411,\n","                     2027,\n","                     2020,\n","                     4855,\n","                     1998,\n","                     5207,\n","                     3718,\n","                     1996,\n","                     19255,\n","                     2004,\n","                     2574,\n","                     2004,\n","                     2825,\n","                     1012,\n","                     102,\n","                     2129,\n","                     2052,\n","                     5207,\n","                     2514,\n","                     5728,\n","                     1029,\n","                     102,\n","                     4855,\n","                     1037,\n","                     6411,\n","                     102],\n","    'token_type_ids': [   0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0],\n","    'attention_mask': [   1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1]}\n"]}],"source":["example = train_data[100]\n","context = example[\"context\"]\n","question = example[\"question\"]\n","answerA = example[\"answerA\"]\n","\n","# Concatenate the context, question and answerA\n","cqa = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n","print(cqa)\n","\n","tokenized_cqa = bert_tokenizer(cqa)\n","pprint(tokenized_cqa, sort_dicts=False, indent=4)"]},{"cell_type":"markdown","id":"db9af625","metadata":{"deletable":false,"editable":false,"id":"db9af625","nbgrader":{"cell_type":"markdown","checksum":"f52a28ddb4d75b1276a34f6979c44e06","grade":false,"grade_id":"cell-b6df8e66edd0daed","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["For the reasons that will become clear once we work on the modeling part, we need three input tensors for each dataset example, one for concatenating each answer with the context and question."]},{"cell_type":"code","execution_count":15,"id":"4a8bddb8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"4a8bddb8","nbgrader":{"cell_type":"code","checksum":"08e2c414ebaa5ef36cf53cb8bd764c54","grade":false,"grade_id":"cell-01d931a0dfdd5054","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"9bfe52f1-6ede-4de1-c2ab-2bc408b6b2cc","executionInfo":{"status":"ok","timestamp":1716675490974,"user_tz":-330,"elapsed":9,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n","Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Disgusted\n","Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Relieved\n"]}],"source":["example = train_data[100]\n","context = example[\"context\"]\n","question = example[\"question\"]\n","answerA = example[\"answerA\"]\n","\n","answerB = example[\"answerB\"]\n","answerC = example[\"answerC\"]\n","\n","cqaA = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n","cqaB = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerB\n","cqaC = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerC\n","\n","print(cqaA)\n","print(cqaB)\n","print(cqaC)\n","\n","tokenized_cqaA = bert_tokenizer(cqaA)\n","tokenized_cqaB = bert_tokenizer(cqaB)\n","tokenized_cqaC = bert_tokenizer(cqaC)\n"]},{"cell_type":"markdown","id":"4627768e","metadata":{"deletable":false,"editable":false,"id":"4627768e","nbgrader":{"cell_type":"markdown","checksum":"32623222e465ee736771be5decbe6f1c","grade":false,"grade_id":"cell-a3cb841c571f2b47","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Task 1.1: Custom Dataset Class\n","\n","Now that we know how to use the hugging face tokenizers we can define the custom `torch.utils.Dataset` class like we did in the previous assignments to process and store the data as well as provides a way to iterate through the dataset. Implement the `SIQABertDataset` class below. Recall to create a custom class you need to implement 3 methods `__init__`, `__len__` and `__getitem__`."]},{"cell_type":"code","execution_count":16,"id":"65ffd2ac","metadata":{"deletable":false,"id":"65ffd2ac","nbgrader":{"cell_type":"code","checksum":"5b1458f97a435cb685eec31b40452871","grade":false,"grade_id":"cell-84536c665947d263","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1716675490975,"user_tz":-330,"elapsed":7,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class SIQABertDataset(Dataset):\n","\n","    def __init__(self, data, labels, bert_variant = \"bert-base-uncased\"):\n","        \"\"\"\n","        Constructor for the `SST2BertDataset` class. Stores the `sentences` and `labels` which can then be used by\n","        other methods. Also initializes the tokenizer\n","\n","        Inputs:\n","            - data (list) : A list SIQA dataset examples\n","            - labels (list): A list of labels corresponding to each example\n","            - bert_variant (str): A string indicating the variant of BERT to be used.\n","        \"\"\"\n","        self.label2label_id = {\"A\": 0, \"B\": 1, \"C\": 2}\n","        self.data = None\n","        self.labels = None\n","        self.tokenizer = None\n","\n","        # YOUR CODE HERE\n","        self.data = data\n","        self.labels = labels\n","        self.tokenizer = BertTokenizer.from_pretrained(bert_variant)\n","\n","        if (not self.data) or (not self.labels) or (not self.tokenizer):\n","          raise NotImplementedError()\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the length of the dataset\n","        \"\"\"\n","        length = None\n","\n","        # YOUR CODE HERE\n","        length = len(self.data)\n","\n","        if length is None:\n","          raise NotImplementedError()\n","\n","        return length\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns the training example corresponding to review present at the `idx` position in the dataset\n","\n","        Inputs:\n","            - idx (int): Index corresponding to the review,label to be returned\n","\n","        Returns:\n","            - tokenized_input_dict (dict(str, dict)): A dictionary corresponding to tokenizer outputs for the three resulting sequences due to each answer choices as described above\n","            - label (int): Answer label for the corresponding sentence. We will use 0, 1 and 2 to represent A, B and C respectively.\n","\n","        Example Output:\n","            - tokenized_input_dict: {\n","                \"A\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 4855, 1037, 6411, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                \"B\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 17733, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                \"C\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 7653, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","            }\n","            - label: 0\n","\n","        \"\"\"\n","\n","        tokenized_input_dict = {\"A\": None, \"B\": None, \"C\": None}\n","        label = None\n","\n","        # YOUR CODE HERE\n","        example = self.data[idx]\n","        context = example[\"context\"]\n","        question = example[\"question\"]\n","        answerA = example[\"answerA\"]\n","        answerB = example[\"answerB\"]\n","        answerC = example[\"answerC\"]\n","\n","        cqaA = context + self.tokenizer.sep_token + question + self.tokenizer.sep_token + answerA\n","        cqaB = context + self.tokenizer.sep_token + question + self.tokenizer.sep_token + answerB\n","        cqaC = context + self.tokenizer.sep_token + question + self.tokenizer.sep_token + answerC\n","\n","        tokenized_input_dict[\"A\"] = self.tokenizer(cqaA)\n","        tokenized_input_dict[\"B\"] = self.tokenizer(cqaB)\n","        tokenized_input_dict[\"C\"] = self.tokenizer(cqaC)\n","\n","        label = self.label2label_id[self.labels[idx]]\n","\n","        if label is None:\n","          raise NotImplementedError()\n","\n","        return tokenized_input_dict, label"]},{"cell_type":"code","execution_count":17,"id":"7b8c8dea","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":911,"referenced_widgets":["69d0ac9c5eb94ea7947e63dc39a8c54f","c7dccd63d8d8405bb73cabf84b65f26d","b0bf8818ca5a4c71b310938601292856","38aa388a35e7420b95d0e68bfd1e3b19","9b1832e2fd4e472daeb0d41176517c5a","f4761241a06b4c4fb0a0e805829f7c41","a0200107131d4f51b7ef248c4a9cfa61","61fad1d4767349a391db77661d428209","26430408343a4caba7b41f5cfef25c63","8926a955a86b425cb5b8c5a9cfb263b9","a7c5a420acbb480f85b37d2ec60aed95","913c42f3c5c2468692565faabae27b71","30149b85e74342518cead936905e7366","5bd65577d7c14fa8a993dd2b9cd10430","9de85c65e96d47b7bc8e2301f05145d4","391685a804014deb9199145acab28489","6a18cffd954741b9aeac6bfa8a2259c6","6d8a19aa89b54a5f868eabb0069dcfe1","833355d85b634c37827d3e7ee657418f","f861a9410a664965aa5884ee881bb790","0cf0288a4343490883eec3dd66bf411a","070508c9a4014ea0be8d0feb7d5bfeb3","dcd2e55e34ab4fc3ae1b0ee77f5c7911","f3ef9245585342bdaa92a091265c5f9f","8f8aafb261ee4f118a89e6945efc0a81","799e8d462f4845b98109ec16defb7edd","0d10cd3ed40746a08c0abfe0e5430186","095e5730ae384171bf0bbdeaedc4da40","6e8995b4ecab45b6880a3013df5514ad","5a3f1c4419c74b9baa21a600fff4c21f","d26436fff32c451bac3f3f229f55c9f6","aa8c98ceb45f4b7396b08269c9004d00","b437508c19184c3e9492fcabcc98e971","49245a9068a14832b567627565d42bbd","7ca59874797e481883b03f35122855ee","49c59b802eb74626bdab5e4053b002d5","1137839b130c448da8ea12d0132edfda","f0c4edb23a1b4cc3821b7b9c16952f08","b7a77befe9e24135b4cf0910c934129a","49ef551e1b1046d9a630312065a2f0c3","a965f1f0be2f4e109184e5cd20298535","2b80944e9f124db1bdc79d5fb4e76235","a0603735e3f645399a99992987bdc4ed","44f21d24bd784d3db529fd16571b8892"]},"deletable":false,"editable":false,"id":"7b8c8dea","nbgrader":{"cell_type":"code","checksum":"dafc70d40580a2a14cc89778b4a1c88d","grade":true,"grade_id":"cell-0ff4b72642c7bacb","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"outputId":"c7bf7ae0-e395-474e-eb86-90de4eb9608c","executionInfo":{"status":"ok","timestamp":1716675492634,"user_tz":-330,"elapsed":1666,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Cases\n","Sample Test Case 1: Checking if `__len__` is implemented correctly\n","Dataset Length: 2\n","Expected Length: 2\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\n","tokenized_input_dict:\n"," {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 0\n","Expected label:\n"," 0\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\n","tokenized_input_dict:\n"," {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 1\n","Expected label:\n"," 1\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d0ac9c5eb94ea7947e63dc39a8c54f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"913c42f3c5c2468692565faabae27b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd2e55e34ab4fc3ae1b0ee77f5c7911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49245a9068a14832b567627565d42bbd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["tokenized_input_dict:\n"," {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 0\n","Expected label:\n"," 0\n","Sample Test Case Passed!\n","****************************************\n","\n"]}],"source":["print(\"Running Sample Test Cases\")\n","\n","sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-uncased\")\n","\n","print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n","dataset_len= len(sample_dataset)\n","expected_len = 2\n","print(f\"Dataset Length: {dataset_len}\")\n","print(f\"Expected Length: {expected_len}\")\n","assert len(sample_dataset) == expected_len\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n","sample_idx = 0\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict = {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","  'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","  'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","expected_label = 0\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","\n","print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n","sample_idx = 1\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict =  {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                                  'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                                'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","\n","\n","expected_label = 1\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\")\n","sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-cased\")\n","sample_idx = 0\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict = {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n"," 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n"," 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","expected_label = 0\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n"]},{"cell_type":"markdown","id":"8ec2e35e","metadata":{"deletable":false,"editable":false,"id":"8ec2e35e","nbgrader":{"cell_type":"markdown","checksum":"cc5d11b79dc8589bdd3573c800432f66","grade":false,"grade_id":"cell-cd1f8ca8abf3200c","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["We can now create Dataset instances for both training and dev datasets"]},{"cell_type":"code","execution_count":18,"id":"d3994ab0","metadata":{"deletable":false,"editable":false,"id":"d3994ab0","nbgrader":{"cell_type":"code","checksum":"735c155fc2aa10ce309a01ee3633e498","grade":false,"grade_id":"cell-8429b84248f83374","locked":true,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1716675492635,"user_tz":-330,"elapsed":14,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["train_dataset = SIQABertDataset(train_data, train_labels, bert_variant=\"bert-base-uncased\")\n","dev_dataset = SIQABertDataset(dev_data, dev_labels, bert_variant=\"bert-base-uncased\")"]},{"cell_type":"markdown","id":"fdbc4e1e","metadata":{"deletable":false,"editable":false,"id":"fdbc4e1e","nbgrader":{"cell_type":"markdown","checksum":"4be15d36f57f79a7c99c9ac3c6999af7","grade":false,"grade_id":"cell-4c64650b199dc2fc","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Before we instantiate the dataloaders for iterating over the dataset like last time, we need define a collate function, that creates batches from a list of dataset examples. In the last class we didn't have to create one, because all of our examples were of the same size, but that's not the case anymore, and we need to pad the sequences so that they all are of same size. We have implemented the collate_fn for you below, but we recommend going through it step by step, as it is used often in practice."]},{"cell_type":"code","execution_count":19,"id":"6ab6fb0b","metadata":{"deletable":false,"editable":false,"id":"6ab6fb0b","nbgrader":{"cell_type":"code","checksum":"a11cdf1b79593a7073b05a78506d044b","grade":false,"grade_id":"cell-9047ce580136ef0f","locked":true,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1716675492635,"user_tz":-330,"elapsed":10,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["def collate_fn(tokenizer, batch):\n","    \"\"\"\n","    Collate function to be used when creating a data loader for the SIQA dataset.\n","    :param tokenizer: The tokenizer to be used to tokenize the inputs.\n","    :param batch: A list of tuples of the form (tokenized_input_dict, label)\n","    :return: A tuple of the form (tokenized_inputs_dict_batch, labels_batch)\n","    \"\"\"\n","\n","    tokenized_inputsA_batch = []\n","    tokenized_inputsB_batch = []\n","    tokenized_inputsC_batch = []\n","    labels_batch = []\n","    for tokenized_inputs_dict, label in batch:\n","        tokenized_inputsA_batch.append(tokenized_inputs_dict[\"A\"])\n","        tokenized_inputsB_batch.append(tokenized_inputs_dict[\"B\"])\n","        tokenized_inputsC_batch.append(tokenized_inputs_dict[\"C\"])\n","        labels_batch.append(label)\n","\n","    #Pad the inputs\n","    tokenized_inputsA_batch = tokenizer.pad(tokenized_inputsA_batch, padding=True, return_tensors=\"pt\")\n","    tokenized_inputsB_batch = tokenizer.pad(tokenized_inputsB_batch, padding=True, return_tensors=\"pt\")\n","    tokenized_inputsC_batch = tokenizer.pad(tokenized_inputsC_batch, padding=True, return_tensors=\"pt\")\n","\n","    # Convert labels list to a tensor\n","    labels_batch = torch.tensor(labels_batch)\n","    return (\n","        {\"A\": tokenized_inputsA_batch[\"input_ids\"], \"B\": tokenized_inputsB_batch[\"input_ids\"], \"C\": tokenized_inputsC_batch[\"input_ids\"]},\n","        {\"A\": tokenized_inputsA_batch[\"attention_mask\"], \"B\": tokenized_inputsB_batch[\"attention_mask\"], \"C\": tokenized_inputsC_batch[\"attention_mask\"]},\n","        labels_batch\n","    )\n"]},{"cell_type":"markdown","id":"2548c845","metadata":{"deletable":false,"editable":false,"id":"2548c845","nbgrader":{"cell_type":"markdown","checksum":"276f9714f43f47be2ce8d32465f0dda0","grade":false,"grade_id":"cell-e25eecdae3525f47","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Now that we have defined the collate_fn, lets create the dataloaders. It is common to use smaller batch size while fine-tuning these big models, as they occupy quite a lot of memory."]},{"cell_type":"code","execution_count":20,"id":"6c8784f5","metadata":{"deletable":false,"editable":false,"id":"6c8784f5","nbgrader":{"cell_type":"code","checksum":"38520c5b41b69dc2a467c460409e961c","grade":false,"grade_id":"cell-0155f0a92349c779","locked":true,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1716675492635,"user_tz":-330,"elapsed":9,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, train_dataset.tokenizer))\n","dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, dev_dataset.tokenizer))"]},{"cell_type":"code","execution_count":21,"id":"b9b42884","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"b9b42884","nbgrader":{"cell_type":"code","checksum":"7b9f3c4ce63be8d665ca5a5b7ea0f1cc","grade":false,"grade_id":"cell-04007c519041a199","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"bf0d60d1-fc3d-47fa-b721-db9dfb41b3d1","executionInfo":{"status":"ok","timestamp":1716675492635,"user_tz":-330,"elapsed":8,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["batch_input_ids:\n"," {'A': tensor([[  101,  5553,  2001,  2559,  5305,  2061,  9036,  9720,  5553,  2000,\n","          2202,  2070,  4200,  1012,   102,  2129,  2052,  2017,  6235,  9036,\n","          1029,   102,  2004,  4895, 10010,  2075,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  7546,  8823,  1996,  9850,  1998,  2018,  2070,  7852, 29593,\n","          2000,  2175,  2247,  2007,  2009,  1012,   102,  2129,  2052,  7546,\n","          2514,  5728,  1029,   102,  3201,  2005, 18064,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2626,  9806,  1005,  1055,  2338,  1998,  2002,  2001,\n","          2200,  7537,  2007,  1996,  3463,  1012,   102,  2054,  2097,  9806,\n","          2215,  2000,  2079,  2279,  1029,   102,  3477,  8804,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5863,  2165,  2185,  1996,  3274,  2013,  2035,  1997,  2014,\n","          4268,  1012,   102,  2129,  2052,  2017,  6235,  5863,  1029,   102,\n","          1037,  3625,  6687,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2356,  5899,  2000,  6235,  1996,  4169,  1010,  2138,\n","          8804,  2001,  6397,  1998,  2071,  2069,  5674,  2009,  1012,   102,\n","          2054,  2097,  5899,  2215,  2000,  2079,  2279,  1029,   102,  2424,\n","          2204, 24931,  2015,   102,     0,     0,     0,     0,     0],\n","        [  101, 14509,  2734,  2000,  2817,  2524,  2005,  1996, 11360,  2021,\n","          2069,  2985,  1037,  2261,  2781,  5702,  1012,   102,  2054,  2097,\n","          4148,  2000, 14509,  1029,   102,  8046, 27571, 16294,  2290,  2125,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5207,  4233, 11928,  1005,  1055,  3921,  1998,  2253,  2000,\n","          1996,  3573,  2004,  2855,  2004,  2825,  1012,   102,  2129,  2052,\n","          5207,  2514,  5728,  1029,   102, 12990,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101, 14509,  2106,  2025,  2131,  2247,  2007, 10555,  2021,  2027,\n","          8678,  2092,  1999,  2019,  8775,  2012,  2147,  1012,  1996,  2466,\n","          4527, 10555,  1005,  1055,  2767,  1012,   102,  2129,  2052, 14509,\n","          2514,  5728,  1029,   102,  1999, 19699, 23496,  2094,   102],\n","        [  101,  7232,  2409,  6683,  1005,  1055,  2365,  1037,  2466,  2043,\n","          6683,  2001,  5881,  2012,  2147,  1012,   102,  2054,  2097,  6683,\n","          2215,  2000,  2079,  2279,  1029,   102, 20432,  6683,  1005,  1055,\n","          2365,   102,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  4202,  2467,  7771,  1998,  2028,  2154,  4771,  2019,  2590,\n","         11360,  2349,  2000,  5777,  1012,   102,  2129,  2052,  4202,  2514,\n","          5728,  1029,   102,  2066,  1037,  4945,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101, 14509,  3271,  9036,  2147,  2006,  2037,  4646,  1012,   102,\n","          2054,  2097,  4148,  2000, 14509,  1029,   102,  6358,  9305,  8069,\n","          2005,  9036,  2000,  2147,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  3994,  2253,  2000,  1996,  2283,  1998,  2777,  1037,  2428,\n","          3835,  3124,  2045,  1012,   102,  2054,  2097, 14509,  2215,  2000,\n","          2079,  2279,  1029,   102,  2831,  2000,  1996,  3124,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2904,  2037, 13310,  2048,  2335,  2021,  2027,  2921,\n","          2183,  4257,  2005,  2019,  4242,  3114,  1012,   102,  2129,  2052,\n","          8804,  2514,  2004,  1037,  2765,  1029,   102,  7568,  2055,  1996,\n","         13310,   102,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  3389,  2001,  3480,  1998,  2359,  2000,  2425,  2500,  2027,\n","          2020,  2200,  5506,  1012,   102,  2054,  2097,  4148,  2000,  2500,\n","          1029,   102,  2022,  7568,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5207,  2001,  2397,  2000,  2147,  1998, 13842,  3071,  1005,\n","          1055,  2051,  1012,  5207,  3253,  5863,  2019,  7526,  1012,   102,\n","          2129,  2052,  2017,  6235,  5207,  1029,   102, 29454, 29206,  2102,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  4074,  2921, 18403,  1005,  1055,  5703,  2006,  1996,  7123,\n","          2008,  2003,  2006,  1996,  3274,  1012,   102,  2054,  2097,  4074,\n","          2215,  2000,  2079,  2279,  1029,   102,  6047,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'B': tensor([[  101,  5553,  2001,  2559,  5305,  2061,  9036,  9720,  5553,  2000,\n","          2202,  2070,  4200,  1012,   102,  2129,  2052,  2017,  6235,  9036,\n","          1029,   102,  9474,  1997,  5553,  1005,  1055,  2740,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  7546,  8823,  1996,  9850,  1998,  2018,  2070,  7852, 29593,\n","          2000,  2175,  2247,  2007,  2009,  1012,   102,  2129,  2052,  7546,\n","          2514,  5728,  1029,   102, 17727, 23004,  2055,  2833,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2626,  9806,  1005,  1055,  2338,  1998,  2002,  2001,\n","          2200,  7537,  2007,  1996,  3463,  1012,   102,  2054,  2097,  9806,\n","          2215,  2000,  2079,  2279,  1029,   102, 16360, 20026,  5685,  8804,\n","           102,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5863,  2165,  2185,  1996,  3274,  2013,  2035,  1997,  2014,\n","          4268,  1012,   102,  2129,  2052,  2017,  6235,  5863,  1029,   102,\n","          2919,  2021,  2113,  2016,  2734,  2000,  2079,  2009,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2356,  5899,  2000,  6235,  1996,  4169,  1010,  2138,\n","          8804,  2001,  6397,  1998,  2071,  2069,  5674,  2009,  1012,   102,\n","          2054,  2097,  5899,  2215,  2000,  2079,  2279,  1029,   102,  2507,\n","          1037,  6412,  1997,  1996,  3185,   102,     0,     0],\n","        [  101, 14509,  2734,  2000,  2817,  2524,  2005,  1996, 11360,  2021,\n","          2069,  2985,  1037,  2261,  2781,  5702,  1012,   102,  2054,  2097,\n","          4148,  2000, 14509,  1029,   102,  2079,  2092,  2006,  1996,  3231,\n","           102,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5207,  4233, 11928,  1005,  1055,  3921,  1998,  2253,  2000,\n","          1996,  3573,  2004,  2855,  2004,  2825,  1012,   102,  2129,  2052,\n","          5207,  2514,  5728,  1029,   102, 27863,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101, 14509,  2106,  2025,  2131,  2247,  2007, 10555,  2021,  2027,\n","          8678,  2092,  1999,  2019,  8775,  2012,  2147,  1012,  1996,  2466,\n","          4527, 10555,  1005,  1055,  2767,  1012,   102,  2129,  2052, 14509,\n","          2514,  5728,  1029,   102, 12456,  6820,  6553,   102],\n","        [  101,  7232,  2409,  6683,  1005,  1055,  2365,  1037,  2466,  2043,\n","          6683,  2001,  5881,  2012,  2147,  1012,   102,  2054,  2097,  6683,\n","          2215,  2000,  2079,  2279,  1029,   102,  4067,  7232,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  4202,  2467,  7771,  1998,  2028,  2154,  4771,  2019,  2590,\n","         11360,  2349,  2000,  5777,  1012,   102,  2129,  2052,  4202,  2514,\n","          5728,  1029,   102,  4394,  2019,  2590, 11360,  2349,  2000,  5777,\n","           102,     0,     0,     0,     0,     0,     0,     0],\n","        [  101, 14509,  3271,  9036,  2147,  2006,  2037,  4646,  1012,   102,\n","          2054,  2097,  4148,  2000, 14509,  1029,   102,  8069,  9036,  2515,\n","          2092,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  3994,  2253,  2000,  1996,  2283,  1998,  2777,  1037,  2428,\n","          3835,  3124,  2045,  1012,   102,  2054,  2097, 14509,  2215,  2000,\n","          2079,  2279,  1029,   102,  4965,  2070,  7967,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2904,  2037, 13310,  2048,  2335,  2021,  2027,  2921,\n","          2183,  4257,  2005,  2019,  4242,  3114,  1012,   102,  2129,  2052,\n","          8804,  2514,  2004,  1037,  2765,  1029,   102,  4854,  2055,  1996,\n","         13310,   102,     0,     0,     0,     0,     0,     0],\n","        [  101,  3389,  2001,  3480,  1998,  2359,  2000,  2425,  2500,  2027,\n","          2020,  2200,  5506,  1012,   102,  2054,  2097,  4148,  2000,  2500,\n","          1029,   102,  4553,  2055,  1996,  4963,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5207,  2001,  2397,  2000,  2147,  1998, 13842,  3071,  1005,\n","          1055,  2051,  1012,  5207,  3253,  5863,  2019,  7526,  1012,   102,\n","          2129,  2052,  2017,  6235,  5207,  1029,   102, 16655,  2595,  7874,\n","          2098,   102,     0,     0,     0,     0,     0,     0],\n","        [  101,  4074,  2921, 18403,  1005,  1055,  5703,  2006,  1996,  7123,\n","          2008,  2003,  2006,  1996,  3274,  1012,   102,  2054,  2097,  4074,\n","          2215,  2000,  2079,  2279,  1029,   102,  4339,  2091, 18403,  1005,\n","          1055,  3361,  2015,   102,     0,     0,     0,     0]]), 'C': tensor([[  101,  5553,  2001,  2559,  5305,  2061,  9036,  9720,  5553,  2000,\n","          2202,  2070,  4200,  1012,   102,  2129,  2052,  2017,  6235,  9036,\n","          1029,   102,  2004, 11922,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  7546,  8823,  1996,  9850,  1998,  2018,  2070,  7852, 29593,\n","          2000,  2175,  2247,  2007,  2009,  1012,   102,  2129,  2052,  7546,\n","          2514,  5728,  1029,   102, 23667, 17701,  2232,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2626,  9806,  1005,  1055,  2338,  1998,  2002,  2001,\n","          2200,  7537,  2007,  1996,  3463,  1012,   102,  2054,  2097,  9806,\n","          2215,  2000,  2079,  2279,  1029,   102,  2543,  8804,   102,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  5863,  2165,  2185,  1996,  3274,  2013,  2035,  1997,  2014,\n","          4268,  1012,   102,  2129,  2052,  2017,  6235,  5863,  1029,   102,\n","         10597, 14480,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2356,  5899,  2000,  6235,  1996,  4169,  1010,  2138,\n","          8804,  2001,  6397,  1998,  2071,  2069,  5674,  2009,  1012,   102,\n","          2054,  2097,  5899,  2215,  2000,  2079,  2279,  1029,   102,  2027,\n","          2481,  1005,  1056,  2156,   102,     0],\n","        [  101, 14509,  2734,  2000,  2817,  2524,  2005,  1996, 11360,  2021,\n","          2069,  2985,  1037,  2261,  2781,  5702,  1012,   102,  2054,  2097,\n","          4148,  2000, 14509,  1029,   102,  2131,  1037,  2659,  3556,   102,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  5207,  4233, 11928,  1005,  1055,  3921,  1998,  2253,  2000,\n","          1996,  3573,  2004,  2855,  2004,  2825,  1012,   102,  2129,  2052,\n","          5207,  2514,  5728,  1029,   102,  9943,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101, 14509,  2106,  2025,  2131,  2247,  2007, 10555,  2021,  2027,\n","          8678,  2092,  1999,  2019,  8775,  2012,  2147,  1012,  1996,  2466,\n","          4527, 10555,  1005,  1055,  2767,  1012,   102,  2129,  2052, 14509,\n","          2514,  5728,  1029,   102, 14424,   102],\n","        [  101,  7232,  2409,  6683,  1005,  1055,  2365,  1037,  2466,  2043,\n","          6683,  2001,  5881,  2012,  2147,  1012,   102,  2054,  2097,  6683,\n","          2215,  2000,  2079,  2279,  1029,   102,  2994,  2012,  2147,  2035,\n","          2305,   102,     0,     0,     0,     0],\n","        [  101,  4202,  2467,  7771,  1998,  2028,  2154,  4771,  2019,  2590,\n","         11360,  2349,  2000,  5777,  1012,   102,  2129,  2052,  4202,  2514,\n","          5728,  1029,   102,  5777,  2467,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101, 14509,  3271,  9036,  2147,  2006,  2037,  4646,  1012,   102,\n","          2054,  2097,  4148,  2000, 14509,  1029,   102,  2022,  4844,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  3994,  2253,  2000,  1996,  2283,  1998,  2777,  1037,  2428,\n","          3835,  3124,  2045,  1012,   102,  2054,  2097, 14509,  2215,  2000,\n","          2079,  2279,  1029,   102,  2377,  1999,  1037,  2492,   102,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2904,  2037, 13310,  2048,  2335,  2021,  2027,  2921,\n","          2183,  4257,  2005,  2019,  4242,  3114,  1012,   102,  2129,  2052,\n","          8804,  2514,  2004,  1037,  2765,  1029,   102,  3407,  2055,  1996,\n","          3663,   102,     0,     0,     0,     0],\n","        [  101,  3389,  2001,  3480,  1998,  2359,  2000,  2425,  2500,  2027,\n","          2020,  2200,  5506,  1012,   102,  2054,  2097,  4148,  2000,  2500,\n","          1029,   102,  2425,  1996,  2500,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  5207,  2001,  2397,  2000,  2147,  1998, 13842,  3071,  1005,\n","          1055,  2051,  1012,  5207,  3253,  5863,  2019,  7526,  1012,   102,\n","          2129,  2052,  2017,  6235,  5207,  1029,   102,  3404, 13966,   102,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  4074,  2921, 18403,  1005,  1055,  5703,  2006,  1996,  7123,\n","          2008,  2003,  2006,  1996,  3274,  1012,   102,  2054,  2097,  4074,\n","          2215,  2000,  2079,  2279,  1029,   102, 13971,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0]])}\n","batch_attn_mask:\n"," {'A': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'B': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]), 'C': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","batch_labels:\n"," tensor([2, 1, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1])\n"]}],"source":["batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n","print(f\"batch_input_ids:\\n {batch_input_ids}\")\n","print(f\"batch_attn_mask:\\n {batch_attn_mask}\")\n","print(f\"batch_labels:\\n {batch_labels}\")"]},{"cell_type":"markdown","id":"849edfc0","metadata":{"deletable":false,"editable":false,"id":"849edfc0","nbgrader":{"cell_type":"markdown","checksum":"7f5abac97eb62041f0add8617ad42871","grade":false,"grade_id":"cell-21c75958035402dc","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Task 2: Implementing and Training BERT-based Multiple Choice Classifier (1 hour 30 minutes)\n","\n","Similar to pretrained tokenizers, the transformers library also provide numerous pre-trained language models that can be fine-tuned on a wide variety of downstream tasks. We demonstrate usage of these models below."]},{"cell_type":"code","execution_count":22,"id":"c9322600","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":760,"referenced_widgets":["0aaf0b8f7d354e53a6a60aa8a2b1fa1f","ec5db08cf21747e2b600ab5ea53ca384","20ebee06a3cc453185cb47cc48a70d2b","0dd528b7d9a5464db8ff75ce53fd0158","d34f043f4cf2446b94072b8a76e8134b","8a8b83f74ad14aab9917bebf6ea77d2b","c7c3a53e576345b8b6b6ee9a497d01ba","9bd15de28db4447d98b342788f4e3b74","3b163175b6d5426bb83010f69d0f9ccc","d6f0423f6b064854a99165e2a0c8b4ac","05a5cdd3e99d41ff8388be670bd45ed2"]},"deletable":false,"editable":false,"id":"c9322600","nbgrader":{"cell_type":"code","checksum":"292023c90e82c1cc3836a7c39718fb03","grade":false,"grade_id":"cell-0518264e94de005b","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"14898c32-a290-4d77-9dda-1623537aa3fd","executionInfo":{"status":"ok","timestamp":1716675500869,"user_tz":-330,"elapsed":8239,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aaf0b8f7d354e53a6a60aa8a2b1fa1f"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSdpaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":22}],"source":["# Import BertModel from the library\n","from transformers import BertModel\n","\n","# Create an instance of pretrained BERT\n","bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n","bert_model"]},{"cell_type":"markdown","id":"4a7f074b","metadata":{"deletable":false,"editable":false,"id":"4a7f074b","nbgrader":{"cell_type":"markdown","checksum":"98a82e5b7717eda4f5226c6ae269fe58","grade":false,"grade_id":"cell-ad0d391f24e70315","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["As you can see very similar to how we created pre-trained tokenizer, we can load a pretrained BERT model by calling `BertModel.from_pretrained(bert-base-uncased)`. This can actually be considered just a Pytorch `nn.Module` like `nn.Linear` and can be similarly plugged into a network architecture. Also, notice the model contains 12 BERT layers, where each layer consists of a Self Attention layer followed by a sequence of linear layers and activation functions (MLP), as we discussed when talking about Transformer architecture in the lecture."]},{"cell_type":"code","execution_count":23,"id":"c994b139","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"c994b139","nbgrader":{"cell_type":"code","checksum":"ac922a7cea63724c27c0cf5c6f00ad83","grade":false,"grade_id":"cell-44c7f2f43d5ba025","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"ccffb787-8cb7-443e-ae9f-2195ad5f66d8","executionInfo":{"status":"ok","timestamp":1716675502675,"user_tz":-330,"elapsed":1824,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2823, -0.2353, -0.3529,  ..., -0.0834,  0.2548,  0.4870],\n","         [ 0.4055, -1.1768, -0.2842,  ..., -0.3740,  0.3920, -0.4480],\n","         [ 0.0377, -0.7788, -0.1174,  ..., -0.4201, -0.3078,  0.1824],\n","         ...,\n","         [-1.1595, -1.5650, -0.2526,  ..., -0.4569, -0.5474,  0.2315],\n","         [-1.0644, -0.5952, -0.3912,  ...,  0.2788, -0.0207, -0.1262],\n","         [ 0.5158,  0.4573,  0.0263,  ...,  0.1445, -0.6398, -0.5258]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1342e-01, -3.3350e-01, -4.4551e-01,  4.5434e-01,  5.0347e-01,\n","         -8.4619e-02,  4.8077e-01,  8.4160e-02, -2.2890e-01, -9.9974e-01,\n","         -2.4549e-01,  7.9481e-01,  9.8030e-01, -2.8878e-02,  9.1588e-01,\n","         -2.4135e-01,  5.3666e-01, -5.0656e-01,  2.2218e-01,  3.2123e-01,\n","          6.4942e-01,  9.9757e-01,  4.7173e-01,  2.0766e-01,  3.6500e-01,\n","          8.8725e-01, -4.2194e-01,  9.3109e-01,  9.1743e-01,  7.2114e-01,\n","         -1.4354e-02, -1.8941e-02, -9.8951e-01,  3.0721e-02, -2.8992e-01,\n","         -9.8191e-01,  1.7840e-01, -5.9347e-01,  1.1879e-01,  2.5965e-01,\n","         -8.6462e-01,  1.4619e-01,  9.9952e-01, -4.7740e-01,  1.0605e-01,\n","         -1.2106e-01, -9.9986e-01,  2.2875e-01, -8.5771e-01,  5.0008e-01,\n","          2.7700e-01,  7.1537e-01,  9.0395e-02,  3.0770e-01,  3.6540e-01,\n","         -7.2003e-02, -2.7237e-01, -2.9247e-02, -2.4881e-01, -4.1984e-01,\n","         -6.4784e-01,  1.4369e-01, -5.4503e-01, -7.8703e-01,  4.1154e-01,\n","          2.9400e-01, -1.3110e-01, -8.3965e-02,  6.9508e-03, -1.4154e-01,\n","          6.8059e-01,  8.8733e-02,  1.5826e-02, -8.0095e-01,  1.2098e-01,\n","          1.5737e-01, -5.1418e-01,  1.0000e+00,  4.2995e-02, -9.8423e-01,\n","          1.7809e-01,  2.3727e-01,  3.4371e-01,  4.4902e-01, -2.5424e-01,\n","         -1.0000e+00,  3.7379e-01, -2.9287e-03, -9.9117e-01,  1.0483e-01,\n","          4.8637e-01, -2.1140e-01, -1.3496e-01,  4.0792e-01,  5.8095e-02,\n","         -1.5223e-01, -2.3065e-01, -4.2501e-01, -1.0952e-01, -1.6091e-01,\n","          2.2054e-01,  7.6145e-02, -4.6462e-02, -2.1545e-01,  2.0504e-01,\n","         -4.3479e-01,  2.3063e-02,  4.2032e-01, -3.0370e-01,  5.1830e-01,\n","          3.7295e-01, -1.9882e-01,  2.8470e-01, -9.4566e-01,  3.9429e-01,\n","         -2.5140e-01, -9.8011e-01, -4.6321e-01, -9.9085e-01,  6.2031e-01,\n","          3.0362e-02, -2.4470e-01,  9.4980e-01,  4.4613e-01,  1.0056e-01,\n","          1.2657e-01, -4.9239e-01, -1.0000e+00, -3.8979e-01,  1.7305e-02,\n","          6.2904e-02,  7.5698e-03, -9.6858e-01, -9.6126e-01,  3.2523e-01,\n","          9.4696e-01,  2.4653e-02,  9.9795e-01, -1.0900e-01,  9.2609e-01,\n","          2.7076e-01, -2.3979e-01,  2.4427e-03, -4.1432e-01,  3.1651e-01,\n","         -2.5690e-01, -5.1866e-02,  1.3838e-01, -1.3608e-01,  1.5090e-02,\n","         -2.7091e-01,  1.0166e-02, -6.0732e-02, -9.0828e-01, -2.3963e-01,\n","          9.5270e-01, -1.2421e-01, -5.1564e-01,  4.5463e-01, -9.4697e-02,\n","          1.9800e-02,  6.7040e-01,  3.5773e-01,  3.0287e-01, -3.0182e-01,\n","          3.2714e-01, -3.3120e-01,  4.4667e-01, -5.9530e-01,  3.9171e-01,\n","          2.3601e-01, -1.9295e-01, -1.1556e-01, -9.7889e-01, -2.1943e-01,\n","          1.7873e-01,  9.8326e-01,  6.1193e-01,  1.2009e-01,  4.6305e-01,\n","         -2.2367e-01,  5.1362e-01, -9.4201e-01,  9.8198e-01,  2.3595e-02,\n","          1.6000e-01, -1.6324e-01,  2.4670e-01, -8.0511e-01, -3.3924e-01,\n","          4.8846e-01, -5.1100e-01, -7.3509e-01,  4.6975e-02, -3.3179e-01,\n","         -1.8198e-01, -4.1462e-01,  1.0465e-01, -2.1432e-01, -3.4277e-01,\n","          9.7266e-02,  9.3661e-01,  7.2161e-01,  4.8405e-01, -3.6871e-01,\n","          3.1814e-01, -8.4417e-01, -4.6677e-01,  2.3358e-02,  8.2281e-02,\n","         -3.5102e-02,  9.9018e-01, -2.8120e-01,  1.5138e-01, -8.7715e-01,\n","         -9.8174e-01, -1.8074e-01, -8.3024e-01, -1.6251e-01, -4.6465e-01,\n","          4.1388e-01, -5.0448e-01,  7.9161e-03,  1.6464e-01, -8.4324e-01,\n","         -5.9927e-01,  3.1949e-01, -1.7097e-01,  3.2296e-01, -2.7389e-01,\n","          9.0340e-01,  6.0541e-01, -4.7819e-01, -3.6939e-01,  9.1910e-01,\n","         -2.9349e-01, -7.2003e-01,  3.8927e-01, -1.0532e-01,  5.2019e-01,\n","         -4.1045e-01,  9.4339e-01,  6.6610e-01,  4.9983e-01, -8.7163e-01,\n","          1.0597e-02, -6.2514e-01,  5.1350e-02,  7.7687e-04, -5.0177e-01,\n","          2.8172e-01,  4.3400e-01,  2.9356e-01,  7.4288e-01, -4.1969e-02,\n","          8.6796e-01, -9.1876e-01, -9.4036e-01, -7.8274e-01,  9.0406e-02,\n","         -9.8649e-01,  1.0411e-01,  1.7797e-01, -1.0403e-01, -2.1737e-01,\n","         -1.7412e-01, -9.4650e-01,  3.6575e-01, -4.9280e-02,  9.1307e-01,\n","         -3.6831e-01, -6.6573e-01, -4.2411e-01, -9.2306e-01, -2.2497e-01,\n","         -1.3000e-01,  6.1592e-02, -1.6496e-01, -9.4146e-01,  4.3231e-01,\n","          4.3532e-01,  4.4700e-01, -1.5150e-01,  9.6835e-01,  9.9996e-01,\n","          9.6499e-01,  8.9856e-01,  4.0772e-01, -9.9106e-01, -7.2477e-01,\n","          9.9990e-01, -8.8650e-01, -9.9999e-01, -8.8209e-01, -3.2363e-01,\n","         -1.0676e-02, -1.0000e+00, -6.8102e-02,  2.3007e-01, -8.1374e-01,\n","          1.3577e-01,  9.7153e-01,  9.1199e-01, -1.0000e+00,  7.6972e-01,\n","          9.3895e-01, -4.7636e-01,  7.2471e-01, -1.8839e-01,  9.6564e-01,\n","          2.1396e-01,  3.6877e-01, -6.0258e-02,  2.6505e-01, -5.1108e-01,\n","         -4.2091e-01,  1.9411e-02, -2.5991e-01,  9.7003e-01, -2.1713e-02,\n","         -4.3161e-01, -8.6556e-01,  2.8448e-01,  5.9014e-02, -4.4330e-01,\n","         -9.4856e-01, -1.1291e-01,  2.5513e-02,  3.7114e-01,  9.9048e-03,\n","          5.2219e-02, -3.4138e-01, -3.1813e-02, -3.0110e-01, -5.4604e-02,\n","          5.3167e-01, -9.1741e-01, -2.2246e-01, -1.0531e-02, -4.1508e-01,\n","          3.0833e-01, -9.7295e-01,  9.5327e-01, -2.9202e-01,  5.2656e-01,\n","          1.0000e+00, -1.5967e-02, -7.8224e-01,  2.5205e-01,  8.0371e-02,\n","         -1.1654e-01,  1.0000e+00,  5.2342e-01, -9.7992e-01, -4.5646e-01,\n","          4.4127e-01, -3.6342e-01, -5.4687e-01,  9.9663e-01, -1.6571e-01,\n","         -2.4607e-01,  7.1765e-02,  9.8694e-01, -9.8773e-01,  9.2531e-01,\n","         -7.8466e-01, -9.7499e-01,  9.5526e-01,  9.4014e-01, -3.1454e-02,\n","         -3.7029e-01, -8.3566e-02,  7.2884e-02,  7.8482e-02, -8.5670e-01,\n","          2.4106e-01,  1.2382e-01,  2.2440e-02,  9.0840e-01,  4.9767e-02,\n","         -4.8200e-01,  1.0067e-01, -3.3606e-01,  2.7571e-01,  5.1258e-01,\n","          3.4788e-01,  2.0840e-02, -4.1821e-02,  2.2078e-02, -5.3827e-01,\n","         -9.6365e-01,  5.3596e-01,  1.0000e+00,  1.7988e-01,  2.2127e-01,\n","          1.1171e-01,  4.2886e-02, -2.8482e-01,  2.7275e-01,  2.8186e-01,\n","         -1.9890e-01, -6.9904e-01,  5.4585e-01, -8.2020e-01, -9.8801e-01,\n","          3.9557e-01,  1.4115e-01, -9.0601e-02,  9.9788e-01,  5.2466e-02,\n","          8.7988e-02, -6.1068e-02,  8.0411e-01, -1.0107e-01,  4.5895e-02,\n","          2.8721e-01,  9.7186e-01, -5.0391e-02,  4.3895e-01,  5.3262e-01,\n","         -3.7363e-01, -1.4349e-01, -5.3335e-01, -1.7288e-01, -9.3699e-01,\n","          2.4583e-01, -9.5441e-01,  9.4962e-01,  6.8467e-01,  2.8843e-01,\n","          2.7844e-02,  1.9399e-01,  1.0000e+00, -5.5154e-01,  2.7470e-01,\n","          7.3334e-01,  2.1665e-01, -9.9376e-01, -6.3638e-01, -4.0457e-01,\n","          8.1165e-02, -1.0932e-01, -1.6521e-01,  1.1090e-01, -9.6194e-01,\n","          1.4183e-01,  2.6996e-01, -9.0304e-01, -9.8854e-01, -1.8411e-01,\n","         -1.4560e-01,  1.0233e-01, -8.8277e-01, -4.2014e-01, -5.6760e-01,\n","          2.0938e-01, -7.8962e-02, -9.2779e-01,  3.7535e-01, -3.2772e-01,\n","          3.7110e-01, -1.7455e-02,  4.4611e-01,  3.7664e-01,  8.9600e-01,\n","         -1.4597e-01, -4.1541e-02, -2.2711e-02, -5.6407e-01,  4.9451e-01,\n","         -6.0685e-01, -5.7528e-01,  1.6080e-02,  1.0000e+00, -2.4831e-01,\n","          4.7874e-01,  3.2856e-01,  4.0354e-01,  3.5562e-02,  7.1662e-02,\n","          5.1132e-01,  1.7113e-01, -5.2386e-04, -2.9695e-01,  7.3751e-01,\n","         -1.8529e-01,  4.4554e-01,  2.2911e-01,  2.8564e-02,  7.7319e-01,\n","          5.5269e-01,  1.0272e-01,  2.6211e-01, -1.2719e-03,  9.7222e-01,\n","         -2.8842e-02,  5.6738e-02, -2.8775e-01, -1.6172e-02, -1.9353e-01,\n","          4.9592e-01,  1.0000e+00,  8.4379e-02, -1.5369e-01, -9.8806e-01,\n","         -3.3429e-01, -7.0185e-01,  9.9986e-01,  7.6013e-01, -6.0292e-01,\n","          3.8539e-01,  2.5654e-01, -1.1947e-01,  3.1904e-01, -2.9968e-02,\n","         -8.2039e-02, -3.6788e-02, -4.1350e-02,  9.4251e-01, -3.8563e-01,\n","         -9.6644e-01, -1.8887e-01,  3.3731e-01, -9.5321e-01,  9.9444e-01,\n","         -3.1648e-01, -7.6926e-02, -2.2872e-01, -1.2125e-01, -8.2212e-01,\n","         -1.8324e-01, -9.8104e-01,  3.3370e-03,  6.5977e-02,  9.6485e-01,\n","          6.2124e-02, -4.1838e-01, -8.8928e-01,  4.6512e-01,  1.6997e-01,\n","         -4.9288e-01, -9.0313e-01,  9.4608e-01, -9.6796e-01,  4.0968e-01,\n","          9.9998e-01,  2.5584e-01, -4.0539e-01,  1.3442e-01, -1.9956e-01,\n","          2.1901e-01, -7.6150e-02,  4.1793e-01, -9.3571e-01, -2.5734e-01,\n","         -2.6220e-02,  1.9381e-01, -1.0859e-02,  1.0623e-02,  5.5476e-01,\n","          1.3884e-01, -3.7362e-01, -5.1091e-01, -2.0795e-02,  2.3886e-01,\n","          4.4453e-01, -1.9578e-01,  2.4322e-02,  1.2190e-01,  4.5708e-02,\n","         -8.9249e-01, -2.3091e-01, -2.3275e-01, -9.9933e-01,  3.8215e-01,\n","         -1.0000e+00,  2.4109e-01, -3.3743e-01, -9.5531e-02,  7.7688e-01,\n","          6.7944e-01,  5.0124e-01, -5.4854e-01, -4.4368e-01,  7.4304e-01,\n","          6.9326e-01, -1.1128e-01,  5.8065e-02, -5.1514e-01, -1.8507e-02,\n","          4.7066e-02, -3.9159e-02, -1.2526e-01,  6.4872e-01, -2.8116e-01,\n","          1.0000e+00,  9.4141e-02, -1.6344e-01, -8.3718e-01,  9.6640e-02,\n","         -1.5380e-01,  9.9999e-01, -4.3615e-01, -9.4652e-01,  1.8444e-01,\n","         -3.2900e-01, -7.3108e-01,  2.8296e-01, -1.6438e-01, -5.5741e-01,\n","         -5.1496e-01,  9.4393e-01,  1.5646e-01, -4.8041e-01,  3.7670e-01,\n","         -7.6502e-02, -3.4085e-01, -2.3005e-01,  4.6745e-01,  9.8561e-01,\n","          1.8930e-01,  6.5902e-01, -1.4607e-01, -5.6217e-02,  9.6718e-01,\n","          2.0109e-01, -4.4703e-01,  2.6058e-03,  1.0000e+00,  2.5435e-01,\n","         -8.5391e-01,  1.4343e-01, -9.7004e-01,  6.7542e-02, -9.1234e-01,\n","          2.3352e-01, -1.0207e-01,  9.0571e-01, -1.2382e-01,  8.9837e-01,\n","         -2.5035e-01, -9.6540e-02, -1.3500e-02,  7.9871e-02,  3.4886e-01,\n","         -9.0798e-01, -9.8696e-01, -9.8399e-01,  2.2660e-01, -2.6349e-01,\n","         -1.7571e-03,  2.5412e-01, -4.6728e-02,  3.2300e-01,  2.3782e-01,\n","         -1.0000e+00,  9.4718e-01,  2.8214e-01,  5.5079e-01,  9.5964e-01,\n","          4.3230e-01,  2.9824e-01,  1.1796e-01, -9.8534e-01, -9.1499e-01,\n","         -2.2060e-01, -2.2493e-01,  4.2716e-01,  4.1828e-01,  7.7361e-01,\n","          2.5652e-01, -4.4088e-01, -5.4283e-01, -1.9670e-01, -9.2347e-01,\n","         -9.9092e-01,  1.6564e-01, -2.1976e-02, -7.5007e-01,  9.5135e-01,\n","         -4.2565e-01,  4.1158e-02,  3.4651e-01, -3.7918e-01,  5.3755e-01,\n","          6.6361e-01, -1.7167e-01, -1.6186e-01,  3.5413e-01,  8.6273e-01,\n","          5.4597e-01,  9.7368e-01, -3.1056e-01,  3.8867e-01, -3.5970e-01,\n","          3.0381e-01,  7.2659e-01, -8.7832e-01,  4.5919e-02,  5.9937e-02,\n","          1.7157e-01,  1.6686e-01, -1.6294e-01, -7.8675e-01,  4.3243e-02,\n","         -2.4215e-01,  9.7457e-02, -2.9032e-01,  2.1996e-01, -2.2742e-01,\n","          4.9331e-02, -3.6432e-01, -2.3407e-01,  5.0983e-01, -1.0163e-01,\n","          9.1016e-01,  5.7243e-01,  4.2006e-02, -4.0053e-01, -9.6793e-03,\n","         -1.7138e-01, -8.6321e-01,  6.2138e-01,  2.0165e-01,  2.1101e-01,\n","          1.2278e-01, -1.7986e-01,  8.6453e-01, -5.4444e-01, -3.0941e-01,\n","         -3.3675e-01, -3.4412e-01,  7.0476e-01, -5.8997e-01, -3.5156e-01,\n","         -5.8885e-02,  4.5485e-01,  1.9294e-01,  9.9796e-01, -1.2720e-01,\n","         -3.9385e-01, -3.1919e-01, -2.4795e-01,  2.6886e-01,  3.8581e-02,\n","         -1.0000e+00,  1.7946e-01, -1.6161e-01, -9.5155e-02, -2.4447e-01,\n","          3.1449e-01, -3.1113e-01, -9.0355e-01, -1.8078e-02,  5.0788e-01,\n","          4.1148e-01, -4.0674e-01, -3.7335e-01,  4.8784e-01, -2.1415e-01,\n","          8.2619e-01,  8.2344e-01, -1.7268e-01,  6.6992e-01,  5.1202e-01,\n","         -4.5926e-01, -5.4121e-01,  9.0145e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"metadata":{},"execution_count":23}],"source":["sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n","tokenizer_output = bert_tokenizer(sentence, return_tensors=\"pt\")\n","input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n","\n","output = bert_model(input_ids, attention_mask = attn_mask)\n","output"]},{"cell_type":"markdown","id":"919f7bfc","metadata":{"deletable":false,"editable":false,"id":"919f7bfc","nbgrader":{"cell_type":"markdown","checksum":"473fa901be8e6117f55619d66f2ff8c3","grade":false,"grade_id":"cell-dc933bfb33ddbf97","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["As you can see, calling `bert_model` returns a bunch of different things. Let's go through them one by one and understand"]},{"cell_type":"code","execution_count":24,"id":"1f337132","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"1f337132","nbgrader":{"cell_type":"code","checksum":"af8fbb32c44fe43322dffc6d07810f4e","grade":false,"grade_id":"cell-40ff5447737be114","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"f03924f1-0932-476d-8eb0-7989d2fe735e","executionInfo":{"status":"ok","timestamp":1716675502675,"user_tz":-330,"elapsed":16,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids shape: torch.Size([1, 21])\n","last_hidden_state shape: torch.Size([1, 21, 768])\n"]}],"source":["last_hidden_state = output.last_hidden_state\n","print(f\"input_ids shape: {input_ids.shape}\")\n","print(f\"last_hidden_state shape: {last_hidden_state.shape}\")"]},{"cell_type":"markdown","id":"7fa9e5c2","metadata":{"deletable":false,"editable":false,"id":"7fa9e5c2","nbgrader":{"cell_type":"markdown","checksum":"de5e5dd782c7ff39ef7ee46739bcb46e","grade":false,"grade_id":"cell-3b5b43db00757d9a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["For an input of shape `[1,21]` which just means a single sequence of 21 tokens, last_hidden_state is a tensor of shape `[1, 21, 768]` denoting the contextual embedding of each of the 21 tokens in the sequence. These representations can be then used for solving a downstream task, by adding a linear layer or MLP layer on top. These can be useful for sequence labelling type of tasks."]},{"cell_type":"code","execution_count":25,"id":"c9c00632","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"c9c00632","nbgrader":{"cell_type":"code","checksum":"00a748ad98adedcbc525dda6496de309","grade":false,"grade_id":"cell-2b52de7298f3eda7","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"282a839e-ba66-41f5-f6c7-2a8949341c0d","executionInfo":{"status":"ok","timestamp":1716675502675,"user_tz":-330,"elapsed":11,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids shape: torch.Size([1, 21])\n","pooler_output shape: torch.Size([1, 768])\n"]}],"source":["pooler_output = output.pooler_output\n","print(f\"input_ids shape: {input_ids.shape}\")\n","print(f\"pooler_output shape: {pooler_output.shape}\")"]},{"cell_type":"markdown","id":"88dae74d","metadata":{"deletable":false,"editable":false,"id":"88dae74d","nbgrader":{"cell_type":"markdown","checksum":"a6a16d47faf0f2b84efa11ea9416a8c6","grade":false,"grade_id":"cell-720761c11ee8fa23","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["`pooler_output` is an aggregate representation of the entire sentence and can be thought of as a sentence embedding. It is obtained by passing the representation of the \\[CLS\\] token through a linear layer. This can be useful for sentence-level tasks like sentiment analysis as well as multiple choice classification tasks etc."]},{"cell_type":"markdown","id":"5785c1f7","metadata":{"deletable":false,"editable":false,"id":"5785c1f7","nbgrader":{"cell_type":"markdown","checksum":"94333c842666bbf724ab301c505a21a9","grade":false,"grade_id":"cell-4b59e5d86dbd6e41","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Apart from these two we can also obtain other values by providing additional arguments. Like if we want to obtain attention maps which can be useful for interpretating the model's behavior, we can just specify `output_attentions=True` while calling the model"]},{"cell_type":"code","execution_count":26,"id":"50ae6ee7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"50ae6ee7","nbgrader":{"cell_type":"code","checksum":"dac70c849fa65e8f8d505d88c9e88019","grade":false,"grade_id":"cell-08b6ef4cef17c16e","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"dda07680-c027-4bef-fa45-c06f418ad53c","executionInfo":{"status":"ok","timestamp":1716675502675,"user_tz":-330,"elapsed":10,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data type of attentions output: <class 'tuple'>\n","Number of elements: 12\n","Shape of individual element: torch.Size([1, 12, 21, 21])\n","Example attention map: tensor([[0.0365, 0.0188, 0.0239, 0.0918, 0.0375, 0.0409, 0.0390, 0.0327, 0.0256,\n","         0.0260, 0.0488, 0.0387, 0.0532, 0.0188, 0.0251, 0.0408, 0.0321, 0.0840,\n","         0.0793, 0.0262, 0.1801],\n","        [0.0178, 0.0460, 0.0240, 0.0306, 0.0482, 0.0194, 0.0510, 0.0919, 0.0260,\n","         0.0818, 0.0425, 0.0482, 0.0159, 0.0576, 0.0703, 0.1526, 0.0337, 0.0405,\n","         0.0194, 0.0371, 0.0454],\n","        [0.0527, 0.0466, 0.0899, 0.0241, 0.0464, 0.0245, 0.0645, 0.0587, 0.0256,\n","         0.0401, 0.0298, 0.0606, 0.0302, 0.0735, 0.1222, 0.0573, 0.0297, 0.0204,\n","         0.0224, 0.0539, 0.0268],\n","        [0.0425, 0.0528, 0.0454, 0.0338, 0.0402, 0.0353, 0.0515, 0.0666, 0.0397,\n","         0.0464, 0.0336, 0.0518, 0.0492, 0.0609, 0.0529, 0.0731, 0.0509, 0.0580,\n","         0.0381, 0.0492, 0.0280],\n","        [0.0492, 0.1514, 0.0892, 0.0078, 0.0237, 0.0282, 0.0478, 0.0639, 0.0288,\n","         0.0346, 0.0376, 0.0427, 0.0386, 0.0865, 0.0633, 0.0612, 0.0589, 0.0099,\n","         0.0217, 0.0326, 0.0225],\n","        [0.0328, 0.0851, 0.0669, 0.0210, 0.0342, 0.0278, 0.0618, 0.0965, 0.0244,\n","         0.0388, 0.0382, 0.0554, 0.0281, 0.0809, 0.0671, 0.0634, 0.0372, 0.0276,\n","         0.0302, 0.0289, 0.0536],\n","        [0.0390, 0.0402, 0.1059, 0.0350, 0.0359, 0.0453, 0.0443, 0.0639, 0.0261,\n","         0.0437, 0.0524, 0.0837, 0.0440, 0.0611, 0.0661, 0.0560, 0.0265, 0.0284,\n","         0.0247, 0.0368, 0.0410],\n","        [0.0637, 0.0480, 0.0736, 0.0488, 0.0387, 0.0301, 0.0386, 0.1177, 0.0174,\n","         0.0286, 0.0322, 0.0448, 0.0351, 0.0715, 0.0595, 0.0795, 0.0202, 0.0495,\n","         0.0239, 0.0297, 0.0488],\n","        [0.0379, 0.0930, 0.0471, 0.0373, 0.0634, 0.0240, 0.0501, 0.0724, 0.0077,\n","         0.0576, 0.1025, 0.0672, 0.0309, 0.0510, 0.0276, 0.0513, 0.0243, 0.0229,\n","         0.0249, 0.0429, 0.0641],\n","        [0.0287, 0.1002, 0.0683, 0.0142, 0.0493, 0.0313, 0.0819, 0.0516, 0.0350,\n","         0.0476, 0.0602, 0.0453, 0.0288, 0.0470, 0.1209, 0.0661, 0.0310, 0.0140,\n","         0.0197, 0.0225, 0.0367],\n","        [0.0285, 0.0755, 0.0461, 0.0240, 0.0530, 0.0329, 0.0516, 0.0549, 0.0636,\n","         0.0900, 0.0263, 0.0202, 0.0290, 0.0448, 0.0628, 0.0496, 0.0781, 0.0263,\n","         0.0399, 0.0470, 0.0559],\n","        [0.0478, 0.0464, 0.0582, 0.0576, 0.0513, 0.0424, 0.0580, 0.0508, 0.0362,\n","         0.0565, 0.0378, 0.0387, 0.0565, 0.0367, 0.0458, 0.0434, 0.0477, 0.0464,\n","         0.0433, 0.0513, 0.0475],\n","        [0.0320, 0.0923, 0.0587, 0.0300, 0.0370, 0.0399, 0.0736, 0.1011, 0.0183,\n","         0.0490, 0.0473, 0.0639, 0.0212, 0.0637, 0.0596, 0.0799, 0.0234, 0.0308,\n","         0.0306, 0.0149, 0.0330],\n","        [0.0335, 0.0893, 0.1025, 0.0227, 0.0361, 0.0223, 0.0555, 0.1325, 0.0104,\n","         0.0403, 0.0382, 0.0476, 0.0227, 0.0636, 0.0781, 0.0949, 0.0216, 0.0269,\n","         0.0159, 0.0252, 0.0202],\n","        [0.0544, 0.1423, 0.0956, 0.0254, 0.0281, 0.0320, 0.0471, 0.0546, 0.0136,\n","         0.0673, 0.0418, 0.0395, 0.0531, 0.0422, 0.0400, 0.1038, 0.0275, 0.0331,\n","         0.0201, 0.0151, 0.0234],\n","        [0.0403, 0.0612, 0.0584, 0.0240, 0.0489, 0.0368, 0.0703, 0.0585, 0.0444,\n","         0.0276, 0.0329, 0.0323, 0.0453, 0.0484, 0.1187, 0.0948, 0.0410, 0.0257,\n","         0.0229, 0.0184, 0.0490],\n","        [0.0249, 0.0599, 0.0252, 0.0498, 0.0809, 0.0517, 0.0329, 0.0799, 0.0372,\n","         0.0461, 0.0336, 0.0263, 0.0299, 0.0659, 0.0677, 0.0610, 0.0249, 0.0287,\n","         0.0350, 0.0519, 0.0864],\n","        [0.0298, 0.0571, 0.0516, 0.0481, 0.0414, 0.0276, 0.0582, 0.0824, 0.0294,\n","         0.0469, 0.0406, 0.0386, 0.0444, 0.0660, 0.0562, 0.0832, 0.0321, 0.0636,\n","         0.0414, 0.0343, 0.0273],\n","        [0.0209, 0.0511, 0.0464, 0.0348, 0.0476, 0.0474, 0.0860, 0.0561, 0.0389,\n","         0.0365, 0.0628, 0.0366, 0.0508, 0.0486, 0.0764, 0.0663, 0.0431, 0.0291,\n","         0.0349, 0.0329, 0.0528],\n","        [0.0228, 0.1058, 0.1149, 0.0203, 0.0668, 0.0299, 0.0469, 0.0448, 0.0492,\n","         0.0439, 0.0376, 0.0540, 0.0458, 0.0640, 0.0903, 0.0373, 0.0353, 0.0134,\n","         0.0297, 0.0104, 0.0368],\n","        [0.0342, 0.0267, 0.0292, 0.0788, 0.0409, 0.0381, 0.0354, 0.0651, 0.0243,\n","         0.0478, 0.0643, 0.0683, 0.0461, 0.0384, 0.0317, 0.0849, 0.0342, 0.0737,\n","         0.0565, 0.0304, 0.0511]], grad_fn=<SelectBackward0>)\n"]}],"source":["output = bert_model(input_ids, attention_mask = attn_mask, output_attentions=True)\n","attentions = output.attentions\n","print(f\"Data type of attentions output: {type(attentions)}\")\n","print(f\"Number of elements: {len(attentions)}\")\n","print(f\"Shape of individual element: {attentions[0].shape}\")\n","print(f\"Example attention map: {attentions[0][0,0]}\")"]},{"cell_type":"markdown","id":"a3a4519b","metadata":{"deletable":false,"editable":false,"id":"a3a4519b","nbgrader":{"cell_type":"markdown","checksum":"6b9a58816d3b1ec81fb7081f80acb009","grade":false,"grade_id":"cell-34a578f52897b7d7","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["As you can see `attentions` is a tuple containing 12 elements which corresponds to the attention maps of each of the 12 layers in the network. Further each layer's attention maps also contains 12 attention maps corresponding to 12 heads in each layer. A single attention map as you can see is a 18x18 matrix representing the attention pattern for all the tokens in the sequence"]},{"cell_type":"markdown","id":"8a79f03f","metadata":{"deletable":false,"editable":false,"id":"8a79f03f","nbgrader":{"cell_type":"markdown","checksum":"345bea080f4368b2a6d87453a22a1edc","grade":false,"grade_id":"cell-c224cab96a9915ec","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Task 2.1: Implementing BERT-based Classifier for Multiple Choice Classification\n","\n","In this task you will implement a bert-based classifier in Pytorch very similar to how we created bag of word classifiers in the previous assignments. The architecture of the model is as follows:\n","\n","![architecture](https://i.ibb.co/hVmS9Qx/siqa-bert-arch-excalli.png)\n","\n","Essentially, what we have here is a model that takes a context and question, and scores a particular answer (denoted as a score(a)). At the backbone we have the BERT model, using which we obtain the contextualized representation of the [context, question, answer] sequence. We then use the \\[CLS\\] token's embedding as the sequence representation and feed it to a 2 layer MLP (Linear(768, 768) -> ReLU -> Linear(768, 1)) that scores the answer. To predict the correct answer, score each of the three answers, obtain their scores and normalize them by applying softmax, that gives us the probability of each option being the correct answer.\n","\n","\n","![forward pass](https://i.ibb.co/r3SrLHY/siqa-bert-forward-excalli.png)\n","\n","Implement the architecture and forward pass in `BertMultiChoiceClassifierModel` class below:"]},{"cell_type":"code","execution_count":27,"id":"0f626042","metadata":{"deletable":false,"id":"0f626042","nbgrader":{"cell_type":"code","checksum":"34a2a6ddf2f67acbfe7fd30857f6de91","grade":false,"grade_id":"cell-7930c03ec3b56775","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1716675502675,"user_tz":-330,"elapsed":9,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["class BertMultiChoiceClassifierModel(nn.Module):\n","\n","    def __init__(self, d_hidden = 768, bert_variant = \"bert-base-uncased\"):\n","        \"\"\"\n","        Define the architecture of Bert-Based mulit-choice classifier.\n","        You will mainly need to define 3 components, first a BERT layer\n","        using `BertModel` from transformers library,\n","        a two layer MLP layer to map the representation from Bert to the output i.e. (Linear(d_hidden, d_hidden) -> ReLU -> Linear(d_hidden, 1)),\n","        and a log sftmax layer to map the scores to a probabilities\n","\n","        Inputs:\n","            - d_hidden (int): Size of the hidden representations of bert\n","            - bert_variant (str): BERT variant to use\n","        \"\"\"\n","        super(BertMultiChoiceClassifierModel, self).__init__()\n","        self.bert_layer = None\n","        self.mlp_layer = None\n","        self.log_softmax_layer = None\n","\n","        # YOUR CODE HERE\n","        self.bert_layer = BertModel.from_pretrained(bert_variant)\n","        self.mlp_layer = nn.Sequential(\n","            nn.Linear(d_hidden, d_hidden),\n","            nn.ReLU(),\n","            nn.Linear(d_hidden, 1)\n","        )\n","        self.log_softmax_layer = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, input_ids_dict, attn_mask_dict):\n","        \"\"\"\n","        Forward Passes the inputs through the network and obtains the prediction\n","\n","        Inputs:\n","            - input_ids_dict (dict(str,torch.tensor)): A dictionary containing input_ids corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n","                                        representing the sequence of token ids\n","            - attn_mask_dict (dict(str,torch.tensor)): A dictionary containing attention mask corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n","\n","        Returns:\n","          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n","\n","\n","        Hints:\n","            1. Recall which of the outputs from BertModel is appropriate for the sentence classification task and how to access it.\n","            2. `torch.cat` might come in handy before performing softmax\n","        \"\"\"\n","        output = None\n","        key_outs = []\n","        # YOUR CODE HERE\n","        for key in input_ids_dict.keys():\n","            bert_output = self.bert_layer(input_ids_dict[key], attention_mask = attn_mask_dict[key])\n","            pooler_output = bert_output.pooler_output\n","            mlp_output = self.mlp_layer(pooler_output)\n","            key_outs.append(mlp_output)\n","\n","        output = self.log_softmax_layer(torch.cat(key_outs, dim=-1))\n","\n","        return output"]},{"cell_type":"code","execution_count":28,"id":"1c65d7c7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"1c65d7c7","nbgrader":{"cell_type":"code","checksum":"35f231082355ea3357feed7ad243b08a","grade":true,"grade_id":"cell-cf9b5db5de53eeac","locked":true,"points":3,"schema_version":3,"solution":false,"task":false},"outputId":"b40665be-3ea0-48e3-86ca-a69297279d7b","executionInfo":{"status":"ok","timestamp":1716675512084,"user_tz":-330,"elapsed":9417,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Cases!\n","Sample Test Case 1\n","Model Output: [[-1.1189674 -1.0885007 -1.0886751]\n"," [-1.1045516 -1.0834142 -1.1080489]\n"," [-1.1027125 -1.0822924 -1.1110513]\n"," [-1.1008493 -1.0936636 -1.1013423]\n"," [-1.0921422 -1.0974907 -1.1062546]\n"," [-1.0798944 -1.1088552 -1.1073539]\n"," [-1.1030428 -1.0939085 -1.0989065]\n"," [-1.0971034 -1.097092  -1.1016482]\n"," [-1.131921  -1.0825678 -1.082162 ]\n"," [-1.0961349 -1.1014836 -1.0982255]\n"," [-1.0979307 -1.0836825 -1.1144607]\n"," [-1.1034715 -1.0959276 -1.0964555]\n"," [-1.1019452 -1.0958116 -1.0980899]\n"," [-1.1050866 -1.098639  -1.0921534]\n"," [-1.1013198 -1.0821338 -1.112621 ]\n"," [-1.1027979 -1.0906712 -1.1024152]]\n","Expected Output: [[-1.1189675 -1.0885007 -1.0886753]\n"," [-1.1045516 -1.0834142 -1.108049 ]\n"," [-1.1027125 -1.0822924 -1.1110513]\n"," [-1.1008494 -1.0936636 -1.1013424]\n"," [-1.0921422 -1.0974907 -1.1062546]\n"," [-1.0798943 -1.1088552 -1.1073538]\n"," [-1.1030427 -1.0939085 -1.0989065]\n"," [-1.0971034 -1.097092  -1.1016482]\n"," [-1.131921  -1.0825679 -1.0821619]\n"," [-1.0961349 -1.1014836 -1.0982255]\n"," [-1.0979307 -1.0836827 -1.1144608]\n"," [-1.1034715 -1.0959275 -1.0964555]\n"," [-1.1019452 -1.0958116 -1.0980899]\n"," [-1.1050864 -1.0986389 -1.0921533]\n"," [-1.1013198 -1.0821339 -1.112621 ]\n"," [-1.1027979 -1.0906712 -1.1024152]]\n","Test Case Passed! :)\n","******************************\n","\n","Sample Test Case 2\n","Model Output: [[-1.100536  -1.1009305 -1.0943841]\n"," [-1.0732511 -1.1178819 -1.1052346]\n"," [-1.1025076 -1.094363  -1.0989832]\n"," [-1.1236264 -1.1056151 -1.0674216]\n"," [-1.0999551 -1.1014045 -1.0944904]\n"," [-1.0953273 -1.0959654 -1.1045709]\n"," [-1.1084402 -1.0971688 -1.0903118]\n"," [-1.099349  -1.1130908 -1.0836148]\n"," [-1.1031718 -1.0897288 -1.1029954]\n"," [-1.0929244 -1.1077557 -1.0952207]\n"," [-1.0995091 -1.0998485 -1.0964826]\n"," [-1.1419647 -1.1081929 -1.0479566]\n"," [-1.1052557 -1.0851237 -1.1055952]\n"," [-1.0840429 -1.1084775 -1.1034834]\n"," [-1.0872697 -1.1025087 -1.1061593]\n"," [-1.1060573 -1.0939908 -1.0958312]]\n","Expected Output: [[-1.1005359 -1.1009303 -1.094384 ]\n"," [-1.073251  -1.1178819 -1.1052346]\n"," [-1.1025076 -1.094363  -1.098983 ]\n"," [-1.1236262 -1.1056151 -1.0674216]\n"," [-1.0999551 -1.1014045 -1.0944905]\n"," [-1.0953273 -1.0959654 -1.1045709]\n"," [-1.1084402 -1.0971687 -1.0903118]\n"," [-1.099349  -1.1130908 -1.0836148]\n"," [-1.1031718 -1.0897288 -1.1029954]\n"," [-1.0929244 -1.1077557 -1.0952206]\n"," [-1.0995092 -1.0998485 -1.0964826]\n"," [-1.1419646 -1.1081928 -1.0479565]\n"," [-1.1052557 -1.0851235 -1.1055952]\n"," [-1.0840428 -1.1084775 -1.1034834]\n"," [-1.0872697 -1.1025085 -1.1061592]\n"," [-1.1060572 -1.0939908 -1.095831 ]]\n","Test Case Passed! :)\n","******************************\n","\n"]}],"source":["print(f\"Running Sample Test Cases!\")\n","torch.manual_seed(42)\n","model = BertMultiChoiceClassifierModel()\n","\n","print(\"Sample Test Case 1\")\n","batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n","bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n","expected_bert_out = np.array([[-1.1189675, -1.0885007, -1.0886753],\n","                            [-1.1045516, -1.0834142, -1.108049 ],\n","                            [-1.1027125, -1.0822924, -1.1110513],\n","                            [-1.1008494, -1.0936636, -1.1013424],\n","                            [-1.0921422, -1.0974907, -1.1062546],\n","                            [-1.0798943, -1.1088552, -1.1073538],\n","                            [-1.1030427, -1.0939085, -1.0989065],\n","                            [-1.0971034, -1.097092 , -1.1016482],\n","                            [-1.131921 , -1.0825679, -1.0821619],\n","                            [-1.0961349, -1.1014836, -1.0982255],\n","                            [-1.0979307, -1.0836827, -1.1144608],\n","                            [-1.1034715, -1.0959275, -1.0964555],\n","                            [-1.1019452, -1.0958116, -1.0980899],\n","                            [-1.1050864, -1.0986389, -1.0921533],\n","                            [-1.1013198, -1.0821339, -1.112621 ],\n","                            [-1.1027979, -1.0906712, -1.1024152]],)\n","print(f\"Model Output: {bert_out}\")\n","print(f\"Expected Output: {expected_bert_out}\")\n","\n","assert bert_out.shape == expected_bert_out.shape\n","assert np.allclose(bert_out, expected_bert_out, 1e-4)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")\n","\n","print(\"Sample Test Case 2\")\n","batch_input_ids, batch_attn_mask, batch_labels = next(iter(dev_loader))\n","bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n","expected_bert_out = np.array([[-1.1005359, -1.1009303, -1.094384 ],\n","                            [-1.073251 , -1.1178819, -1.1052346],\n","                            [-1.1025076, -1.094363 , -1.098983 ],\n","                            [-1.1236262, -1.1056151, -1.0674216],\n","                            [-1.0999551, -1.1014045, -1.0944905],\n","                            [-1.0953273, -1.0959654, -1.1045709],\n","                            [-1.1084402, -1.0971687, -1.0903118],\n","                            [-1.099349 , -1.1130908, -1.0836148],\n","                            [-1.1031718, -1.0897288, -1.1029954],\n","                            [-1.0929244, -1.1077557, -1.0952206],\n","                            [-1.0995092, -1.0998485, -1.0964826],\n","                            [-1.1419646, -1.1081928, -1.0479565],\n","                            [-1.1052557, -1.0851235, -1.1055952],\n","                            [-1.0840428, -1.1084775, -1.1034834],\n","                            [-1.0872697, -1.1025085, -1.1061592],\n","                            [-1.1060572, -1.0939908, -1.095831 ]])\n","print(f\"Model Output: {bert_out}\")\n","print(f\"Expected Output: {expected_bert_out}\")\n","\n","assert bert_out.shape == expected_bert_out.shape\n","assert np.allclose(bert_out, expected_bert_out, 1e-4)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")"]},{"cell_type":"markdown","id":"f38f523b","metadata":{"deletable":false,"editable":false,"id":"f38f523b","nbgrader":{"cell_type":"markdown","checksum":"b96506abcb89deafbdc245e7e22a0509","grade":false,"grade_id":"cell-d26d1e16847283ff","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Task 2.2: Training and Evaluating the Model\n","\n","Now that we have implemented the custom Dataset and a BERT based classifier model, we can start training and evaluating the model. This time we will modify the training loop slightly. At the end of each training epoch we will now evaluate on the validation data and check the accuracy. Based on this we will select the best model across the epochs that obtains highest validation accuracy. You will need to implement the `train` and `evaluate` functions below."]},{"cell_type":"code","execution_count":29,"id":"af1e43b1","metadata":{"deletable":false,"id":"af1e43b1","nbgrader":{"cell_type":"code","checksum":"fce28e32786d4f34b17a1826f752e3a2","grade":true,"grade_id":"cell-37de7065d6cb7392","locked":false,"points":3,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1716675512084,"user_tz":-330,"elapsed":14,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["def evaluate(model, test_dataloader, device = \"cpu\"):\n","    \"\"\"\n","    Evaluates `model` on test dataset\n","\n","    Inputs:\n","        - model (BertMultiChoiceClassifierModel): BERT based multiple choice classifier model to be evaluated\n","        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n","\n","    Returns:\n","        - accuracy (float): Average accuracy over the test dataset\n","    \"\"\"\n","\n","    model.eval()\n","    model = model.to(device)\n","    accuracy = 0\n","\n","\n","    model = model.to(device)\n","    with torch.no_grad():\n","        for test_batch in test_dataloader:\n","\n","            # Read the batch from dataloader\n","            input_ids_dict, attn_mask_dict, labels = test_batch\n","\n","            # Send all values of dicts to device\n","            for key in input_ids_dict.keys():\n","                input_ids_dict[key] = input_ids_dict[key].to(device)\n","                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n","            labels = labels.float().to(device)\n","\n","            # Step 1: Compute model's prediction on the test batch (Note here you need to get the final prediction from the model's output)\n","            preds = None\n","            # YOUR CODE HERE\n","            preds = model(input_ids_dict, attn_mask_dict)\n","\n","            # Step 2: then compute accuracy and store it in batch_accuracy\n","            batch_accuracy = 0\n","            # YOUR CODE HERE - Referred to Assignment 1\n","            preds = torch.argmax(preds, dim=1)\n","            pred_accuracy = torch.sum(preds == labels).item()\n","            batch_accuracy = pred_accuracy / len(labels)\n","\n","            accuracy += batch_accuracy\n","\n","    accuracy = accuracy / len(test_dataloader)\n","    return accuracy\n","\n","\n","def train(model, train_dataloader, val_dataloader,\n","          lr = 1e-5, num_epochs = 3,\n","          device = \"cpu\"):\n","    \"\"\"\n","    Runs the training loop. Define the loss function as BCELoss like the last tine\n","    and optimizer as Adam and traine for `num_epochs` epochs.\n","\n","    Inputs:\n","        - model (BertMultiChoiceClassifierModel): BERT based classifer model to be trained\n","        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n","        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n","        - lr (float): The learning rate for the optimizer\n","        - num_epochs (int): Number of epochs to train the model for.\n","        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n","\n","    Returns:\n","        - best_model (BertMultiChoiceClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n","        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n","    \"\"\"\n","    epoch_loss = 0\n","    model = model.to(device)\n","\n","    best_val_accuracy = float(\"-inf\")\n","    best_model = None\n","\n","    # 1. Define Loss function and optimizer\n","    loss_fn = None\n","    optimizer = None\n","    # YOUR CODE HERE\n","    loss_fn = nn.NLLLoss()\n","    optimizer = Adam(model.parameters(), lr=lr)\n","\n","    # Iterate over `num_epochs`\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0 # We can use this to keep track of how the loss value changes as we train the model.\n","        # Iterate over each batch using the `train_dataloader`\n","        for train_batch in tqdm(train_dataloader):\n","\n","            # Zero out any gradients stored in the previous steps\n","            optimizer.zero_grad()\n","\n","            # Read the batch from dataloader\n","            input_ids_dict, attn_mask_dict, labels = train_batch\n","\n","            # Send all values of dicts to device\n","            for key in input_ids_dict.keys():\n","                input_ids_dict[key] = input_ids_dict[key].to(device)\n","                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n","            labels = labels.to(device)\n","\n","            # Step 3: Feed the input features to the model to get outputs log-probabilities\n","            model_outs = None\n","            # YOUR CODE HERE\n","            model_outs = model(input_ids_dict, attn_mask_dict)\n","\n","            # Step 4: Compute the loss and perform backward pass\n","            loss = None\n","            # YOUR CODE HERE\n","            loss = loss_fn(model_outs, labels)\n","            loss.backward()\n","\n","            # Step 5: Take optimizer step\n","            # YOUR CODE HERE\n","            optimizer.step()\n","\n","            # Store loss value for tracking\n","            epoch_loss += loss.item()\n","\n","        epoch_loss = epoch_loss / len(train_dataloader)\n","        # Step 6. Evaluate on validation data by calling `evaluate` and store the validation accuracy in `val_accurracy`\n","        val_accuracy = 0\n","        # YOUR CODE HERE\n","        val_accuracy = evaluate(model, val_dataloader, device)\n","\n","        # Model selection\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            best_model = copy.deepcopy(model) # Create a copy of model\n","\n","        print(f\"Epoch {epoch} completed | Average Training Loss: {epoch_loss} | Validation Accuracy: {val_accuracy}\")\n","\n","    return best_model, best_val_accuracy"]},{"cell_type":"code","execution_count":30,"id":"b408695c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316,"referenced_widgets":["858c79ab486f4e46a659b2f136523c49","0e30ca4c996e487886292c78c7698bbd","26acb773e49149e8ac2a329c62c961c3","2dc334a023cd41feaa4d46e56138cb47","e8f07b2ff1f441e39e36d48d57e92a75","8c5987a929004ce9aa54e70f6d2d1aff","5e423a9393fa45c88873cf8ee1875896","c353cbfb34d04fe48039a469ce45426f","e13d4324484f43768a637ba7b158d273","ab12374fdddb4a5fbd03e28fe32d9e9b","305cd87e57b84195b333d575f4d56029","19464bd6ed2b447e85801a74127d15a2","12b7aab266754ca196aa1334b53fe33e","8d49759ad68a490c95c160e3d558de4c","e05c07f2ffde483f9e40d4671a3ffec3","7b0e5a9792b84505b340df2191060fdd","6ea50b09dfb54b9dafee339bf265a5b8","16ebb12512c94070a7492a077934c711","59a9c75b80f04ee4b4be078a0b437e2b","92654f9c78904d3699b484ff17d53988","a48382cd013c4ba39ee44c0b54c399cd","c907cc48e0844bc9b654d9824b1c3b7f","d81cfdbe40e44ff080e1e97f6e99f75e","aa4c6b4413394353983ded680f9f4e21","a8e32f02814c4fac884d572a6b952054","7fc31b87f49c4815ab96ab544cf336b2","f749315d228c424d8681b62c2678ab9d","4b7bd703341a47f79f69e8278bc62a40","15aa1ee38f624635b694cb4435b6ec88","5ca6b829711148d8a3eab5f87d083060","4267476ca046440997ba661b1fb627a2","b32c3e6e002f40f4946f97899af94ce2","f3b248811203449c83e3ffa0d04b782d","2ea3ffcbca5d42b983b501edd7af9e47","0bdb5cbe06f34761ab6090ad89ba132d","00286542dbf94f99b86c5955efee82cb","b41c7477bdf042069ca17aa75702de52","92a7570044054d8bb4e45658cc846c4f","c6f766df34374b3c9c917f483f141ab2","2bfa61d9892b461db6f60f5ad1fa7a3b","5692ea92c5ea4d38bf5da33d035c9f36","601ee21615544ee5b8e27e12e0f6d602","3493eb2705114fb981b951aeeb7c68f5","afaeafabda6c4d43b997890fff791500","7b50063428bd47e3a5d3dbaf83e6c1a5","832cc3dee6c24f0ea4fb5dbb6df0cd52","c76a1cb729e946d39ccb78465e327408","0d34142e05b543a58ce081b7f419e739","5048ac58b88e48f6b0dba024eff86596","a4d82521087040a0935d139654522c8c","3dcb582fcf354a709d3b97026351dce7","3ef8dc89a0e04f8591d1669b42032ba3","9c823f20cec14dfb81c9fc07112495ad","f49abab427ee41cb982fb8896e30e6cb","47dacc03d46c42698c8df647da2132c6"]},"deletable":false,"editable":false,"id":"b408695c","nbgrader":{"cell_type":"code","checksum":"6e8f472b79d52b3b88d3ea6a7b0b6e74","grade":true,"grade_id":"cell-f17fa8b3f55ab382","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"outputId":"dc5b9b72-c907-48c6-c550-11b03411f9a5","executionInfo":{"status":"ok","timestamp":1716675537973,"user_tz":-330,"elapsed":25902,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training on 100 data points for sanity check\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"858c79ab486f4e46a659b2f136523c49"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 completed | Average Training Loss: 1.0994049215316772 | Validation Accuracy: 0.78\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19464bd6ed2b447e85801a74127d15a2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1 completed | Average Training Loss: 1.0217101526260377 | Validation Accuracy: 0.77\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81cfdbe40e44ff080e1e97f6e99f75e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 2 completed | Average Training Loss: 0.7112202858924865 | Validation Accuracy: 0.93\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea3ffcbca5d42b983b501edd7af9e47"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 3 completed | Average Training Loss: 0.4205554702877998 | Validation Accuracy: 0.98\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b50063428bd47e3a5d3dbaf83e6c1a5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 4 completed | Average Training Loss: 0.15647668369114398 | Validation Accuracy: 0.98\n","Best Validation Accuracy: 0.98\n","Expected Best Validation Accuracy: 1.0\n"]}],"source":["torch.manual_seed(42)\n","print(\"Training on 100 data points for sanity check\")\n","sample_data = train_data[:100]\n","sample_labels = train_labels[:100]\n","sample_dataset = SIQABertDataset(sample_data, sample_labels)\n","sample_dataloader = DataLoader(sample_dataset, batch_size=4, collate_fn=partial(collate_fn, sample_dataset.tokenizer))\n","\n","model = BertMultiChoiceClassifierModel()\n","best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, num_epochs = 5, device = \"cuda\")\n","print(f\"Best Validation Accuracy: {best_val_acc}\")\n","print(f\"Expected Best Validation Accuracy: {1.0}\")"]},{"cell_type":"markdown","id":"e958b3c7","metadata":{"deletable":false,"editable":false,"id":"e958b3c7","nbgrader":{"cell_type":"markdown","checksum":"dbd0f3b1ded747e9f489e15a0fd7dd8e","grade":false,"grade_id":"cell-e98f8c2ba73c862e","locked":true,"schema_version":3,"solution":false,"task":false}},"source":[" You can expect the validation accuracy of 1.0 by the end of training. This is so high because we trained on just 100 examples and just use those for validation for a sanity check. This is often done to debug the model and training loop. Let's now train on the entire dataset. This can take some time approximately 50 minutes per epoch, since we are fine-tuning all the 12 layers of BERT."]},{"cell_type":"code","execution_count":31,"id":"298b6cf0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["04fda340a0324b538bcb59225d699228","7837000932194042bafb4b0d3150915c","368bbc8dc0f94d70852626c017c7e497","795da76318f24e5e9211d9c51987607e","363086fb9682487ba70720275d51db20","2a53f84b02014e26bfe11040bf8e79c4","f417f966aebb4cf49285662f2bfc53c0","56d19949198c46f8b086bc6bc762ef94","e06dd35685414812829e2cb137252ad6","56f0347513274d77988fb979002d3d5e","475bef04019d4921a319d90036f5c053","ae208d465935440a9364093ec0b5f9e4","6a794d449065452a998a7eb237f1cc83","580c30969bc243c88c564b576ac898ac","5ffb4b2afb3b49bf8c8ba268d45dfce4","90e87ba2110f425f9542ec239fd69476","63be7ed20427469d87f65acbed4c735d","2772b40fda56403b9c1ebc1a27c31520","a53b09f511ee4649b3835e759aa9c4aa","7770b41b7f4349fc9fcb01671f4cb373","a437b1e28a624bfaa6314521eac014bc","43e26859421c4a149688217f28f623c4"]},"deletable":false,"editable":false,"id":"298b6cf0","nbgrader":{"cell_type":"code","checksum":"19c69cac6d134da18698445835172a21","grade":false,"grade_id":"cell-33834a90b1557f37","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"254a4682-bd8d-42ba-b3be-e8f89f0b4330","executionInfo":{"status":"ok","timestamp":1716677231719,"user_tz":-330,"elapsed":1693749,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2089 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04fda340a0324b538bcb59225d699228"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 completed | Average Training Loss: 0.6978719963403681 | Validation Accuracy: 0.6148373983739838\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2089 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae208d465935440a9364093ec0b5f9e4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1 completed | Average Training Loss: 0.4189727396618213 | Validation Accuracy: 0.6016260162601627\n"]}],"source":["model = BertMultiChoiceClassifierModel()\n","best_model, best_val_acc = train(model, train_loader, dev_loader, num_epochs = 2, device = \"cuda\")"]},{"cell_type":"markdown","id":"a397015f","metadata":{"deletable":false,"editable":false,"id":"a397015f","nbgrader":{"cell_type":"markdown","checksum":"48068cd407a033e3d3a83d6f5ffc416f","grade":false,"grade_id":"cell-eb742cd2053baea4","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["You should expect about ~61% validation accuracy (random classifier will have an accuracy of 33%), which is around what's reported in the SocialIQA paper. Note that this is a much more complex task than the news classification that we had in the last lab. You can further improve the performance by using bigger models like bert-base-large or roberta-large."]},{"cell_type":"markdown","id":"95b18060","metadata":{"deletable":false,"editable":false,"id":"95b18060","nbgrader":{"cell_type":"markdown","checksum":"fb7eeb52eab4c3dee032a34d8cab16a8","grade":false,"grade_id":"cell-92df0b4e94279d7b","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Now that we have a model ready for the task, we can save it on disk, so we can use it later (This will come handy for Assignment2)"]},{"cell_type":"code","execution_count":32,"id":"c3168433","metadata":{"deletable":false,"editable":false,"id":"c3168433","nbgrader":{"cell_type":"code","checksum":"3840681b544c16526676ad44bfbbf4d7","grade":false,"grade_id":"cell-58bf127a74f43114","locked":true,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1716677232940,"user_tz":-330,"elapsed":1226,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["# Save the best model\n","save_dir = \"gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/models\"\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","\n","torch.save(best_model.state_dict(), f\"{save_dir}/model.pt\")"]},{"cell_type":"markdown","id":"82f7ea51","metadata":{"deletable":false,"editable":false,"id":"82f7ea51","nbgrader":{"cell_type":"markdown","checksum":"3a3028590f28ef4abb1bba217f555232","grade":false,"grade_id":"cell-28db2cd28da09990","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Task 2.3: Making Predictions from scratch\n","\n","Similar to assignment 1, implement the function `predict_siqa` that takes as input the context, question and answers and runs them through the BERT classifier model to obtain the prediction."]},{"cell_type":"code","execution_count":33,"id":"639651fe","metadata":{"deletable":false,"id":"639651fe","nbgrader":{"cell_type":"code","checksum":"f7d2ad4a45affa789ed2eab66e144df8","grade":false,"grade_id":"cell-8a815364723b18a7","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1716677232941,"user_tz":-330,"elapsed":5,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":["def predict_text(siqa_instance, model, tokenizer,device = \"cpu\"):\n","    \"\"\"\n","    Predicts the correct answer for a piece of a Social IQA instance using the BERT classifier model\n","\n","    Inputs:\n","        - siqa_instance (dict(str, str)): An SIQA instance containing the context, question and the three answer choices.\n","        - model (BertMultiChoiceClassifierModel): Fine-tuned BERT based classifer model\n","        - tokenizer (BertTokenizer): Pre-trained BERT tokenizer\n","    Returns:\n","        - pred_label (float): Predicted answer for `siqa_instance`\n","    \"\"\"\n","\n","    model = model.to(device)\n","    model.eval()\n","\n","    pred_label = None\n","\n","    input_ids_dict = None\n","    attn_mask_dict = None\n","    # Step 1: Tokenize the [sentence, question, answer] triplet using the tokenizer and create input_ids_dict and attn_mask_dict, as done in the Dataset class\n","    # (Don't forget to convert the lists to tensors, torch.Tensor() can come handy or just use return_tensors = \"pt\" while calling the tokenizer)\n","    # YOUR CODE HERE\n","    context = siqa_instance[\"context\"]\n","    question = siqa_instance[\"question\"]\n","    answerA = siqa_instance[\"answerA\"]\n","    answerB = siqa_instance[\"answerB\"]\n","    answerC = siqa_instance[\"answerC\"]\n","\n","    tokenized_input_dict = {\"A\": None, \"B\": None, \"C\": None}\n","\n","    cqaA = context + tokenizer.sep_token + question + tokenizer.sep_token + answerA\n","    cqaB = context + tokenizer.sep_token + question + tokenizer.sep_token + answerB\n","    cqaC = context + tokenizer.sep_token + question + tokenizer.sep_token + answerC\n","\n","    tokenized_input_dict[\"A\"] = tokenizer(cqaA, return_tensors=\"pt\")\n","    tokenized_input_dict[\"B\"] = tokenizer(cqaB, return_tensors=\"pt\")\n","    tokenized_input_dict[\"C\"] = tokenizer(cqaC, return_tensors=\"pt\")\n","\n","\n","    input_ids_dict = {key: tokenized_input_dict[key][\"input_ids\"].to(device) for key in tokenized_input_dict.keys()}\n","    attn_mask_dict = {key: tokenized_input_dict[key][\"attention_mask\"].to(device) for key in tokenized_input_dict.keys()}\n","\n","\n","    # Step 2: Feed the input_ids_dict and attn_mask_dict to the model and get the final predictions\n","    # (Don't forget torch.no_grad())\n","    pred_label = None\n","    # YOUR CODE HERE\n","    with torch.no_grad():\n","        pred_label = model(input_ids_dict, attn_mask_dict)\n","        pred_label = torch.argmax(pred_label, dim=1).item()\n","\n","    # Step 3: Make the predicted human readable i.e. convert 0 to A, 1 to B and 2 to C\n","    pred_label_hr = None\n","    # YOUR CODE HERE\n","    mapping = {0: \"A\", 1: \"B\", 2: \"C\"}\n","    pred_label_hr = mapping[pred_label]\n","\n","    return pred_label_hr"]},{"cell_type":"code","execution_count":34,"id":"b79990de","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["ff477d5a4a9040589ee46cfade9cfbfb","1ca46bbc65b3494fbd3e6203ba942993","f73ce990e61c4521a4f18849c3db4ec1","c2d2538ed07c46c7b14ad31793aeb1d6","64b1edb5053c404582a964e51737ec0e","131d7685cf914e7faee39803b4e2bad5","5d01119c450b496d98ea7d57d14ad974","2eb77ab934db47a5aaf946f7191d1b44","ffbaab52f54e42d8aaa2263499675284","d0dbc0f522fd4242ac9480524cfd724a","967133e7e7e0482c910978b512681c03"]},"deletable":false,"editable":false,"id":"b79990de","nbgrader":{"cell_type":"code","checksum":"4e2ea8703c05b6e64a2fd3574ac83692","grade":false,"grade_id":"cell-2a404905599a658d","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"08ea4224-1beb-488c-f331-2596d78e723c","executionInfo":{"status":"ok","timestamp":1716677298948,"user_tz":-330,"elapsed":66012,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1954 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff477d5a4a9040589ee46cfade9cfbfb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Accuracy by calling `predict_text`: 0.6120777891504606\n","Expected Accuracy: 0.6148373983739838\n","Test Case Passed! :)\n","******************************\n","\n"]}],"source":["print(\"Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\")\n","preds = [\n","    predict_text(siqa_instance, best_model, bert_tokenizer, device = \"cuda\")\n","    for siqa_instance in tqdm(dev_data)\n","]\n","test_case_accuracy = (np.array(preds) == np.array(dev_labels)).mean()\n","print(f\"Accuracy by calling `predict_text`: {test_case_accuracy}\")\n","print(f\"Expected Accuracy: {best_val_acc}\")\n","\n","assert np.allclose(test_case_accuracy, best_val_acc, 1e-2)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")"]},{"cell_type":"code","execution_count":35,"id":"7c969000","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"id":"7c969000","nbgrader":{"cell_type":"code","checksum":"5236adbcc6dc50b20fd6f56c133a3ee0","grade":true,"grade_id":"cell-00c5243a8d59f404","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"outputId":"3ad55a11-6922-4208-f2ae-24bf47075d05","executionInfo":{"status":"ok","timestamp":1716677300222,"user_tz":-330,"elapsed":1278,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{   'context': \"Tracy didn't go home that evening and resisted Riley's \"\n","               'attacks.',\n","    'question': 'What does Tracy need to do before this?',\n","    'answerA': 'make a new plan',\n","    'answerB': 'Go home and see Riley',\n","    'answerC': 'Find somewhere to go'}\n","Predicted Label: C\n","Gold Label: C\n","**********************************\n","\n","{   'context': 'Robin left food out for the animals in her backyard to come '\n","               'and enjoy.',\n","    'question': 'What will Robin want to do next?',\n","    'answerA': 'chase the animals away',\n","    'answerB': 'watch the animals eat',\n","    'answerC': 'go out in the backyard'}\n","Predicted Label: B\n","Gold Label: B\n","**********************************\n","\n","{   'context': 'As usual, Aubrey went to the park but this morning, he met a '\n","               'stranger at the park who jogged with him.',\n","    'question': 'What will Others want to do next?',\n","    'answerA': 'win against Aubrey',\n","    'answerB': 'have a nice jog',\n","    'answerC': 'go to eat'}\n","Predicted Label: B\n","Gold Label: B\n","**********************************\n","\n"]}],"source":["idx = 0\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","print(\"**********************************\\n\")\n","\n","idx = 100\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","print(\"**********************************\\n\")\n","\n","\n","idx = 200\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","\n","print(\"**********************************\\n\")"]},{"cell_type":"code","execution_count":35,"id":"5ef516c5","metadata":{"id":"5ef516c5","executionInfo":{"status":"ok","timestamp":1716677300222,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amal Nair","userId":"10552548839158349771"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1a763aed81e04a11a24a99f7a6e8b55d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b22a80dbefac4657aca9803024c8a94f","IPY_MODEL_2929637c3dab402cb1d1b0f66ea8e7bc","IPY_MODEL_fb39ab4250cb451d8638193aef35e7a2"],"layout":"IPY_MODEL_cff7ca0d469747a1b4723428bd93fa18"}},"b22a80dbefac4657aca9803024c8a94f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21eec649413b4b38866a196bf5f5a4db","placeholder":"â€‹","style":"IPY_MODEL_80103da6c6174393aeaf7e6e949cb5d7","value":"tokenizer_config.json:â€‡100%"}},"2929637c3dab402cb1d1b0f66ea8e7bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09eace6487f24f35bb4daca9ed5c41fa","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ab10cb9c08b4cf7ae6335b960e2c78b","value":48}},"fb39ab4250cb451d8638193aef35e7a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28400d62c4274c3eb7bf5573ed381533","placeholder":"â€‹","style":"IPY_MODEL_5ceb9aed2fe64767aca207f04cec9620","value":"â€‡48.0/48.0â€‡[00:00&lt;00:00,â€‡3.38kB/s]"}},"cff7ca0d469747a1b4723428bd93fa18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21eec649413b4b38866a196bf5f5a4db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80103da6c6174393aeaf7e6e949cb5d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09eace6487f24f35bb4daca9ed5c41fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ab10cb9c08b4cf7ae6335b960e2c78b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28400d62c4274c3eb7bf5573ed381533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ceb9aed2fe64767aca207f04cec9620":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca58e55fa96b45c8a98b02133b99a33c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a09991a4389449ac998bfc25c4de0dbb","IPY_MODEL_1bb291abccea43b9844cb16ec80f5726","IPY_MODEL_7de3e6a9737643d48b5f89fbf98b5198"],"layout":"IPY_MODEL_24958d54d0df496b940b1c6bd53385ae"}},"a09991a4389449ac998bfc25c4de0dbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c28a85c393754c39b799dd8f9162d244","placeholder":"â€‹","style":"IPY_MODEL_477317aedf3345b29f6f47b3f1bb3057","value":"vocab.txt:â€‡100%"}},"1bb291abccea43b9844cb16ec80f5726":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c0cfa87698f400c974bbe0128d39e7d","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2e14bd2934e4a6e9108df9507ef9c35","value":231508}},"7de3e6a9737643d48b5f89fbf98b5198":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a25c01c593f847f89e62666b896e75b8","placeholder":"â€‹","style":"IPY_MODEL_a38c4ece31844358bc41743a32f0b2cb","value":"â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡3.20MB/s]"}},"24958d54d0df496b940b1c6bd53385ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c28a85c393754c39b799dd8f9162d244":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"477317aedf3345b29f6f47b3f1bb3057":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c0cfa87698f400c974bbe0128d39e7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2e14bd2934e4a6e9108df9507ef9c35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a25c01c593f847f89e62666b896e75b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38c4ece31844358bc41743a32f0b2cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be69add35cb741db9d5d1a59ddfd5acc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc9d024b4a1143778c7e1f0aa65d57f1","IPY_MODEL_8b77293e1e4b4fdc85cdfdb1a047cf09","IPY_MODEL_4fddb337d3934c058c0c4211318eb17f"],"layout":"IPY_MODEL_9660205e7d614a5f872cba5ba5531519"}},"fc9d024b4a1143778c7e1f0aa65d57f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad3d65e902524f47954daa8f7f6a19b0","placeholder":"â€‹","style":"IPY_MODEL_da5437b79cbd4d4e928f3e30243f7559","value":"tokenizer.json:â€‡100%"}},"8b77293e1e4b4fdc85cdfdb1a047cf09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88c6926293164ca9afe1d5d38ba13edf","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_811af9d2d6b94f6caf441259472b4e05","value":466062}},"4fddb337d3934c058c0c4211318eb17f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c08155b450a448a69fd79e39791e5571","placeholder":"â€‹","style":"IPY_MODEL_9e280f0408ce4eda9a3de2854f4507e8","value":"â€‡466k/466kâ€‡[00:00&lt;00:00,â€‡2.21MB/s]"}},"9660205e7d614a5f872cba5ba5531519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad3d65e902524f47954daa8f7f6a19b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da5437b79cbd4d4e928f3e30243f7559":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88c6926293164ca9afe1d5d38ba13edf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"811af9d2d6b94f6caf441259472b4e05":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c08155b450a448a69fd79e39791e5571":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e280f0408ce4eda9a3de2854f4507e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d0e0a9d1341482db08155f398f9c861":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5cc22b4b84e4430a31b11b4af8046ad","IPY_MODEL_fb6a51efc60241be88a973d72ef4e57f","IPY_MODEL_8692f10c36d642448767b7f10aab1de2"],"layout":"IPY_MODEL_1bd3e0576a5741fdaee8158f6ff9cfd4"}},"c5cc22b4b84e4430a31b11b4af8046ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8b9abe0f6274fff87c2f3b65db3b330","placeholder":"â€‹","style":"IPY_MODEL_22a51b8b12e045649e97447b388ff9dd","value":"config.json:â€‡100%"}},"fb6a51efc60241be88a973d72ef4e57f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e84acaa3f3c4be78c6d7673fa5e1802","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_492b31fb2cb34aaaa1105476a154801e","value":570}},"8692f10c36d642448767b7f10aab1de2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95430f19bf5b4fe6bf7fdc19a4632321","placeholder":"â€‹","style":"IPY_MODEL_c785f86062824e30a9239e886ef8b2ab","value":"â€‡570/570â€‡[00:00&lt;00:00,â€‡29.4kB/s]"}},"1bd3e0576a5741fdaee8158f6ff9cfd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8b9abe0f6274fff87c2f3b65db3b330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22a51b8b12e045649e97447b388ff9dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e84acaa3f3c4be78c6d7673fa5e1802":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492b31fb2cb34aaaa1105476a154801e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95430f19bf5b4fe6bf7fdc19a4632321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c785f86062824e30a9239e886ef8b2ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69d0ac9c5eb94ea7947e63dc39a8c54f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7dccd63d8d8405bb73cabf84b65f26d","IPY_MODEL_b0bf8818ca5a4c71b310938601292856","IPY_MODEL_38aa388a35e7420b95d0e68bfd1e3b19"],"layout":"IPY_MODEL_9b1832e2fd4e472daeb0d41176517c5a"}},"c7dccd63d8d8405bb73cabf84b65f26d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4761241a06b4c4fb0a0e805829f7c41","placeholder":"â€‹","style":"IPY_MODEL_a0200107131d4f51b7ef248c4a9cfa61","value":"tokenizer_config.json:â€‡100%"}},"b0bf8818ca5a4c71b310938601292856":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61fad1d4767349a391db77661d428209","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26430408343a4caba7b41f5cfef25c63","value":49}},"38aa388a35e7420b95d0e68bfd1e3b19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8926a955a86b425cb5b8c5a9cfb263b9","placeholder":"â€‹","style":"IPY_MODEL_a7c5a420acbb480f85b37d2ec60aed95","value":"â€‡49.0/49.0â€‡[00:00&lt;00:00,â€‡2.31kB/s]"}},"9b1832e2fd4e472daeb0d41176517c5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4761241a06b4c4fb0a0e805829f7c41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0200107131d4f51b7ef248c4a9cfa61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61fad1d4767349a391db77661d428209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26430408343a4caba7b41f5cfef25c63":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8926a955a86b425cb5b8c5a9cfb263b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7c5a420acbb480f85b37d2ec60aed95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"913c42f3c5c2468692565faabae27b71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30149b85e74342518cead936905e7366","IPY_MODEL_5bd65577d7c14fa8a993dd2b9cd10430","IPY_MODEL_9de85c65e96d47b7bc8e2301f05145d4"],"layout":"IPY_MODEL_391685a804014deb9199145acab28489"}},"30149b85e74342518cead936905e7366":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a18cffd954741b9aeac6bfa8a2259c6","placeholder":"â€‹","style":"IPY_MODEL_6d8a19aa89b54a5f868eabb0069dcfe1","value":"vocab.txt:â€‡100%"}},"5bd65577d7c14fa8a993dd2b9cd10430":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_833355d85b634c37827d3e7ee657418f","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f861a9410a664965aa5884ee881bb790","value":213450}},"9de85c65e96d47b7bc8e2301f05145d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cf0288a4343490883eec3dd66bf411a","placeholder":"â€‹","style":"IPY_MODEL_070508c9a4014ea0be8d0feb7d5bfeb3","value":"â€‡213k/213kâ€‡[00:00&lt;00:00,â€‡1.58MB/s]"}},"391685a804014deb9199145acab28489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a18cffd954741b9aeac6bfa8a2259c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d8a19aa89b54a5f868eabb0069dcfe1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"833355d85b634c37827d3e7ee657418f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f861a9410a664965aa5884ee881bb790":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cf0288a4343490883eec3dd66bf411a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"070508c9a4014ea0be8d0feb7d5bfeb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcd2e55e34ab4fc3ae1b0ee77f5c7911":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3ef9245585342bdaa92a091265c5f9f","IPY_MODEL_8f8aafb261ee4f118a89e6945efc0a81","IPY_MODEL_799e8d462f4845b98109ec16defb7edd"],"layout":"IPY_MODEL_0d10cd3ed40746a08c0abfe0e5430186"}},"f3ef9245585342bdaa92a091265c5f9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_095e5730ae384171bf0bbdeaedc4da40","placeholder":"â€‹","style":"IPY_MODEL_6e8995b4ecab45b6880a3013df5514ad","value":"tokenizer.json:â€‡100%"}},"8f8aafb261ee4f118a89e6945efc0a81":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a3f1c4419c74b9baa21a600fff4c21f","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d26436fff32c451bac3f3f229f55c9f6","value":435797}},"799e8d462f4845b98109ec16defb7edd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa8c98ceb45f4b7396b08269c9004d00","placeholder":"â€‹","style":"IPY_MODEL_b437508c19184c3e9492fcabcc98e971","value":"â€‡436k/436kâ€‡[00:00&lt;00:00,â€‡6.88MB/s]"}},"0d10cd3ed40746a08c0abfe0e5430186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"095e5730ae384171bf0bbdeaedc4da40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e8995b4ecab45b6880a3013df5514ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a3f1c4419c74b9baa21a600fff4c21f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d26436fff32c451bac3f3f229f55c9f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa8c98ceb45f4b7396b08269c9004d00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b437508c19184c3e9492fcabcc98e971":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49245a9068a14832b567627565d42bbd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ca59874797e481883b03f35122855ee","IPY_MODEL_49c59b802eb74626bdab5e4053b002d5","IPY_MODEL_1137839b130c448da8ea12d0132edfda"],"layout":"IPY_MODEL_f0c4edb23a1b4cc3821b7b9c16952f08"}},"7ca59874797e481883b03f35122855ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7a77befe9e24135b4cf0910c934129a","placeholder":"â€‹","style":"IPY_MODEL_49ef551e1b1046d9a630312065a2f0c3","value":"config.json:â€‡100%"}},"49c59b802eb74626bdab5e4053b002d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a965f1f0be2f4e109184e5cd20298535","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b80944e9f124db1bdc79d5fb4e76235","value":570}},"1137839b130c448da8ea12d0132edfda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0603735e3f645399a99992987bdc4ed","placeholder":"â€‹","style":"IPY_MODEL_44f21d24bd784d3db529fd16571b8892","value":"â€‡570/570â€‡[00:00&lt;00:00,â€‡44.2kB/s]"}},"f0c4edb23a1b4cc3821b7b9c16952f08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7a77befe9e24135b4cf0910c934129a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49ef551e1b1046d9a630312065a2f0c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a965f1f0be2f4e109184e5cd20298535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b80944e9f124db1bdc79d5fb4e76235":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0603735e3f645399a99992987bdc4ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f21d24bd784d3db529fd16571b8892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aaf0b8f7d354e53a6a60aa8a2b1fa1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec5db08cf21747e2b600ab5ea53ca384","IPY_MODEL_20ebee06a3cc453185cb47cc48a70d2b","IPY_MODEL_0dd528b7d9a5464db8ff75ce53fd0158"],"layout":"IPY_MODEL_d34f043f4cf2446b94072b8a76e8134b"}},"ec5db08cf21747e2b600ab5ea53ca384":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a8b83f74ad14aab9917bebf6ea77d2b","placeholder":"â€‹","style":"IPY_MODEL_c7c3a53e576345b8b6b6ee9a497d01ba","value":"model.safetensors:â€‡100%"}},"20ebee06a3cc453185cb47cc48a70d2b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bd15de28db4447d98b342788f4e3b74","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b163175b6d5426bb83010f69d0f9ccc","value":440449768}},"0dd528b7d9a5464db8ff75ce53fd0158":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6f0423f6b064854a99165e2a0c8b4ac","placeholder":"â€‹","style":"IPY_MODEL_05a5cdd3e99d41ff8388be670bd45ed2","value":"â€‡440M/440Mâ€‡[00:05&lt;00:00,â€‡194MB/s]"}},"d34f043f4cf2446b94072b8a76e8134b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a8b83f74ad14aab9917bebf6ea77d2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7c3a53e576345b8b6b6ee9a497d01ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bd15de28db4447d98b342788f4e3b74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b163175b6d5426bb83010f69d0f9ccc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6f0423f6b064854a99165e2a0c8b4ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05a5cdd3e99d41ff8388be670bd45ed2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"858c79ab486f4e46a659b2f136523c49":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e30ca4c996e487886292c78c7698bbd","IPY_MODEL_26acb773e49149e8ac2a329c62c961c3","IPY_MODEL_2dc334a023cd41feaa4d46e56138cb47"],"layout":"IPY_MODEL_e8f07b2ff1f441e39e36d48d57e92a75"}},"0e30ca4c996e487886292c78c7698bbd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c5987a929004ce9aa54e70f6d2d1aff","placeholder":"â€‹","style":"IPY_MODEL_5e423a9393fa45c88873cf8ee1875896","value":"100%"}},"26acb773e49149e8ac2a329c62c961c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c353cbfb34d04fe48039a469ce45426f","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e13d4324484f43768a637ba7b158d273","value":25}},"2dc334a023cd41feaa4d46e56138cb47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab12374fdddb4a5fbd03e28fe32d9e9b","placeholder":"â€‹","style":"IPY_MODEL_305cd87e57b84195b333d575f4d56029","value":"â€‡25/25â€‡[00:05&lt;00:00,â€‡â€‡6.80it/s]"}},"e8f07b2ff1f441e39e36d48d57e92a75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c5987a929004ce9aa54e70f6d2d1aff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e423a9393fa45c88873cf8ee1875896":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c353cbfb34d04fe48039a469ce45426f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13d4324484f43768a637ba7b158d273":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab12374fdddb4a5fbd03e28fe32d9e9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"305cd87e57b84195b333d575f4d56029":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19464bd6ed2b447e85801a74127d15a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12b7aab266754ca196aa1334b53fe33e","IPY_MODEL_8d49759ad68a490c95c160e3d558de4c","IPY_MODEL_e05c07f2ffde483f9e40d4671a3ffec3"],"layout":"IPY_MODEL_7b0e5a9792b84505b340df2191060fdd"}},"12b7aab266754ca196aa1334b53fe33e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ea50b09dfb54b9dafee339bf265a5b8","placeholder":"â€‹","style":"IPY_MODEL_16ebb12512c94070a7492a077934c711","value":"100%"}},"8d49759ad68a490c95c160e3d558de4c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59a9c75b80f04ee4b4be078a0b437e2b","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92654f9c78904d3699b484ff17d53988","value":25}},"e05c07f2ffde483f9e40d4671a3ffec3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a48382cd013c4ba39ee44c0b54c399cd","placeholder":"â€‹","style":"IPY_MODEL_c907cc48e0844bc9b654d9824b1c3b7f","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.80it/s]"}},"7b0e5a9792b84505b340df2191060fdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ea50b09dfb54b9dafee339bf265a5b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16ebb12512c94070a7492a077934c711":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59a9c75b80f04ee4b4be078a0b437e2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92654f9c78904d3699b484ff17d53988":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a48382cd013c4ba39ee44c0b54c399cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c907cc48e0844bc9b654d9824b1c3b7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d81cfdbe40e44ff080e1e97f6e99f75e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa4c6b4413394353983ded680f9f4e21","IPY_MODEL_a8e32f02814c4fac884d572a6b952054","IPY_MODEL_7fc31b87f49c4815ab96ab544cf336b2"],"layout":"IPY_MODEL_f749315d228c424d8681b62c2678ab9d"}},"aa4c6b4413394353983ded680f9f4e21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b7bd703341a47f79f69e8278bc62a40","placeholder":"â€‹","style":"IPY_MODEL_15aa1ee38f624635b694cb4435b6ec88","value":"100%"}},"a8e32f02814c4fac884d572a6b952054":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ca6b829711148d8a3eab5f87d083060","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4267476ca046440997ba661b1fb627a2","value":25}},"7fc31b87f49c4815ab96ab544cf336b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b32c3e6e002f40f4946f97899af94ce2","placeholder":"â€‹","style":"IPY_MODEL_f3b248811203449c83e3ffa0d04b782d","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡5.98it/s]"}},"f749315d228c424d8681b62c2678ab9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b7bd703341a47f79f69e8278bc62a40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15aa1ee38f624635b694cb4435b6ec88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ca6b829711148d8a3eab5f87d083060":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4267476ca046440997ba661b1fb627a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b32c3e6e002f40f4946f97899af94ce2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3b248811203449c83e3ffa0d04b782d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ea3ffcbca5d42b983b501edd7af9e47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bdb5cbe06f34761ab6090ad89ba132d","IPY_MODEL_00286542dbf94f99b86c5955efee82cb","IPY_MODEL_b41c7477bdf042069ca17aa75702de52"],"layout":"IPY_MODEL_92a7570044054d8bb4e45658cc846c4f"}},"0bdb5cbe06f34761ab6090ad89ba132d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6f766df34374b3c9c917f483f141ab2","placeholder":"â€‹","style":"IPY_MODEL_2bfa61d9892b461db6f60f5ad1fa7a3b","value":"100%"}},"00286542dbf94f99b86c5955efee82cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5692ea92c5ea4d38bf5da33d035c9f36","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_601ee21615544ee5b8e27e12e0f6d602","value":25}},"b41c7477bdf042069ca17aa75702de52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3493eb2705114fb981b951aeeb7c68f5","placeholder":"â€‹","style":"IPY_MODEL_afaeafabda6c4d43b997890fff791500","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.83it/s]"}},"92a7570044054d8bb4e45658cc846c4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6f766df34374b3c9c917f483f141ab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bfa61d9892b461db6f60f5ad1fa7a3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5692ea92c5ea4d38bf5da33d035c9f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"601ee21615544ee5b8e27e12e0f6d602":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3493eb2705114fb981b951aeeb7c68f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afaeafabda6c4d43b997890fff791500":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b50063428bd47e3a5d3dbaf83e6c1a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_832cc3dee6c24f0ea4fb5dbb6df0cd52","IPY_MODEL_c76a1cb729e946d39ccb78465e327408","IPY_MODEL_0d34142e05b543a58ce081b7f419e739"],"layout":"IPY_MODEL_5048ac58b88e48f6b0dba024eff86596"}},"832cc3dee6c24f0ea4fb5dbb6df0cd52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4d82521087040a0935d139654522c8c","placeholder":"â€‹","style":"IPY_MODEL_3dcb582fcf354a709d3b97026351dce7","value":"100%"}},"c76a1cb729e946d39ccb78465e327408":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ef8dc89a0e04f8591d1669b42032ba3","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c823f20cec14dfb81c9fc07112495ad","value":25}},"0d34142e05b543a58ce081b7f419e739":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f49abab427ee41cb982fb8896e30e6cb","placeholder":"â€‹","style":"IPY_MODEL_47dacc03d46c42698c8df647da2132c6","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.80it/s]"}},"5048ac58b88e48f6b0dba024eff86596":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4d82521087040a0935d139654522c8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dcb582fcf354a709d3b97026351dce7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ef8dc89a0e04f8591d1669b42032ba3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c823f20cec14dfb81c9fc07112495ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f49abab427ee41cb982fb8896e30e6cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47dacc03d46c42698c8df647da2132c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04fda340a0324b538bcb59225d699228":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7837000932194042bafb4b0d3150915c","IPY_MODEL_368bbc8dc0f94d70852626c017c7e497","IPY_MODEL_795da76318f24e5e9211d9c51987607e"],"layout":"IPY_MODEL_363086fb9682487ba70720275d51db20"}},"7837000932194042bafb4b0d3150915c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a53f84b02014e26bfe11040bf8e79c4","placeholder":"â€‹","style":"IPY_MODEL_f417f966aebb4cf49285662f2bfc53c0","value":"100%"}},"368bbc8dc0f94d70852626c017c7e497":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56d19949198c46f8b086bc6bc762ef94","max":2089,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e06dd35685414812829e2cb137252ad6","value":2089}},"795da76318f24e5e9211d9c51987607e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56f0347513274d77988fb979002d3d5e","placeholder":"â€‹","style":"IPY_MODEL_475bef04019d4921a319d90036f5c053","value":"â€‡2089/2089â€‡[13:49&lt;00:00,â€‡â€‡3.31it/s]"}},"363086fb9682487ba70720275d51db20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a53f84b02014e26bfe11040bf8e79c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f417f966aebb4cf49285662f2bfc53c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56d19949198c46f8b086bc6bc762ef94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e06dd35685414812829e2cb137252ad6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56f0347513274d77988fb979002d3d5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"475bef04019d4921a319d90036f5c053":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae208d465935440a9364093ec0b5f9e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a794d449065452a998a7eb237f1cc83","IPY_MODEL_580c30969bc243c88c564b576ac898ac","IPY_MODEL_5ffb4b2afb3b49bf8c8ba268d45dfce4"],"layout":"IPY_MODEL_90e87ba2110f425f9542ec239fd69476"}},"6a794d449065452a998a7eb237f1cc83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63be7ed20427469d87f65acbed4c735d","placeholder":"â€‹","style":"IPY_MODEL_2772b40fda56403b9c1ebc1a27c31520","value":"100%"}},"580c30969bc243c88c564b576ac898ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a53b09f511ee4649b3835e759aa9c4aa","max":2089,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7770b41b7f4349fc9fcb01671f4cb373","value":2089}},"5ffb4b2afb3b49bf8c8ba268d45dfce4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a437b1e28a624bfaa6314521eac014bc","placeholder":"â€‹","style":"IPY_MODEL_43e26859421c4a149688217f28f623c4","value":"â€‡2089/2089â€‡[13:48&lt;00:00,â€‡â€‡3.32it/s]"}},"90e87ba2110f425f9542ec239fd69476":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63be7ed20427469d87f65acbed4c735d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2772b40fda56403b9c1ebc1a27c31520":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a53b09f511ee4649b3835e759aa9c4aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7770b41b7f4349fc9fcb01671f4cb373":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a437b1e28a624bfaa6314521eac014bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43e26859421c4a149688217f28f623c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff477d5a4a9040589ee46cfade9cfbfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ca46bbc65b3494fbd3e6203ba942993","IPY_MODEL_f73ce990e61c4521a4f18849c3db4ec1","IPY_MODEL_c2d2538ed07c46c7b14ad31793aeb1d6"],"layout":"IPY_MODEL_64b1edb5053c404582a964e51737ec0e"}},"1ca46bbc65b3494fbd3e6203ba942993":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_131d7685cf914e7faee39803b4e2bad5","placeholder":"â€‹","style":"IPY_MODEL_5d01119c450b496d98ea7d57d14ad974","value":"100%"}},"f73ce990e61c4521a4f18849c3db4ec1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eb77ab934db47a5aaf946f7191d1b44","max":1954,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffbaab52f54e42d8aaa2263499675284","value":1954}},"c2d2538ed07c46c7b14ad31793aeb1d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0dbc0f522fd4242ac9480524cfd724a","placeholder":"â€‹","style":"IPY_MODEL_967133e7e7e0482c910978b512681c03","value":"â€‡1954/1954â€‡[01:05&lt;00:00,â€‡33.58it/s]"}},"64b1edb5053c404582a964e51737ec0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"131d7685cf914e7faee39803b4e2bad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d01119c450b496d98ea7d57d14ad974":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2eb77ab934db47a5aaf946f7191d1b44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffbaab52f54e42d8aaa2263499675284":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0dbc0f522fd4242ac9480524cfd724a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967133e7e7e0482c910978b512681c03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}