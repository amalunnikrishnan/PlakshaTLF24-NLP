{"cells":[{"cell_type":"markdown","id":"19c247ef","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a55d3ed06eee4d3a7da8b7d095623f91","grade":false,"grade_id":"cell-23a7f8d3de35de0b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"19c247ef"},"source":["# Lab 2: Fine-tuning BERT To Perform Common Sense Reasoning\n","\n","## May 13, 2024\n","\n","Welcome to the Lab 2 of our course on Natural Language Processing. As the name suggests in this lab you will learn how to fine-tune a pretrained model like BERT on a downstream task to improve much more superior performance compared to the methods discussed so far. We will be working with the [SocialIQA](https://arxiv.org/abs/1904.09728) dataset this week, which is a multiple choice classification dataset designed to learn and measure social and emotional intelligence in NLP models.\n","\n","\n","This assignment will also make heavy use of the [ðŸ¤— Transformers Library](https://huggingface.co/docs/transformers/index). Don't worry if you are not familiar with the library, we will discuss its usage in detail.\n","\n","Note: Access to a GPU will be crucial for working on this assignment. So do select a GPU runtime in Colab before you start working.\n","\n","Learning Outcomes from this Lab:\n","- Learn how to use ðŸ¤— Transformer library to load and fine-tune pre-trained langauge models\n","- Learn how to solve common sense reasoning problems using Masked Language Models like BERT\n","\n","Suggested Reading:\n","- [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)\n","- [Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense Reasoning about Social Interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463â€“4473, Hong Kong, China. Association for Computational Linguistics.] (https://arxiv.org/pdf/1810.04805.pdf)"]},{"cell_type":"code","execution_count":null,"id":"26c077e7","metadata":{"id":"26c077e7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6dcc2145-c7d9-4b4b-8663-55b250a289fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","siqa_data_dir = \"gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/\" #renamed basis directory organization on my drive."]},{"cell_type":"code","source":["!ls -l gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ah77NB3QsU9","outputId":"e7261401-4da2-42a5-cd2e-102c7a36e18f"},"id":"0ah77NB3QsU9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 8479\n","-rw------- 1 root root  476394 May 13 13:39 dev.jsonl\n","-rw------- 1 root root    5862 May 13 13:39 dev-labels.lst\n","-rw------- 1 root root 8098489 May 13 13:39 train.jsonl\n","-rw------- 1 root root  100230 May 13 13:39 train-labels.lst\n"]}]},{"cell_type":"code","execution_count":null,"id":"3242992d","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9b746d7065e47851c69e7bd3f33db670","grade":false,"grade_id":"cell-00e4313e5ec6522c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"3242992d"},"outputs":[],"source":["# If using Colab, NO NEED TO INSTALL ANYTHING\n","# Install required libraries\n","# !pip install numpy\n","# !pip install pandas\n","# !pip install torch\n","# !pip install tqdm\n","# !pip install matplotlib\n","# !pip install transformers\n","# !pip install scikit-learn\n","# !pip install tqdm"]},{"cell_type":"code","execution_count":null,"id":"9e7b0e64","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6f6521e81b123c3d00a56e3fca57d658","grade":false,"grade_id":"cell-eb2f90d7f62cad18","locked":true,"schema_version":3,"solution":false,"task":false},"id":"9e7b0e64"},"outputs":[],"source":["# We start by importing libraries that we will be making use of in the assignment.\n","import os\n","from functools import partial\n","import json\n","from pprint import pprint\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import copy\n","from tqdm.notebook import tqdm\n","\n","from transformers.utils import logging\n","logging.set_verbosity(40) # to avoid warnings from transformers"]},{"cell_type":"markdown","id":"3c04bc11","metadata":{"id":"3c04bc11"},"source":["## SocialIQA Dataset\n","\n","We start by discussing the dataset that we will making use of in today's Lab. As described above SocialIQA was designed to learn and measure social and emotional intelligence in NLP models. It is a multiple choice classification task, where you are given a context of some social situation, a question about the context and then three possible answers to the questions. The task is to predict which of the three options answers the question given the context.\n","\n","![siqa dataset](https://i.ibb.co/s5tMpY8/siqa.png)\n","\n","Below we load the dataset in memory"]},{"cell_type":"code","execution_count":null,"id":"b3f82e9d","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5d3586a8ea681ba874073f19efd8d10d","grade":false,"grade_id":"cell-61ae8095088b1abc","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b3f82e9d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d80f351f-d3a7-4e32-e56a-648979ea6672"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Training Examples: 33410\n","Number of Validation Examples: 1954\n"]}],"source":["def load_siqa_data(split):\n","\n","    # We first load the file containing context, question and answers\n","    with open(f\"gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/{split}.jsonl\") as f:\n","        data = [json.loads(jline) for jline in f.read().splitlines()]\n","\n","    # We then load the file containing the correct answer for each question\n","    with open(f\"gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev/{split}-labels.lst\") as f:\n","        labels = f.read().splitlines()\n","\n","    labels_dict = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\"}\n","    labels = [labels_dict[label] for label in labels]\n","\n","    return data, labels\n","\n","\n","train_data, train_labels = load_siqa_data(\"train\")\n","dev_data, dev_labels = load_siqa_data(\"dev\")\n","\n","print(f\"Number of Training Examples: {len(train_data)}\")\n","print(f\"Number of Validation Examples: {len(dev_data)}\")"]},{"cell_type":"code","execution_count":null,"id":"3dc04307","metadata":{"id":"3dc04307","colab":{"base_uri":"https://localhost:8080/","height":447},"outputId":"75a83b8d-50a0-42aa-e3a3-3919dce8d243"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Axes: ylabel='count'>"]},"metadata":{},"execution_count":6},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["sns.countplot(x = train_labels)"]},{"cell_type":"code","execution_count":null,"id":"adf24a61","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"70e3ecb51abfd2ca15b0b8f5f24de0e7","grade":false,"grade_id":"cell-602fe165abcf4503","locked":true,"schema_version":3,"solution":false,"task":false},"id":"adf24a61","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3f2d085-2f7a-4935-cda7-62ff327df406"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example from dataset\n","{   'context': \"Jordan's dog peed on the couch they were selling and Jordan \"\n","               'removed the odor as soon as possible.',\n","    'question': 'How would Jordan feel afterwards?',\n","    'answerA': 'selling a couch',\n","    'answerB': 'Disgusted',\n","    'answerC': 'Relieved'}\n","Label: B\n"]}],"source":["# View a sample of the dataset\n","print(\"Example from dataset\")\n","pprint(train_data[100], sort_dicts=False, indent=4)\n","print(f\"Label: {train_labels[100]}\")"]},{"cell_type":"code","execution_count":null,"id":"271a12e6","metadata":{"id":"271a12e6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a64f6701-61d7-407d-ee81-23a2b16d61db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': 'kendall was a person who kept her word so she got my money the other day.',\n"," 'question': 'What will Others want to do next?',\n"," 'answerA': 'resent kendall',\n"," 'answerB': 'support kendall',\n"," 'answerC': 'hate kendall'}"]},"metadata":{},"execution_count":8}],"source":["train_data[500]"]},{"cell_type":"markdown","id":"99a6b904","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"e77cbee4bcad7c097c84525407a83d89","grade":false,"grade_id":"cell-23562a8e437add81","locked":true,"schema_version":3,"solution":false,"task":false},"id":"99a6b904"},"source":["## Task 1: Tokenization and Data Preperation (1 hour)"]},{"cell_type":"markdown","id":"0af7f506","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9634252b2664ab163c234b45f7cac77f","grade":false,"grade_id":"cell-ee0e1d7bb6a346a4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"0af7f506"},"source":["As discussed in the lectures, BERT and other pretrained language models use sub-word tokenization i.e. individual words can also be split into constituent subwords to reduce the vocabulary size. The Transformer library provides tokenizer for all the popular language models. Below we demonstrate how to create and use these tokenizers."]},{"cell_type":"code","execution_count":null,"id":"4e8aa458","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1791efdd95ba6e41368fcf3d0e742e0b","grade":false,"grade_id":"cell-cf4d278c7b6855d7","locked":true,"schema_version":3,"solution":false,"task":false},"id":"4e8aa458","colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["5524eb95b9aa41ecb89d492015b94b6a","10c8f4658c62444c93f016a41a7d12d3","99e375f78e56458eb4408c24ccd13638","4b3a02382f514b6fb3719e38e86fa6ae","a968f3539bdf4f6e9b08d904068f139c","1cc0c67d0259471e8861d3b02be04706","92fab9190f4c425e850a96367ff284dc","d26ce4149f794600b67fd72c97ec5730","5f519fb508154a19bd269a62898fca79","42ef31ce7244430d8e07c16e858ab0db","72873faab0494705818692250ed5a80a","03402e6fc94f4980a998c94b1da88254","f2523e6214214e108b3e6cff5ebfa3f5","e03bdf8dbd6d4598acc33078c5bb6424","88aaf87c98da4a6686a7ed1edad3e4e2","bb7e609b679b410697b3af3617b2a6ec","ad006ea701f7441a9cc16682c6987147","122fcf6f96324d77a73acbc0a6fc3cce","532886af2c644141b185fb55d4bcfe52","14a140f56fd34f008b3662c8db44c90c","bd3e63c937da4e57bcc5c3e03b26cd4e","89ecd1cb5d2440e79a85898c34ade2f0","e34594c4aaa64044935ecf2a807a1a2a","5a80d7dc903d47ba9b2d51623166eafe","a3ebe4648937466ebdced1126c4af567","b5ae0ee0d9da48feb8e78071753956c7","3a856ff7cf7e47508eb72ca1583f9b88","5075d03df46e425585e106ad50fbde90","c563b1a12635423ba03170b54739bd29","c60027d42c8a4175a163c58c518afb9f","1a8be88108f648108c0fa5e715167d35","8edf41106ec846cbb0596a3e3bccf3cc","cfadc104db334d4891f0fe0bb33feba1","da3de40dd9ce452b872a7a6d2ede989e","3b276266af2940b4a064cc95f6096062","f952c79c9ceb4f8b880cc5e1d989c7cf","58b7b597e4904269b83212e83a9971b6","282fbecafeed41a99530dbad9efd4f2b","2f7aab27fe2340b0a6db8a2c1a7f78de","08a228dd7b4e49738efcfe25458143bd","e56be8c8ab8a4975b9311922cc8d41e7","d283e8008d3e45beaa99e1f4e2ce30e1","0c5aae6c1e694764b7caf2f81955c232","0a60bb90de014cf9b6ebb42c97c61e8d"]},"outputId":"9588467c-6682-4866-d291-5a72d74c7c64"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5524eb95b9aa41ecb89d492015b94b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03402e6fc94f4980a998c94b1da88254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e34594c4aaa64044935ecf2a807a1a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3de40dd9ce452b872a7a6d2ede989e"}},"metadata":{}}],"source":["# Import the BertTokenizer from the library\n","from transformers import BertTokenizer\n","\n","# Load a pre-trained BERT Tokenizer\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"markdown","id":"30724776","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ec587bee5c51d19bfe0135af7f8514b4","grade":false,"grade_id":"cell-e5d5e6b75df21d75","locked":true,"schema_version":3,"solution":false,"task":false},"id":"30724776"},"source":["`BertTokenizer.from_pretrained` is used to load a pre-trained tokenizer. Notice that we provide the argument `\"bert-base-uncased\"` to the method. This refers to the variant of BERT that we want to use. The term \"base\" means we want to use the smaller BERT variant i.e. the one with 12 layers, and \"uncased\" refers to the fact that it treats upper-case and lower-case characters identically. There are 4 variants available for BERT which are:\n","    - `bert-base-uncased`\n","    - `bert-base-cased`\n","    - `bert-large-uncased`\n","    - `bert-large-cased`\n","Now that we have loaded the tokenizer, let's see how to use it."]},{"cell_type":"markdown","id":"13523cc5","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"4cce224a12854b52864b4a47c8cf6afc","grade":false,"grade_id":"cell-64c955b108dc7281","locked":true,"schema_version":3,"solution":false,"task":false},"id":"13523cc5"},"source":["`tokenize` method can be used to split the text into sequence of tokens"]},{"cell_type":"code","execution_count":null,"id":"5ccbad09","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b3bbc88079b2d0531a7be74670e9edb6","grade":false,"grade_id":"cell-d232c469d8bd9d8b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5ccbad09","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24c60ca4-3dd7-46e2-9c5c-0e0434cea563"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['kendall',\n"," 'was',\n"," 'a',\n"," 'person',\n"," 'who',\n"," 'kept',\n"," 'her',\n"," 'word',\n"," 'exquisite',\n"," '##ly',\n"," ',',\n"," 'so',\n"," 'she',\n"," 'got',\n"," 'my',\n"," 'money',\n"," 'the',\n"," 'other',\n"," 'day']"]},"metadata":{},"execution_count":10}],"source":["bert_tokenizer.tokenize(\"kendall was a person who kept her word exquisitely, so she got my money the other day\")"]},{"cell_type":"markdown","id":"daced828","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0b2c517ab6b6cf3d2e479b89ce221764","grade":false,"grade_id":"cell-20733e8cd4cc9db8","locked":true,"schema_version":3,"solution":false,"task":false},"id":"daced828"},"source":["Notice how the tokenizer not only splits the text into words but also subwords like \"exquisitely\" is split into \"exquisite\" and \"ly\"."]},{"cell_type":"markdown","id":"d7260263","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"e24402cff8fc0e8a3dcfda8fe8eead78","grade":false,"grade_id":"cell-c2120e5f1d50ce3e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"d7260263"},"source":["Another use case of the tokenizer is to convert the tokens into indices. This is important because BERT and almost all language models takes as the inputs a sequence of token ids, which they use to map into embeddings. `convert_tokens_to_ids` method can be used to do this"]},{"cell_type":"code","execution_count":null,"id":"dbee421b","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7d55893b443134e6f92a1f6620c0f751","grade":false,"grade_id":"cell-3a05f425316cea54","locked":true,"schema_version":3,"solution":false,"task":false},"id":"dbee421b","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2ca4e26-9756-42c9-b1f3-0685e68e3b46"},"outputs":[{"output_type":"stream","name":"stdout","text":["[14509, 2001, 1037, 2711, 2040, 2921, 2014, 2773, 19401, 2135, 1010, 2061, 2016, 2288, 2026, 2769, 1996, 2060, 2154]\n"]}],"source":["sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n","tokens = bert_tokenizer.tokenize(sentence)\n","token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n","print(token_ids)"]},{"cell_type":"markdown","id":"efd54a05","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1e4e23bc6201fae04c7d4fd982288e98","grade":false,"grade_id":"cell-4ee63c56c4ce3e10","locked":true,"schema_version":3,"solution":false,"task":false},"id":"efd54a05"},"source":["The two steps can also be combined by simply calling the tokenizer object"]},{"cell_type":"code","execution_count":null,"id":"bfcbb696","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f1887b57da3c78d52ecf03ce6e405a29","grade":false,"grade_id":"cell-0e587013451e87df","locked":true,"schema_version":3,"solution":false,"task":false},"id":"bfcbb696","colab":{"base_uri":"https://localhost:8080/"},"outputId":"66e59c6d-b12c-42fb-fdca-6d4557f12d36"},"outputs":[{"output_type":"stream","name":"stdout","text":["{   'input_ids': [   101,\n","                     14509,\n","                     2001,\n","                     1037,\n","                     2711,\n","                     2040,\n","                     2921,\n","                     2014,\n","                     2773,\n","                     19401,\n","                     2135,\n","                     1010,\n","                     2061,\n","                     2016,\n","                     2288,\n","                     2026,\n","                     2769,\n","                     1996,\n","                     2060,\n","                     2154,\n","                     102],\n","    'token_type_ids': [   0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0],\n","    'attention_mask': [   1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1]}\n"]}],"source":["pprint(bert_tokenizer(sentence), sort_dicts=False, indent=4)"]},{"cell_type":"markdown","id":"2ab48ddf","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"7b1a833f0f503757099493ad4e6bad0f","grade":false,"grade_id":"cell-19c0f71b49a7786a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2ab48ddf"},"source":["Notice that it returns a bunch of things in addition to the ids. The `\"input_ids\"` are just the token ids that we obtained in the previous cell. However you will notice that it has a few additional ids, it starts with 101 and ends with 102. These are what we call special tokens and correspond the \\[CLS\\] and \\[SEP\\] tokens used by BERT. \\[CLS\\] token is mainly added to beginning of each sequence, and its representations are used to perform sequence classification. More on \\[SEP\\] token later.\n","\n","`\"token_type_ids\"` contains which sequence does a particular token belongs to.\n","\n","`\"attention_mask`\" is a mask vector that indicates if a particular token corresponds to padding. Padding is extremely important when we are dealing with variable length sequences, which is almost always the case. Through padding we can ensure that all the sequences in a batch are of same size. However, while processing the sequence we need ignore these padding tokens, hence a mask is required to identify such tokens.\n","\n","We can tokenize a batch of sequences by just providing a list instead of a string while calling the tokenizer and later pad them using the `.pad` method."]},{"cell_type":"code","execution_count":null,"id":"1d2c0925","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d4a48370450acebae8f9a4b7953eff16","grade":false,"grade_id":"cell-14fe0d9b9608e600","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1d2c0925","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee73791f-59b0-4638-fedf-cf3a24d8fe13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input Ids shape: torch.Size([4, 23])\n","Attention Mask shape: torch.Size([4, 23])\n","('Input Ids:\\n'\n"," ' tensor([[  101,  7232,  2787,  2000,  2031,  1037, 26375,  1998,  5935,  '\n"," '2014,\\n'\n"," '          2814,  2362,  1012,   102,     0,     0,     0,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0],\\n'\n"," '        [  101,  5553,  2734,  2000,  2507,  2041,  5841,  2005,  2019,  '\n"," '9046,\\n'\n"," '          2622,  2012,  2147,  1012,   102,     0,     0,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0],\\n'\n"," '        [  101, 22712,  2001,  2019,  6739, 19949,  1998,  2001,  2006,  '\n"," '1996,\\n'\n"," '          2300,  2007, 11928,  1012, 22712, 17395,  2098, 11928,  1005,  '\n"," '1055,\\n'\n"," '          8103,  1012,   102],\\n'\n"," '        [  101, 18403,  2435,  1037,  8549,  2000, 27970,  1005,  1055,  '\n"," '2365,\\n'\n"," '          2043,  2027,  2020,  3110,  2091,  1012,   102,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0]])\\n')\n","('Attention Mask:\\n'\n"," ' tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, '\n"," '0],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, '\n"," '0],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '\n"," '1],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, '\n"," '0]])\\n')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n"]}],"source":["batch_size = 4\n","sentence_batch = [train_data[i][\"context\"] for i in range(batch_size)]\n","\n","#Tokenize the batch of sequences\n","tokenized_batch = bert_tokenizer(sentence_batch)\n","\n","# Pad the tokenized batch\n","tokenized_batch_padded = bert_tokenizer.pad(tokenized_batch, padding=True, max_length=32, return_tensors=\"pt\")\n","\n","input_ids = tokenized_batch_padded[\"input_ids\"]\n","attn_mask = tokenized_batch_padded[\"attention_mask\"]\n","print(f\"Input Ids shape: {input_ids.shape}\")\n","print(f\"Attention Mask shape: {attn_mask.shape}\")\n","\n","pprint(f\"Input Ids:\\n {input_ids}\\n\")\n","pprint(f\"Attention Mask:\\n {attn_mask}\\n\")"]},{"cell_type":"markdown","id":"fc12c5d0","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"fce3fa2c6deedb93b575d9cb4e28d20f","grade":false,"grade_id":"cell-c42c7d946429adc4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"fc12c5d0"},"source":["Notice how 0s get appended to the input ids sequence, and the same is also reflected in the output of `attn_mask` where `0` indicates that the particular token was padded and `1` means otherwise. Setting `return_tensors=\"pt\"` results in the outputs as torch tensors"]},{"cell_type":"markdown","id":"c4e65595","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"db13d05cd5920e98eeaf5bb761b1075c","grade":false,"grade_id":"cell-673d0a006e2f027a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"c4e65595"},"source":["Finally, for tasks involving reasoning over multiple sentences (like what we have for the SocialIQA dataset), it is common to seperate out each sentence using a \\[SEP\\] token:\n","\n","<img src=\"https://i.ibb.co/Nx8mK1P/bert-sentence-pair.jpg\" alt=\"bert-sentence-pair\" border=\"0\">\n","\n","We can achieve this by adding concatenating all sentences with the `[SEP]` token before calling the tokenizer"]},{"cell_type":"code","execution_count":null,"id":"19f0e7bd","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6cd951f70f83b450715fb2af85c023fe","grade":false,"grade_id":"cell-a7fcef5940c3feb8","locked":true,"schema_version":3,"solution":false,"task":false},"id":"19f0e7bd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"514254a4-cad1-4493-a236-48be3b833eb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n","{   'input_ids': [   101,\n","                     5207,\n","                     1005,\n","                     1055,\n","                     3899,\n","                     21392,\n","                     2094,\n","                     2006,\n","                     1996,\n","                     6411,\n","                     2027,\n","                     2020,\n","                     4855,\n","                     1998,\n","                     5207,\n","                     3718,\n","                     1996,\n","                     19255,\n","                     2004,\n","                     2574,\n","                     2004,\n","                     2825,\n","                     1012,\n","                     102,\n","                     2129,\n","                     2052,\n","                     5207,\n","                     2514,\n","                     5728,\n","                     1029,\n","                     102,\n","                     4855,\n","                     1037,\n","                     6411,\n","                     102],\n","    'token_type_ids': [   0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0],\n","    'attention_mask': [   1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1]}\n"]}],"source":["example = train_data[100]\n","context = example[\"context\"]\n","question = example[\"question\"]\n","answerA = example[\"answerA\"]\n","\n","# Concatenate the context, question and answerA\n","cqa = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n","print(cqa)\n","\n","tokenized_cqa = bert_tokenizer(cqa)\n","pprint(tokenized_cqa, sort_dicts=False, indent=4)"]},{"cell_type":"markdown","id":"db9af625","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f52a28ddb4d75b1276a34f6979c44e06","grade":false,"grade_id":"cell-b6df8e66edd0daed","locked":true,"schema_version":3,"solution":false,"task":false},"id":"db9af625"},"source":["For the reasons that will become clear once we work on the modeling part, we need three input tensors for each dataset example, one for concatenating each answer with the context and question."]},{"cell_type":"code","execution_count":null,"id":"4a8bddb8","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"08e2c414ebaa5ef36cf53cb8bd764c54","grade":false,"grade_id":"cell-01d931a0dfdd5054","locked":true,"schema_version":3,"solution":false,"task":false},"id":"4a8bddb8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e77dc0d4-2ff1-4bd2-a0d4-13a045aa77d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n","Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Disgusted\n","Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Relieved\n"]}],"source":["example = train_data[100]\n","context = example[\"context\"]\n","question = example[\"question\"]\n","answerA = example[\"answerA\"]\n","\n","answerB = example[\"answerB\"]\n","answerC = example[\"answerC\"]\n","\n","cqaA = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n","cqaB = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerB\n","cqaC = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerC\n","\n","print(cqaA)\n","print(cqaB)\n","print(cqaC)\n","\n","tokenized_cqaA = bert_tokenizer(cqaA)\n","tokenized_cqaB = bert_tokenizer(cqaB)\n","tokenized_cqaC = bert_tokenizer(cqaC)\n"]},{"cell_type":"markdown","id":"4627768e","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"32623222e465ee736771be5decbe6f1c","grade":false,"grade_id":"cell-a3cb841c571f2b47","locked":true,"schema_version":3,"solution":false,"task":false},"id":"4627768e"},"source":["## Task 1.1: Custom Dataset Class\n","\n","Now that we know how to use the hugging face tokenizers we can define the custom `torch.utils.Dataset` class like we did in the previous assignments to process and store the data as well as provides a way to iterate through the dataset. Implement the `SIQABertDataset` class below. Recall to create a custom class you need to implement 3 methods `__init__`, `__len__` and `__getitem__`."]},{"cell_type":"code","execution_count":null,"id":"65ffd2ac","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"5b1458f97a435cb685eec31b40452871","grade":false,"grade_id":"cell-84536c665947d263","locked":false,"schema_version":3,"solution":true,"task":false},"id":"65ffd2ac"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class SIQABertDataset(Dataset):\n","\n","    def __init__(self, data, labels, bert_variant = \"bert-base-uncased\"):\n","        \"\"\"\n","        Constructor for the `SST2BertDataset` class. Stores the `sentences` and `labels` which can then be used by\n","        other methods. Also initializes the tokenizer\n","\n","        Inputs:\n","            - data (list) : A list SIQA dataset examples\n","            - labels (list): A list of labels corresponding to each example\n","            - bert_variant (str): A string indicating the variant of BERT to be used.\n","        \"\"\"\n","        self.label2label_id = {\"A\": 0, \"B\": 1, \"C\": 2}\n","        self.data = None\n","        self.labels = None\n","        self.tokenizer = None\n","\n","        # YOUR CODE HERE\n","        self.data = data\n","        self.labels = labels\n","        self.tokenizer = BertTokenizer.from_pretrained(bert_variant)\n","\n","        if (not self.data) or (not self.labels) or (not self.tokenizer):\n","          raise NotImplementedError()\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the length of the dataset\n","        \"\"\"\n","        length = None\n","\n","        # YOUR CODE HERE\n","        length = len(self.data)\n","\n","        if length is None:\n","          raise NotImplementedError()\n","\n","        return length\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns the training example corresponding to review present at the `idx` position in the dataset\n","\n","        Inputs:\n","            - idx (int): Index corresponding to the review,label to be returned\n","\n","        Returns:\n","            - tokenized_input_dict (dict(str, dict)): A dictionary corresponding to tokenizer outputs for the three resulting sequences due to each answer choices as described above\n","            - label (int): Answer label for the corresponding sentence. We will use 0, 1 and 2 to represent A, B and C respectively.\n","\n","        Example Output:\n","            - tokenized_input_dict: {\n","                \"A\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 4855, 1037, 6411, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                \"B\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 17733, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                \"C\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 7653, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","            }\n","            - label: 0\n","\n","        \"\"\"\n","\n","        tokenized_input_dict = {\"A\": None, \"B\": None, \"C\": None}\n","        label = None\n","\n","        # YOUR CODE HERE\n","        example = self.data[idx]\n","        context = example[\"context\"]\n","        question = example[\"question\"]\n","        answerA = example[\"answerA\"]\n","        answerB = example[\"answerB\"]\n","        answerC = example[\"answerC\"]\n","\n","        cqaA = context + self.tokenizer.sep_token + question + self.tokenizer.sep_token + answerA\n","        cqaB = context + self.tokenizer.sep_token + question + self.tokenizer.sep_token + answerB\n","        cqaC = context + self.tokenizer.sep_token + question + self.tokenizer.sep_token + answerC\n","\n","        tokenized_input_dict[\"A\"] = self.tokenizer(cqaA)\n","        tokenized_input_dict[\"B\"] = self.tokenizer(cqaB)\n","        tokenized_input_dict[\"C\"] = self.tokenizer(cqaC)\n","\n","        label = self.label2label_id[self.labels[idx]]\n","\n","        if label is None:\n","          raise NotImplementedError()\n","\n","        return tokenized_input_dict, label"]},{"cell_type":"code","execution_count":null,"id":"7b8c8dea","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"dafc70d40580a2a14cc89778b4a1c88d","grade":true,"grade_id":"cell-0ff4b72642c7bacb","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"7b8c8dea","colab":{"base_uri":"https://localhost:8080/","height":911,"referenced_widgets":["3f30874aaa6942319220a30c2e33186e","f84e37c7d08e48088f212400c7c7ac17","7620707e40ef43858e0b1c93b3e55ec5","ca881374113b4f2896e3edab915f91f6","c7781536b8d6407c8b5a13897d2b5a3e","6737485405ab4adb97f6ecf152e42810","d3f0c313fccf47aabd260057790e0a2f","c50db090dcb249ea9fe2eb1599d77b3d","7a8a8d9279bd473080b786f1e93dcd1f","2dab1f2446e142f8a58aebf2ddedc874","ab244ffeaed14a56be4edbce1c945949","eb0b6ae457234ac1b2f4995dd3ceb9ad","d789df449461469abf2b77420aad6dbb","d0858acc88654cd0a4948fb07717e64c","378868b731db41eebc46435264ff7a02","a7ed509313eb46cc983238c252af547e","10308e7f0cf945d7aafebba0e3cd3c6b","491f99340bbb4a828626276f6d8fc296","7d14aa5a98b84c51a5fd6892627ad160","ef953583191745c1aa38bcb519b0a09a","a822f6faa0ef4740b3ae234bbc6d937b","ea5c5ea247d746039bc558c03858b595","56cd7cb15fb240c9af2b32a138c43d0e","64021529311a475ba7c0e40044dff8ac","d1e89e4b03d145118c088d45943c3cd2","b7e2ab564f534e959c8c098a15fe1549","802ad382721d4d05ab52c86e2588c153","9302baabddd143adbded88fbc7baa92c","de51a9c1ea864bfbaf2c4a74619d0d57","9c9861d208f04ab0879abd24eca92b3d","e912b8f8298048798d8b6b764ce1be08","55b12731c6b240f58823cbf124d095de","71a7074524874e3f8d0b208ed007b5c1","90deb860c51c4915b019ab6e33b95e18","6e2ad4416ece476284259646eb55cb81","b07044bf05e8409cb483c0e4c3a860ae","fe3580db28c84353b8d30af618bb04d0","89bdf4d2134345fda0935f170ac7b4d7","ceba11a4889c46bf92ed4788d3f086d2","0b3dbb7640be433aa09062a12701ad5c","a0da754374ff4f1887d8f4fea680245b","984d8923007840599e50ba9e8a5802f6","59b0af5c251146a5ad759726b1016717","325a1bde9fe44a4e8accf2763d653648"]},"outputId":"5ca784ed-d3d5-45e3-81f9-be4f874bdde3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Cases\n","Sample Test Case 1: Checking if `__len__` is implemented correctly\n","Dataset Length: 2\n","Expected Length: 2\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\n","tokenized_input_dict:\n"," {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 0\n","Expected label:\n"," 0\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\n","tokenized_input_dict:\n"," {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 1\n","Expected label:\n"," 1\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f30874aaa6942319220a30c2e33186e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb0b6ae457234ac1b2f4995dd3ceb9ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56cd7cb15fb240c9af2b32a138c43d0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90deb860c51c4915b019ab6e33b95e18"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["tokenized_input_dict:\n"," {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 0\n","Expected label:\n"," 0\n","Sample Test Case Passed!\n","****************************************\n","\n"]}],"source":["print(\"Running Sample Test Cases\")\n","\n","sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-uncased\")\n","\n","print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n","dataset_len= len(sample_dataset)\n","expected_len = 2\n","print(f\"Dataset Length: {dataset_len}\")\n","print(f\"Expected Length: {expected_len}\")\n","assert len(sample_dataset) == expected_len\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n","sample_idx = 0\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict = {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","  'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","  'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","expected_label = 0\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","\n","print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n","sample_idx = 1\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict =  {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                                  'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                                'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","\n","\n","expected_label = 1\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\")\n","sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-cased\")\n","sample_idx = 0\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict = {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n"," 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n"," 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","expected_label = 0\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n"]},{"cell_type":"markdown","id":"8ec2e35e","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"cc5d11b79dc8589bdd3573c800432f66","grade":false,"grade_id":"cell-cd1f8ca8abf3200c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8ec2e35e"},"source":["We can now create Dataset instances for both training and dev datasets"]},{"cell_type":"code","execution_count":null,"id":"d3994ab0","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"735c155fc2aa10ce309a01ee3633e498","grade":false,"grade_id":"cell-8429b84248f83374","locked":true,"schema_version":3,"solution":false,"task":false},"id":"d3994ab0"},"outputs":[],"source":["train_dataset = SIQABertDataset(train_data, train_labels, bert_variant=\"bert-base-uncased\")\n","dev_dataset = SIQABertDataset(dev_data, dev_labels, bert_variant=\"bert-base-uncased\")"]},{"cell_type":"markdown","id":"fdbc4e1e","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"4be15d36f57f79a7c99c9ac3c6999af7","grade":false,"grade_id":"cell-4c64650b199dc2fc","locked":true,"schema_version":3,"solution":false,"task":false},"id":"fdbc4e1e"},"source":["Before we instantiate the dataloaders for iterating over the dataset like last time, we need define a collate function, that creates batches from a list of dataset examples. In the last class we didn't have to create one, because all of our examples were of the same size, but that's not the case anymore, and we need to pad the sequences so that they all are of same size. We have implemented the collate_fn for you below, but we recommend going through it step by step, as it is used often in practice."]},{"cell_type":"code","execution_count":null,"id":"6ab6fb0b","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a11cdf1b79593a7073b05a78506d044b","grade":false,"grade_id":"cell-9047ce580136ef0f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"6ab6fb0b"},"outputs":[],"source":["def collate_fn(tokenizer, batch):\n","    \"\"\"\n","    Collate function to be used when creating a data loader for the SIQA dataset.\n","    :param tokenizer: The tokenizer to be used to tokenize the inputs.\n","    :param batch: A list of tuples of the form (tokenized_input_dict, label)\n","    :return: A tuple of the form (tokenized_inputs_dict_batch, labels_batch)\n","    \"\"\"\n","\n","    tokenized_inputsA_batch = []\n","    tokenized_inputsB_batch = []\n","    tokenized_inputsC_batch = []\n","    labels_batch = []\n","    for tokenized_inputs_dict, label in batch:\n","        tokenized_inputsA_batch.append(tokenized_inputs_dict[\"A\"])\n","        tokenized_inputsB_batch.append(tokenized_inputs_dict[\"B\"])\n","        tokenized_inputsC_batch.append(tokenized_inputs_dict[\"C\"])\n","        labels_batch.append(label)\n","\n","    #Pad the inputs\n","    tokenized_inputsA_batch = tokenizer.pad(tokenized_inputsA_batch, padding=True, return_tensors=\"pt\")\n","    tokenized_inputsB_batch = tokenizer.pad(tokenized_inputsB_batch, padding=True, return_tensors=\"pt\")\n","    tokenized_inputsC_batch = tokenizer.pad(tokenized_inputsC_batch, padding=True, return_tensors=\"pt\")\n","\n","    # Convert labels list to a tensor\n","    labels_batch = torch.tensor(labels_batch)\n","    return (\n","        {\"A\": tokenized_inputsA_batch[\"input_ids\"], \"B\": tokenized_inputsB_batch[\"input_ids\"], \"C\": tokenized_inputsC_batch[\"input_ids\"]},\n","        {\"A\": tokenized_inputsA_batch[\"attention_mask\"], \"B\": tokenized_inputsB_batch[\"attention_mask\"], \"C\": tokenized_inputsC_batch[\"attention_mask\"]},\n","        labels_batch\n","    )\n"]},{"cell_type":"markdown","id":"2548c845","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"276f9714f43f47be2ce8d32465f0dda0","grade":false,"grade_id":"cell-e25eecdae3525f47","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2548c845"},"source":["Now that we have defined the collate_fn, lets create the dataloaders. It is common to use smaller batch size while fine-tuning these big models, as they occupy quite a lot of memory."]},{"cell_type":"code","execution_count":null,"id":"6c8784f5","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"38520c5b41b69dc2a467c460409e961c","grade":false,"grade_id":"cell-0155f0a92349c779","locked":true,"schema_version":3,"solution":false,"task":false},"id":"6c8784f5"},"outputs":[],"source":["batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, train_dataset.tokenizer))\n","dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, dev_dataset.tokenizer))"]},{"cell_type":"code","execution_count":null,"id":"b9b42884","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7b9f3c4ce63be8d665ca5a5b7ea0f1cc","grade":false,"grade_id":"cell-04007c519041a199","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b9b42884","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e4bfcca4-a0a1-4c59-a21b-4eb5cff0a210"},"outputs":[{"output_type":"stream","name":"stdout","text":["batch_input_ids:\n"," {'A': tensor([[  101, 22712,  2170,  1000,  1996,  2158,  1000,  1010, 14407, 11404,\n","          2046,  1996,  3614,  1012,   102,  2339,  2106, 22712,  2079,  2023,\n","          1029,   102,  2025,  3693,  1996, 11700,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 10555,  2001,  5287,  2000,  2011,  1037,  8606, 17220,  1998,\n","          2150,  2062,  6625,  1012,   102,  2129,  2052, 10555,  2514,  5728,\n","          1029,   102,  8363,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 18961,  2973,  2006,  1996,  4534,  1012,  2016,  2356, 11928,\n","          2005,  2769,  2043,  2016,  2939,  2011,  1012,   102,  2129,  2052,\n","          2017,  6235, 18961,  1029,   102,  1037,  3532,  2711,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  5863,  2170,  1996,  4614,  2043,  2002,  4384,  1996, 13742,\n","          3875,  1996, 21071,  1012,   102,  2054,  2097,  5863,  2215,  2000,\n","          2079,  2279,  1029,   102,  5466,  2019, 11355,  2012,  1996, 28019,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  7546,  3427,  2998,  2006,  2694,  2005,  1037,  2261,  2847,\n","          1998,  2001,  2200,  7622,  1012,   102,  2129,  2052,  7546,  2514,\n","          5728,  1029,   102, 11471,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  3994,  4778,  2037,  2767,  2058,  2000,  3422,  1996,  4164,\n","          2006,  3477,  1011,  2566,  1011,  3193,  1012,   102,  2054,  2097,\n","          3994,  2215,  2000,  2079,  2279,  1029,   102,  2131,  2070, 27962,\n","          1998,  8974,  3201,  2000,  4521,  2096,  2027,  3422,  1996,  2265,\n","           102,     0],\n","        [  101,  2096,  2667,  2000,  8054,  2037,  2814,  2000,  3693,  1996,\n","          2012,  2277,  1010,  6683,  4207,  2014,  9029,  2055,  4676,  1012,\n","           102,  2054,  2097,  4148,  2000,  6683,  1029,   102,  8796,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 22712,  2170,  1996,  2158,  2055,  1996,  3291,  2027,  2020,\n","          2383,  1012,   102,  2054,  2097,  2500,  2215,  2000,  2079,  2279,\n","          1029,   102,  2655,  1996,  2158,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  7546,  2134,  1005,  1056,  3046,  2004,  2524,  2004,  2016,\n","          2288,  1998,  2288,  2353,  2173,  1012,   102,  2129,  2052,  2017,\n","          6235,  7546,  1029,   102, 12774,  1998, 11922,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 18403,  1005,  1055, 10808,  2291,  3844,  2091,  2021,  2027,\n","          2018,  1037, 13788,  1012,   102,  2129,  2052,  2017,  6235, 18403,\n","          1029,   102,  6742,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  9036,  2001,  1040,  7274,  2571,  9048,  2278,  2061, 14509,\n","          2949,  2037,  4646,  2005,  1037,  3105,  2061,  2008,  9036,  2052,\n","          2131,  2019,  4357,  1998,  2265,  2037,  4813,  2059,  1012,   102,\n","          2129,  2052, 14509,  2514,  5728,  1029,   102,  2066,  1037,  2204,\n","          2767,   102],\n","        [  101, 22712,  2165, 27970,  1005,  1055,  2769,  2029, 12781,  2006,\n","          2010,  9715,  4600,  1012,   102,  2129,  2052, 22712,  2514,  5728,\n","          1029,   102,  2062,  4138,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 18403,  2435,  9321,  2298,  2138,  2016,  2001,  2035,  2039,\n","          1999,  2014,  2449,  1012,   102,  2054,  2097, 18403,  2215,  2000,\n","          2079,  2279,  1029,   102,  5060,  2077,  2023,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  9806,  2001,  4394,  2005,  4466,  2847,  1012,  2111,  2020,\n","          2559,  7249,  1012,  9806,  2170,  2188,  1012,   102,  2129,  2052,\n","          2500,  2514,  2004,  1037,  2765,  1029,   102,  9364,  1998, 11471,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  7627,  2001,  2012,  2188,  2007,  2037,  2155,  1010,  1998,\n","          7627,  2170,  2068,  2035,  2046,  1996,  2542,  2282,  1012,   102,\n","          2339,  2106,  7627,  2079,  2023,  1029,   102,  2359,  2000,  3422,\n","          1037,  2694,  2265,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 10555,  2985,  2051,  2000,  5335,  2014,  7022,  1012,   102,\n","          2054,  2097, 10555,  2215,  2000,  2079,  2279,  1029,   102,  2156,\n","          2055,  2082,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]]), 'B': tensor([[  101, 22712,  2170,  1000,  1996,  2158,  1000,  1010, 14407, 11404,\n","          2046,  1996,  3614,  1012,   102,  2339,  2106, 22712,  2079,  2023,\n","          1029,   102,  3693,  1996, 11700,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 10555,  2001,  5287,  2000,  2011,  1037,  8606, 17220,  1998,\n","          2150,  2062,  6625,  1012,   102,  2129,  2052, 10555,  2514,  5728,\n","          1029,   102,  6314,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 18961,  2973,  2006,  1996,  4534,  1012,  2016,  2356, 11928,\n","          2005,  2769,  2043,  2016,  2939,  2011,  1012,   102,  2129,  2052,\n","          2017,  6235, 18961,  1029,   102,  2066,  2027,  2215,  2000,  4468,\n","         18961,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  5863,  2170,  1996,  4614,  2043,  2002,  4384,  1996, 13742,\n","          3875,  1996, 21071,  1012,   102,  2054,  2097,  5863,  2215,  2000,\n","          2079,  2279,  1029,   102,  4604,  2037,  4937,  2000,  1996, 28019,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  7546,  3427,  2998,  2006,  2694,  2005,  1037,  2261,  2847,\n","          1998,  2001,  2200,  7622,  1012,   102,  2129,  2052,  7546,  2514,\n","          5728,  1029,   102,  1999, 15180,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  3994,  4778,  2037,  2767,  2058,  2000,  3422,  1996,  4164,\n","          2006,  3477,  1011,  2566,  1011,  3193,  1012,   102,  2054,  2097,\n","          3994,  2215,  2000,  2079,  2279,  1029,   102,  2377,  1996,  3682,\n","          2096,  2027,  3422,  1996,  4164,   102,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  2096,  2667,  2000,  8054,  2037,  2814,  2000,  3693,  1996,\n","          2012,  2277,  1010,  6683,  4207,  2014,  9029,  2055,  4676,  1012,\n","           102,  2054,  2097,  4148,  2000,  6683,  1029,   102,  2969, 19556,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 22712,  2170,  1996,  2158,  2055,  1996,  3291,  2027,  2020,\n","          2383,  1012,   102,  2054,  2097,  2500,  2215,  2000,  2079,  2279,\n","          1029,   102,  2831,  2000,  1996,  2158,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  7546,  2134,  1005,  1056,  3046,  2004,  2524,  2004,  2016,\n","          2288,  1998,  2288,  2353,  2173,  1012,   102,  2129,  2052,  2017,\n","          6235,  7546,  1029,   102,  4895, 18938, 21967,  1998, 13971,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 18403,  1005,  1055, 10808,  2291,  3844,  2091,  2021,  2027,\n","          2018,  1037, 13788,  1012,   102,  2129,  2052,  2017,  6235, 18403,\n","          1029,   102,  2001,  5580,  2027,  2018,  1037, 13788,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  9036,  2001,  1040,  7274,  2571,  9048,  2278,  2061, 14509,\n","          2949,  2037,  4646,  2005,  1037,  3105,  2061,  2008,  9036,  2052,\n","          2131,  2019,  4357,  1998,  2265,  2037,  4813,  2059,  1012,   102,\n","          2129,  2052, 14509,  2514,  5728,  1029,   102,  2066,  1037, 11809,\n","          2767,   102],\n","        [  101, 22712,  2165, 27970,  1005,  1055,  2769,  2029, 12781,  2006,\n","          2010,  9715,  4600,  1012,   102,  2129,  2052, 22712,  2514,  5728,\n","          1029,   102,  2200,  3407,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 18403,  2435,  9321,  2298,  2138,  2016,  2001,  2035,  2039,\n","          1999,  2014,  2449,  1012,   102,  2054,  2097, 18403,  2215,  2000,\n","          2079,  2279,  1029,   102,  8568,  2014,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  9806,  2001,  4394,  2005,  4466,  2847,  1012,  2111,  2020,\n","          2559,  7249,  1012,  9806,  2170,  2188,  1012,   102,  2129,  2052,\n","          2500,  2514,  2004,  1037,  2765,  1029,   102,  7653,  1998,  8363,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  7627,  2001,  2012,  2188,  2007,  2037,  2155,  1010,  1998,\n","          7627,  2170,  2068,  2035,  2046,  1996,  2542,  2282,  1012,   102,\n","          2339,  2106,  7627,  2079,  2023,  1029,   102,  2018,  2019,  2590,\n","          8874,  2000,  2191,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 10555,  2985,  2051,  2000,  5335,  2014,  7022,  1012,   102,\n","          2054,  2097, 10555,  2215,  2000,  2079,  2279,  1029,   102,  2022,\n","          1037,  2488,  3076,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]]), 'C': tensor([[  101, 22712,  2170,  1000,  1996,  2158,  1000,  1010, 14407, 11404,\n","          2046,  1996,  3614,  1012,   102,  2339,  2106, 22712,  2079,  2023,\n","          1029,   102,  2954,  2068,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101, 10555,  2001,  5287,  2000,  2011,  1037,  8606, 17220,  1998,\n","          2150,  2062,  6625,  1012,   102,  2129,  2052, 10555,  2514,  5728,\n","          1029,   102,  4854,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101, 18961,  2973,  2006,  1996,  4534,  1012,  2016,  2356, 11928,\n","          2005,  2769,  2043,  2016,  2939,  2011,  1012,   102,  2129,  2052,\n","          2017,  6235, 18961,  1029,   102,  1037,  2204,  6926,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101,  5863,  2170,  1996,  4614,  2043,  2002,  4384,  1996, 13742,\n","          3875,  1996, 21071,  1012,   102,  2054,  2097,  5863,  2215,  2000,\n","          2079,  2279,  1029,   102, 10574,  1996, 10345,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101,  7546,  3427,  2998,  2006,  2694,  2005,  1037,  2261,  2847,\n","          1998,  2001,  2200,  7622,  1012,   102,  2129,  2052,  7546,  2514,\n","          5728,  1029,   102,  7777,  2998,  1037,  2843,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101,  3994,  4778,  2037,  2767,  2058,  2000,  3422,  1996,  4164,\n","          2006,  3477,  1011,  2566,  1011,  3193,  1012,   102,  2054,  2097,\n","          3994,  2215,  2000,  2079,  2279,  1029,   102,  5342,  2007,  2035,\n","          1996,  4597,  2125,  1998, 14694,  2701,  2043,  2014,  2767, 21145,\n","          2006,  1996,  2341,   102],\n","        [  101,  2096,  2667,  2000,  8054,  2037,  2814,  2000,  3693,  1996,\n","          2012,  2277,  1010,  6683,  4207,  2014,  9029,  2055,  4676,  1012,\n","           102,  2054,  2097,  4148,  2000,  6683,  1029,   102,  6848,  4676,\n","          2007,  2014,  2814,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101, 22712,  2170,  1996,  2158,  2055,  1996,  3291,  2027,  2020,\n","          2383,  1012,   102,  2054,  2097,  2500,  2215,  2000,  2079,  2279,\n","          1029,   102,  9611,  1996,  3291,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101,  7546,  2134,  1005,  1056,  3046,  2004,  2524,  2004,  2016,\n","          2288,  1998,  2288,  2353,  2173,  1012,   102,  2129,  2052,  2017,\n","          6235,  7546,  1029,   102, 12774,  1998,  2844,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101, 18403,  1005,  1055, 10808,  2291,  3844,  2091,  2021,  2027,\n","          2018,  1037, 13788,  1012,   102,  2129,  2052,  2017,  6235, 18403,\n","          1029,   102,  2001,  8794,  2027,  2018,  1037, 13788,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101,  9036,  2001,  1040,  7274,  2571,  9048,  2278,  2061, 14509,\n","          2949,  2037,  4646,  2005,  1037,  3105,  2061,  2008,  9036,  2052,\n","          2131,  2019,  4357,  1998,  2265,  2037,  4813,  2059,  1012,   102,\n","          2129,  2052, 14509,  2514,  5728,  1029,   102,  2200, 10791,   102,\n","             0,     0,     0,     0],\n","        [  101, 22712,  2165, 27970,  1005,  1055,  2769,  2029, 12781,  2006,\n","          2010,  9715,  4600,  1012,   102,  2129,  2052, 22712,  2514,  5728,\n","          1029,   102,  2200,  5905,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101, 18403,  2435,  9321,  2298,  2138,  2016,  2001,  2035,  2039,\n","          1999,  2014,  2449,  1012,   102,  2054,  2097, 18403,  2215,  2000,\n","          2079,  2279,  1029,   102,  2131,  1037,  2047,  2767,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101,  9806,  2001,  4394,  2005,  4466,  2847,  1012,  2111,  2020,\n","          2559,  7249,  1012,  9806,  2170,  2188,  1012,   102,  2129,  2052,\n","          2500,  2514,  2004,  1037,  2765,  1029,   102,  5506,  2012,  9806,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101,  7627,  2001,  2012,  2188,  2007,  2037,  2155,  1010,  1998,\n","          7627,  2170,  2068,  2035,  2046,  1996,  2542,  2282,  1012,   102,\n","          2339,  2106,  7627,  2079,  2023,  1029,   102,  2359,  2000,  2022,\n","          2187,  2894,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0],\n","        [  101, 10555,  2985,  2051,  2000,  5335,  2014,  7022,  1012,   102,\n","          2054,  2097, 10555,  2215,  2000,  2079,  2279,  1029,   102,  2156,\n","          2065,  2009,  3271,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0]])}\n","batch_attn_mask:\n"," {'A': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'B': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'C': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","batch_labels:\n"," tensor([2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 0, 2, 1, 1, 1, 2])\n"]}],"source":["batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n","print(f\"batch_input_ids:\\n {batch_input_ids}\")\n","print(f\"batch_attn_mask:\\n {batch_attn_mask}\")\n","print(f\"batch_labels:\\n {batch_labels}\")"]},{"cell_type":"markdown","id":"849edfc0","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"7f5abac97eb62041f0add8617ad42871","grade":false,"grade_id":"cell-21c75958035402dc","locked":true,"schema_version":3,"solution":false,"task":false},"id":"849edfc0"},"source":["## Task 2: Implementing and Training BERT-based Multiple Choice Classifier (1 hour 30 minutes)\n","\n","Similar to pretrained tokenizers, the transformers library also provide numerous pre-trained language models that can be fine-tuned on a wide variety of downstream tasks. We demonstrate usage of these models below."]},{"cell_type":"code","execution_count":null,"id":"c9322600","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"292023c90e82c1cc3836a7c39718fb03","grade":false,"grade_id":"cell-0518264e94de005b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"c9322600","colab":{"base_uri":"https://localhost:8080/","height":760,"referenced_widgets":["0d6e695936b14c2eb56a9d6d3810c92e","25e1fd955cea4ae8b32d4da6421af3f9","fd35e7e6e95041888eca1feb43d9a403","6ea8584d164742a5b69aad5c9a6a06ea","bef3a48f2b5f4805940d7e69575f8847","a1544a6a865d4ca0905c49b697e80f9b","91b65a8139f8432b9d4f00555646820e","722c87cd167e4479bfba9fdcefaddf9a","5baa581239164df9bdba9fd4c4ea2673","dc47abbfd1de4770883a1aea21c97f22","6ad8a2b91f9745daaf4a328bb58884fa"]},"outputId":"f06605ee-a649-449e-d1dd-e1a2029ca6d2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d6e695936b14c2eb56a9d6d3810c92e"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":22}],"source":["# Import BertModel from the library\n","from transformers import BertModel\n","\n","# Create an instance of pretrained BERT\n","bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n","bert_model"]},{"cell_type":"markdown","id":"4a7f074b","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"98a82e5b7717eda4f5226c6ae269fe58","grade":false,"grade_id":"cell-ad0d391f24e70315","locked":true,"schema_version":3,"solution":false,"task":false},"id":"4a7f074b"},"source":["As you can see very similar to how we created pre-trained tokenizer, we can load a pretrained BERT model by calling `BertModel.from_pretrained(bert-base-uncased)`. This can actually be considered just a Pytorch `nn.Module` like `nn.Linear` and can be similarly plugged into a network architecture. Also, notice the model contains 12 BERT layers, where each layer consists of a Self Attention layer followed by a sequence of linear layers and activation functions (MLP), as we discussed when talking about Transformer architecture in the lecture."]},{"cell_type":"code","execution_count":null,"id":"c994b139","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"ac922a7cea63724c27c0cf5c6f00ad83","grade":false,"grade_id":"cell-44c7f2f43d5ba025","locked":true,"schema_version":3,"solution":false,"task":false},"id":"c994b139","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e16836f6-6928-4ad1-ec98-c4eb64d05597"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2823, -0.2353, -0.3529,  ..., -0.0834,  0.2548,  0.4870],\n","         [ 0.4055, -1.1768, -0.2842,  ..., -0.3740,  0.3920, -0.4480],\n","         [ 0.0377, -0.7788, -0.1174,  ..., -0.4201, -0.3078,  0.1824],\n","         ...,\n","         [-1.1595, -1.5650, -0.2526,  ..., -0.4569, -0.5474,  0.2315],\n","         [-1.0644, -0.5952, -0.3912,  ...,  0.2788, -0.0207, -0.1262],\n","         [ 0.5158,  0.4573,  0.0263,  ...,  0.1445, -0.6398, -0.5258]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1342e-01, -3.3350e-01, -4.4551e-01,  4.5434e-01,  5.0347e-01,\n","         -8.4619e-02,  4.8077e-01,  8.4160e-02, -2.2890e-01, -9.9974e-01,\n","         -2.4549e-01,  7.9481e-01,  9.8030e-01, -2.8880e-02,  9.1588e-01,\n","         -2.4136e-01,  5.3666e-01, -5.0656e-01,  2.2218e-01,  3.2123e-01,\n","          6.4942e-01,  9.9757e-01,  4.7173e-01,  2.0766e-01,  3.6500e-01,\n","          8.8725e-01, -4.2194e-01,  9.3109e-01,  9.1743e-01,  7.2114e-01,\n","         -1.4355e-02, -1.8941e-02, -9.8951e-01,  3.0721e-02, -2.8992e-01,\n","         -9.8191e-01,  1.7840e-01, -5.9347e-01,  1.1879e-01,  2.5965e-01,\n","         -8.6462e-01,  1.4619e-01,  9.9952e-01, -4.7740e-01,  1.0605e-01,\n","         -1.2106e-01, -9.9986e-01,  2.2875e-01, -8.5771e-01,  5.0007e-01,\n","          2.7700e-01,  7.1537e-01,  9.0395e-02,  3.0770e-01,  3.6540e-01,\n","         -7.2003e-02, -2.7237e-01, -2.9246e-02, -2.4881e-01, -4.1984e-01,\n","         -6.4784e-01,  1.4369e-01, -5.4503e-01, -7.8703e-01,  4.1153e-01,\n","          2.9400e-01, -1.3110e-01, -8.3965e-02,  6.9508e-03, -1.4154e-01,\n","          6.8059e-01,  8.8732e-02,  1.5826e-02, -8.0095e-01,  1.2098e-01,\n","          1.5737e-01, -5.1418e-01,  1.0000e+00,  4.2994e-02, -9.8423e-01,\n","          1.7809e-01,  2.3727e-01,  3.4371e-01,  4.4902e-01, -2.5424e-01,\n","         -1.0000e+00,  3.7378e-01, -2.9287e-03, -9.9117e-01,  1.0483e-01,\n","          4.8637e-01, -2.1140e-01, -1.3496e-01,  4.0792e-01,  5.8095e-02,\n","         -1.5223e-01, -2.3065e-01, -4.2501e-01, -1.0952e-01, -1.6091e-01,\n","          2.2054e-01,  7.6145e-02, -4.6461e-02, -2.1545e-01,  2.0504e-01,\n","         -4.3479e-01,  2.3062e-02,  4.2032e-01, -3.0370e-01,  5.1830e-01,\n","          3.7295e-01, -1.9882e-01,  2.8470e-01, -9.4566e-01,  3.9429e-01,\n","         -2.5140e-01, -9.8011e-01, -4.6321e-01, -9.9085e-01,  6.2031e-01,\n","          3.0362e-02, -2.4470e-01,  9.4980e-01,  4.4613e-01,  1.0056e-01,\n","          1.2657e-01, -4.9239e-01, -1.0000e+00, -3.8979e-01,  1.7306e-02,\n","          6.2905e-02,  7.5699e-03, -9.6858e-01, -9.6126e-01,  3.2523e-01,\n","          9.4696e-01,  2.4653e-02,  9.9795e-01, -1.0900e-01,  9.2609e-01,\n","          2.7076e-01, -2.3979e-01,  2.4420e-03, -4.1432e-01,  3.1651e-01,\n","         -2.5690e-01, -5.1866e-02,  1.3838e-01, -1.3608e-01,  1.5090e-02,\n","         -2.7091e-01,  1.0166e-02, -6.0731e-02, -9.0828e-01, -2.3963e-01,\n","          9.5270e-01, -1.2421e-01, -5.1564e-01,  4.5463e-01, -9.4697e-02,\n","          1.9799e-02,  6.7040e-01,  3.5773e-01,  3.0287e-01, -3.0182e-01,\n","          3.2714e-01, -3.3120e-01,  4.4667e-01, -5.9530e-01,  3.9171e-01,\n","          2.3601e-01, -1.9295e-01, -1.1556e-01, -9.7889e-01, -2.1943e-01,\n","          1.7873e-01,  9.8326e-01,  6.1193e-01,  1.2009e-01,  4.6305e-01,\n","         -2.2367e-01,  5.1362e-01, -9.4201e-01,  9.8198e-01,  2.3595e-02,\n","          1.6000e-01, -1.6324e-01,  2.4670e-01, -8.0511e-01, -3.3924e-01,\n","          4.8846e-01, -5.1100e-01, -7.3509e-01,  4.6975e-02, -3.3179e-01,\n","         -1.8198e-01, -4.1462e-01,  1.0465e-01, -2.1432e-01, -3.4277e-01,\n","          9.7266e-02,  9.3661e-01,  7.2161e-01,  4.8405e-01, -3.6871e-01,\n","          3.1814e-01, -8.4417e-01, -4.6677e-01,  2.3358e-02,  8.2281e-02,\n","         -3.5102e-02,  9.9018e-01, -2.8120e-01,  1.5138e-01, -8.7715e-01,\n","         -9.8174e-01, -1.8074e-01, -8.3024e-01, -1.6251e-01, -4.6465e-01,\n","          4.1388e-01, -5.0448e-01,  7.9151e-03,  1.6464e-01, -8.4324e-01,\n","         -5.9927e-01,  3.1949e-01, -1.7097e-01,  3.2296e-01, -2.7389e-01,\n","          9.0340e-01,  6.0541e-01, -4.7819e-01, -3.6939e-01,  9.1910e-01,\n","         -2.9349e-01, -7.2003e-01,  3.8927e-01, -1.0532e-01,  5.2019e-01,\n","         -4.1045e-01,  9.4339e-01,  6.6610e-01,  4.9983e-01, -8.7163e-01,\n","          1.0599e-02, -6.2514e-01,  5.1351e-02,  7.7707e-04, -5.0177e-01,\n","          2.8171e-01,  4.3400e-01,  2.9356e-01,  7.4288e-01, -4.1969e-02,\n","          8.6796e-01, -9.1876e-01, -9.4036e-01, -7.8274e-01,  9.0406e-02,\n","         -9.8649e-01,  1.0411e-01,  1.7797e-01, -1.0403e-01, -2.1737e-01,\n","         -1.7412e-01, -9.4650e-01,  3.6575e-01, -4.9280e-02,  9.1307e-01,\n","         -3.6831e-01, -6.6574e-01, -4.2411e-01, -9.2306e-01, -2.2497e-01,\n","         -1.3000e-01,  6.1594e-02, -1.6496e-01, -9.4146e-01,  4.3231e-01,\n","          4.3532e-01,  4.4700e-01, -1.5150e-01,  9.6835e-01,  9.9996e-01,\n","          9.6499e-01,  8.9856e-01,  4.0772e-01, -9.9106e-01, -7.2477e-01,\n","          9.9990e-01, -8.8650e-01, -9.9999e-01, -8.8209e-01, -3.2363e-01,\n","         -1.0676e-02, -1.0000e+00, -6.8102e-02,  2.3007e-01, -8.1374e-01,\n","          1.3577e-01,  9.7153e-01,  9.1199e-01, -1.0000e+00,  7.6972e-01,\n","          9.3895e-01, -4.7636e-01,  7.2471e-01, -1.8839e-01,  9.6564e-01,\n","          2.1396e-01,  3.6877e-01, -6.0258e-02,  2.6505e-01, -5.1107e-01,\n","         -4.2091e-01,  1.9413e-02, -2.5991e-01,  9.7003e-01, -2.1713e-02,\n","         -4.3161e-01, -8.6556e-01,  2.8448e-01,  5.9014e-02, -4.4330e-01,\n","         -9.4856e-01, -1.1291e-01,  2.5513e-02,  3.7114e-01,  9.9050e-03,\n","          5.2219e-02, -3.4138e-01, -3.1813e-02, -3.0110e-01, -5.4604e-02,\n","          5.3167e-01, -9.1741e-01, -2.2246e-01, -1.0530e-02, -4.1509e-01,\n","          3.0833e-01, -9.7295e-01,  9.5327e-01, -2.9202e-01,  5.2656e-01,\n","          1.0000e+00, -1.5967e-02, -7.8224e-01,  2.5205e-01,  8.0371e-02,\n","         -1.1654e-01,  1.0000e+00,  5.2342e-01, -9.7992e-01, -4.5646e-01,\n","          4.4127e-01, -3.6342e-01, -5.4687e-01,  9.9663e-01, -1.6571e-01,\n","         -2.4606e-01,  7.1765e-02,  9.8694e-01, -9.8773e-01,  9.2531e-01,\n","         -7.8466e-01, -9.7499e-01,  9.5526e-01,  9.4014e-01, -3.1455e-02,\n","         -3.7029e-01, -8.3566e-02,  7.2885e-02,  7.8481e-02, -8.5670e-01,\n","          2.4106e-01,  1.2382e-01,  2.2440e-02,  9.0840e-01,  4.9766e-02,\n","         -4.8200e-01,  1.0067e-01, -3.3606e-01,  2.7572e-01,  5.1258e-01,\n","          3.4788e-01,  2.0840e-02, -4.1822e-02,  2.2078e-02, -5.3827e-01,\n","         -9.6365e-01,  5.3595e-01,  1.0000e+00,  1.7988e-01,  2.2127e-01,\n","          1.1171e-01,  4.2887e-02, -2.8482e-01,  2.7275e-01,  2.8186e-01,\n","         -1.9890e-01, -6.9904e-01,  5.4585e-01, -8.2020e-01, -9.8801e-01,\n","          3.9557e-01,  1.4115e-01, -9.0601e-02,  9.9788e-01,  5.2465e-02,\n","          8.7988e-02, -6.1069e-02,  8.0411e-01, -1.0107e-01,  4.5896e-02,\n","          2.8721e-01,  9.7186e-01, -5.0391e-02,  4.3895e-01,  5.3262e-01,\n","         -3.7363e-01, -1.4349e-01, -5.3335e-01, -1.7289e-01, -9.3699e-01,\n","          2.4583e-01, -9.5441e-01,  9.4962e-01,  6.8467e-01,  2.8843e-01,\n","          2.7844e-02,  1.9399e-01,  1.0000e+00, -5.5154e-01,  2.7470e-01,\n","          7.3334e-01,  2.1665e-01, -9.9376e-01, -6.3638e-01, -4.0457e-01,\n","          8.1165e-02, -1.0932e-01, -1.6521e-01,  1.1090e-01, -9.6194e-01,\n","          1.4183e-01,  2.6996e-01, -9.0304e-01, -9.8854e-01, -1.8411e-01,\n","         -1.4559e-01,  1.0233e-01, -8.8277e-01, -4.2014e-01, -5.6760e-01,\n","          2.0938e-01, -7.8962e-02, -9.2779e-01,  3.7535e-01, -3.2771e-01,\n","          3.7110e-01, -1.7455e-02,  4.4611e-01,  3.7664e-01,  8.9600e-01,\n","         -1.4597e-01, -4.1541e-02, -2.2711e-02, -5.6408e-01,  4.9451e-01,\n","         -6.0685e-01, -5.7528e-01,  1.6080e-02,  1.0000e+00, -2.4831e-01,\n","          4.7874e-01,  3.2856e-01,  4.0354e-01,  3.5562e-02,  7.1661e-02,\n","          5.1131e-01,  1.7113e-01, -5.2168e-04, -2.9695e-01,  7.3751e-01,\n","         -1.8529e-01,  4.4554e-01,  2.2911e-01,  2.8562e-02,  7.7319e-01,\n","          5.5270e-01,  1.0272e-01,  2.6211e-01, -1.2720e-03,  9.7222e-01,\n","         -2.8842e-02,  5.6738e-02, -2.8775e-01, -1.6172e-02, -1.9353e-01,\n","          4.9592e-01,  1.0000e+00,  8.4379e-02, -1.5369e-01, -9.8806e-01,\n","         -3.3429e-01, -7.0185e-01,  9.9986e-01,  7.6013e-01, -6.0292e-01,\n","          3.8539e-01,  2.5654e-01, -1.1947e-01,  3.1904e-01, -2.9968e-02,\n","         -8.2039e-02, -3.6788e-02, -4.1350e-02,  9.4251e-01, -3.8563e-01,\n","         -9.6644e-01, -1.8887e-01,  3.3731e-01, -9.5321e-01,  9.9444e-01,\n","         -3.1648e-01, -7.6926e-02, -2.2873e-01, -1.2125e-01, -8.2212e-01,\n","         -1.8324e-01, -9.8104e-01,  3.3368e-03,  6.5977e-02,  9.6485e-01,\n","          6.2124e-02, -4.1838e-01, -8.8928e-01,  4.6512e-01,  1.6997e-01,\n","         -4.9288e-01, -9.0313e-01,  9.4608e-01, -9.6796e-01,  4.0968e-01,\n","          9.9998e-01,  2.5584e-01, -4.0539e-01,  1.3442e-01, -1.9956e-01,\n","          2.1901e-01, -7.6149e-02,  4.1793e-01, -9.3571e-01, -2.5734e-01,\n","         -2.6220e-02,  1.9381e-01, -1.0859e-02,  1.0624e-02,  5.5476e-01,\n","          1.3884e-01, -3.7362e-01, -5.1091e-01, -2.0794e-02,  2.3886e-01,\n","          4.4453e-01, -1.9578e-01,  2.4322e-02,  1.2190e-01,  4.5708e-02,\n","         -8.9249e-01, -2.3091e-01, -2.3275e-01, -9.9933e-01,  3.8215e-01,\n","         -1.0000e+00,  2.4109e-01, -3.3744e-01, -9.5531e-02,  7.7688e-01,\n","          6.7944e-01,  5.0124e-01, -5.4854e-01, -4.4367e-01,  7.4304e-01,\n","          6.9326e-01, -1.1128e-01,  5.8066e-02, -5.1514e-01, -1.8508e-02,\n","          4.7066e-02, -3.9159e-02, -1.2526e-01,  6.4872e-01, -2.8116e-01,\n","          1.0000e+00,  9.4141e-02, -1.6344e-01, -8.3718e-01,  9.6640e-02,\n","         -1.5380e-01,  9.9999e-01, -4.3615e-01, -9.4652e-01,  1.8444e-01,\n","         -3.2900e-01, -7.3108e-01,  2.8296e-01, -1.6438e-01, -5.5741e-01,\n","         -5.1496e-01,  9.4393e-01,  1.5646e-01, -4.8041e-01,  3.7670e-01,\n","         -7.6502e-02, -3.4085e-01, -2.3005e-01,  4.6745e-01,  9.8561e-01,\n","          1.8930e-01,  6.5902e-01, -1.4607e-01, -5.6216e-02,  9.6718e-01,\n","          2.0109e-01, -4.4703e-01,  2.6056e-03,  1.0000e+00,  2.5435e-01,\n","         -8.5391e-01,  1.4343e-01, -9.7004e-01,  6.7542e-02, -9.1234e-01,\n","          2.3352e-01, -1.0207e-01,  9.0571e-01, -1.2382e-01,  8.9837e-01,\n","         -2.5035e-01, -9.6540e-02, -1.3500e-02,  7.9872e-02,  3.4886e-01,\n","         -9.0798e-01, -9.8696e-01, -9.8399e-01,  2.2660e-01, -2.6349e-01,\n","         -1.7568e-03,  2.5412e-01, -4.6728e-02,  3.2300e-01,  2.3782e-01,\n","         -1.0000e+00,  9.4718e-01,  2.8214e-01,  5.5079e-01,  9.5964e-01,\n","          4.3230e-01,  2.9824e-01,  1.1796e-01, -9.8534e-01, -9.1499e-01,\n","         -2.2060e-01, -2.2493e-01,  4.2716e-01,  4.1828e-01,  7.7361e-01,\n","          2.5652e-01, -4.4088e-01, -5.4283e-01, -1.9669e-01, -9.2347e-01,\n","         -9.9092e-01,  1.6564e-01, -2.1975e-02, -7.5007e-01,  9.5135e-01,\n","         -4.2565e-01,  4.1158e-02,  3.4651e-01, -3.7918e-01,  5.3755e-01,\n","          6.6361e-01, -1.7167e-01, -1.6186e-01,  3.5413e-01,  8.6273e-01,\n","          5.4597e-01,  9.7368e-01, -3.1056e-01,  3.8867e-01, -3.5970e-01,\n","          3.0381e-01,  7.2659e-01, -8.7832e-01,  4.5919e-02,  5.9937e-02,\n","          1.7157e-01,  1.6686e-01, -1.6294e-01, -7.8675e-01,  4.3243e-02,\n","         -2.4215e-01,  9.7457e-02, -2.9032e-01,  2.1996e-01, -2.2742e-01,\n","          4.9331e-02, -3.6432e-01, -2.3407e-01,  5.0983e-01, -1.0163e-01,\n","          9.1016e-01,  5.7243e-01,  4.2006e-02, -4.0053e-01, -9.6790e-03,\n","         -1.7138e-01, -8.6321e-01,  6.2138e-01,  2.0165e-01,  2.1101e-01,\n","          1.2278e-01, -1.7986e-01,  8.6453e-01, -5.4444e-01, -3.0941e-01,\n","         -3.3675e-01, -3.4412e-01,  7.0476e-01, -5.8997e-01, -3.5156e-01,\n","         -5.8885e-02,  4.5485e-01,  1.9294e-01,  9.9796e-01, -1.2719e-01,\n","         -3.9385e-01, -3.1919e-01, -2.4795e-01,  2.6886e-01,  3.8581e-02,\n","         -1.0000e+00,  1.7946e-01, -1.6161e-01, -9.5156e-02, -2.4447e-01,\n","          3.1448e-01, -3.1113e-01, -9.0355e-01, -1.8078e-02,  5.0788e-01,\n","          4.1148e-01, -4.0674e-01, -3.7335e-01,  4.8784e-01, -2.1414e-01,\n","          8.2619e-01,  8.2344e-01, -1.7268e-01,  6.6992e-01,  5.1202e-01,\n","         -4.5926e-01, -5.4121e-01,  9.0145e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"metadata":{},"execution_count":23}],"source":["sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n","tokenizer_output = bert_tokenizer(sentence, return_tensors=\"pt\")\n","input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n","\n","output = bert_model(input_ids, attention_mask = attn_mask)\n","output"]},{"cell_type":"markdown","id":"919f7bfc","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"473fa901be8e6117f55619d66f2ff8c3","grade":false,"grade_id":"cell-dc933bfb33ddbf97","locked":true,"schema_version":3,"solution":false,"task":false},"id":"919f7bfc"},"source":["As you can see, calling `bert_model` returns a bunch of different things. Let's go through them one by one and understand"]},{"cell_type":"code","execution_count":null,"id":"1f337132","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"af8fbb32c44fe43322dffc6d07810f4e","grade":false,"grade_id":"cell-40ff5447737be114","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1f337132","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dc94dc42-ee96-4826-d354-dd094abbe846"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids shape: torch.Size([1, 21])\n","last_hidden_state shape: torch.Size([1, 21, 768])\n"]}],"source":["last_hidden_state = output.last_hidden_state\n","print(f\"input_ids shape: {input_ids.shape}\")\n","print(f\"last_hidden_state shape: {last_hidden_state.shape}\")"]},{"cell_type":"markdown","id":"7fa9e5c2","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"de5e5dd782c7ff39ef7ee46739bcb46e","grade":false,"grade_id":"cell-3b5b43db00757d9a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"7fa9e5c2"},"source":["For an input of shape `[1,21]` which just means a single sequence of 21 tokens, last_hidden_state is a tensor of shape `[1, 21, 768]` denoting the contextual embedding of each of the 21 tokens in the sequence. These representations can be then used for solving a downstream task, by adding a linear layer or MLP layer on top. These can be useful for sequence labelling type of tasks."]},{"cell_type":"code","execution_count":null,"id":"c9c00632","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"00a748ad98adedcbc525dda6496de309","grade":false,"grade_id":"cell-2b52de7298f3eda7","locked":true,"schema_version":3,"solution":false,"task":false},"id":"c9c00632","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24ccb788-266c-4df1-8307-2832f678be99"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids shape: torch.Size([1, 21])\n","pooler_output shape: torch.Size([1, 768])\n"]}],"source":["pooler_output = output.pooler_output\n","print(f\"input_ids shape: {input_ids.shape}\")\n","print(f\"pooler_output shape: {pooler_output.shape}\")"]},{"cell_type":"markdown","id":"88dae74d","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a6a16d47faf0f2b84efa11ea9416a8c6","grade":false,"grade_id":"cell-720761c11ee8fa23","locked":true,"schema_version":3,"solution":false,"task":false},"id":"88dae74d"},"source":["`pooler_output` is an aggregate representation of the entire sentence and can be thought of as a sentence embedding. It is obtained by passing the representation of the \\[CLS\\] token through a linear layer. This can be useful for sentence-level tasks like sentiment analysis as well as multiple choice classification tasks etc."]},{"cell_type":"markdown","id":"5785c1f7","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"94333c842666bbf724ab301c505a21a9","grade":false,"grade_id":"cell-4b59e5d86dbd6e41","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5785c1f7"},"source":["Apart from these two we can also obtain other values by providing additional arguments. Like if we want to obtain attention maps which can be useful for interpretating the model's behavior, we can just specify `output_attentions=True` while calling the model"]},{"cell_type":"code","execution_count":null,"id":"50ae6ee7","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"dac70c849fa65e8f8d505d88c9e88019","grade":false,"grade_id":"cell-08b6ef4cef17c16e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"50ae6ee7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a21b0f2c-a2ab-4a63-acb7-738d606c9869"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data type of attentions output: <class 'tuple'>\n","Number of elements: 12\n","Shape of individual element: torch.Size([1, 12, 21, 21])\n","Example attention map: tensor([[0.0365, 0.0188, 0.0239, 0.0918, 0.0375, 0.0409, 0.0390, 0.0327, 0.0256,\n","         0.0260, 0.0488, 0.0387, 0.0532, 0.0188, 0.0251, 0.0408, 0.0321, 0.0840,\n","         0.0793, 0.0262, 0.1801],\n","        [0.0178, 0.0460, 0.0240, 0.0306, 0.0482, 0.0194, 0.0510, 0.0919, 0.0260,\n","         0.0818, 0.0425, 0.0482, 0.0159, 0.0576, 0.0703, 0.1526, 0.0337, 0.0405,\n","         0.0194, 0.0371, 0.0454],\n","        [0.0527, 0.0466, 0.0899, 0.0241, 0.0464, 0.0245, 0.0645, 0.0587, 0.0256,\n","         0.0401, 0.0298, 0.0606, 0.0302, 0.0735, 0.1222, 0.0573, 0.0297, 0.0204,\n","         0.0224, 0.0539, 0.0268],\n","        [0.0425, 0.0528, 0.0454, 0.0338, 0.0402, 0.0353, 0.0515, 0.0666, 0.0397,\n","         0.0464, 0.0336, 0.0518, 0.0492, 0.0609, 0.0529, 0.0731, 0.0509, 0.0580,\n","         0.0381, 0.0492, 0.0280],\n","        [0.0492, 0.1514, 0.0892, 0.0078, 0.0237, 0.0282, 0.0478, 0.0639, 0.0288,\n","         0.0346, 0.0376, 0.0427, 0.0386, 0.0865, 0.0633, 0.0612, 0.0589, 0.0099,\n","         0.0217, 0.0326, 0.0225],\n","        [0.0328, 0.0851, 0.0669, 0.0210, 0.0342, 0.0278, 0.0618, 0.0965, 0.0244,\n","         0.0388, 0.0382, 0.0554, 0.0281, 0.0809, 0.0671, 0.0634, 0.0372, 0.0276,\n","         0.0302, 0.0289, 0.0536],\n","        [0.0390, 0.0402, 0.1059, 0.0350, 0.0359, 0.0453, 0.0443, 0.0639, 0.0261,\n","         0.0437, 0.0524, 0.0837, 0.0440, 0.0611, 0.0661, 0.0560, 0.0265, 0.0284,\n","         0.0247, 0.0368, 0.0410],\n","        [0.0637, 0.0480, 0.0736, 0.0488, 0.0387, 0.0301, 0.0386, 0.1177, 0.0174,\n","         0.0286, 0.0322, 0.0448, 0.0351, 0.0715, 0.0595, 0.0795, 0.0202, 0.0495,\n","         0.0239, 0.0297, 0.0488],\n","        [0.0379, 0.0930, 0.0471, 0.0373, 0.0634, 0.0240, 0.0501, 0.0724, 0.0077,\n","         0.0576, 0.1025, 0.0672, 0.0309, 0.0510, 0.0276, 0.0513, 0.0243, 0.0229,\n","         0.0249, 0.0429, 0.0641],\n","        [0.0287, 0.1002, 0.0683, 0.0142, 0.0493, 0.0313, 0.0819, 0.0516, 0.0350,\n","         0.0476, 0.0602, 0.0453, 0.0288, 0.0470, 0.1209, 0.0661, 0.0310, 0.0140,\n","         0.0197, 0.0225, 0.0367],\n","        [0.0285, 0.0755, 0.0461, 0.0240, 0.0530, 0.0329, 0.0516, 0.0549, 0.0636,\n","         0.0900, 0.0263, 0.0202, 0.0290, 0.0448, 0.0628, 0.0496, 0.0781, 0.0263,\n","         0.0399, 0.0470, 0.0559],\n","        [0.0478, 0.0464, 0.0582, 0.0576, 0.0513, 0.0424, 0.0580, 0.0508, 0.0362,\n","         0.0565, 0.0378, 0.0387, 0.0565, 0.0367, 0.0458, 0.0434, 0.0477, 0.0464,\n","         0.0433, 0.0513, 0.0475],\n","        [0.0320, 0.0923, 0.0587, 0.0300, 0.0370, 0.0399, 0.0736, 0.1011, 0.0183,\n","         0.0490, 0.0473, 0.0639, 0.0212, 0.0637, 0.0596, 0.0799, 0.0234, 0.0308,\n","         0.0306, 0.0149, 0.0330],\n","        [0.0335, 0.0893, 0.1025, 0.0227, 0.0361, 0.0223, 0.0555, 0.1325, 0.0104,\n","         0.0403, 0.0382, 0.0476, 0.0227, 0.0636, 0.0781, 0.0949, 0.0216, 0.0269,\n","         0.0159, 0.0252, 0.0202],\n","        [0.0544, 0.1423, 0.0956, 0.0254, 0.0281, 0.0320, 0.0471, 0.0546, 0.0136,\n","         0.0673, 0.0418, 0.0395, 0.0531, 0.0422, 0.0400, 0.1038, 0.0275, 0.0331,\n","         0.0201, 0.0151, 0.0234],\n","        [0.0403, 0.0612, 0.0584, 0.0240, 0.0489, 0.0368, 0.0703, 0.0585, 0.0444,\n","         0.0276, 0.0329, 0.0323, 0.0453, 0.0484, 0.1187, 0.0948, 0.0410, 0.0257,\n","         0.0229, 0.0184, 0.0490],\n","        [0.0249, 0.0599, 0.0252, 0.0498, 0.0809, 0.0517, 0.0329, 0.0799, 0.0372,\n","         0.0461, 0.0336, 0.0263, 0.0299, 0.0659, 0.0677, 0.0610, 0.0249, 0.0287,\n","         0.0350, 0.0519, 0.0864],\n","        [0.0298, 0.0571, 0.0516, 0.0481, 0.0414, 0.0276, 0.0582, 0.0824, 0.0294,\n","         0.0469, 0.0406, 0.0386, 0.0444, 0.0660, 0.0562, 0.0832, 0.0321, 0.0636,\n","         0.0414, 0.0343, 0.0273],\n","        [0.0209, 0.0511, 0.0464, 0.0348, 0.0476, 0.0474, 0.0860, 0.0561, 0.0389,\n","         0.0365, 0.0628, 0.0366, 0.0508, 0.0486, 0.0764, 0.0663, 0.0431, 0.0291,\n","         0.0349, 0.0329, 0.0528],\n","        [0.0228, 0.1058, 0.1149, 0.0203, 0.0668, 0.0299, 0.0469, 0.0448, 0.0492,\n","         0.0439, 0.0376, 0.0540, 0.0458, 0.0640, 0.0903, 0.0373, 0.0353, 0.0134,\n","         0.0297, 0.0104, 0.0368],\n","        [0.0342, 0.0267, 0.0292, 0.0788, 0.0409, 0.0381, 0.0354, 0.0651, 0.0243,\n","         0.0478, 0.0643, 0.0683, 0.0461, 0.0384, 0.0317, 0.0849, 0.0342, 0.0737,\n","         0.0565, 0.0304, 0.0511]], grad_fn=<SelectBackward0>)\n"]}],"source":["output = bert_model(input_ids, attention_mask = attn_mask, output_attentions=True)\n","attentions = output.attentions\n","print(f\"Data type of attentions output: {type(attentions)}\")\n","print(f\"Number of elements: {len(attentions)}\")\n","print(f\"Shape of individual element: {attentions[0].shape}\")\n","print(f\"Example attention map: {attentions[0][0,0]}\")"]},{"cell_type":"markdown","id":"a3a4519b","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"6b9a58816d3b1ec81fb7081f80acb009","grade":false,"grade_id":"cell-34a578f52897b7d7","locked":true,"schema_version":3,"solution":false,"task":false},"id":"a3a4519b"},"source":["As you can see `attentions` is a tuple containing 12 elements which corresponds to the attention maps of each of the 12 layers in the network. Further each layer's attention maps also contains 12 attention maps corresponding to 12 heads in each layer. A single attention map as you can see is a 18x18 matrix representing the attention pattern for all the tokens in the sequence"]},{"cell_type":"markdown","id":"8a79f03f","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"345bea080f4368b2a6d87453a22a1edc","grade":false,"grade_id":"cell-c224cab96a9915ec","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8a79f03f"},"source":["### Task 2.1: Implementing BERT-based Classifier for Multiple Choice Classification\n","\n","In this task you will implement a bert-based classifier in Pytorch very similar to how we created bag of word classifiers in the previous assignments. The architecture of the model is as follows:\n","\n","![architecture](https://i.ibb.co/hVmS9Qx/siqa-bert-arch-excalli.png)\n","\n","Essentially, what we have here is a model that takes a context and question, and scores a particular answer (denoted as a score(a)). At the backbone we have the BERT model, using which we obtain the contextualized representation of the [context, question, answer] sequence. We then use the \\[CLS\\] token's embedding as the sequence representation and feed it to a 2 layer MLP (Linear(768, 768) -> ReLU -> Linear(768, 1)) that scores the answer. To predict the correct answer, score each of the three answers, obtain their scores and normalize them by applying softmax, that gives us the probability of each option being the correct answer.\n","\n","\n","![forward pass](https://i.ibb.co/r3SrLHY/siqa-bert-forward-excalli.png)\n","\n","Implement the architecture and forward pass in `BertMultiChoiceClassifierModel` class below:"]},{"cell_type":"code","execution_count":null,"id":"0f626042","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"34a2a6ddf2f67acbfe7fd30857f6de91","grade":false,"grade_id":"cell-7930c03ec3b56775","locked":false,"schema_version":3,"solution":true,"task":false},"id":"0f626042"},"outputs":[],"source":["class BertMultiChoiceClassifierModel(nn.Module):\n","\n","    def __init__(self, d_hidden = 768, bert_variant = \"bert-base-uncased\"):\n","        \"\"\"\n","        Define the architecture of Bert-Based mulit-choice classifier.\n","        You will mainly need to define 3 components, first a BERT layer\n","        using `BertModel` from transformers library,\n","        a two layer MLP layer to map the representation from Bert to the output i.e. (Linear(d_hidden, d_hidden) -> ReLU -> Linear(d_hidden, 1)),\n","        and a log sftmax layer to map the scores to a probabilities\n","\n","        Inputs:\n","            - d_hidden (int): Size of the hidden representations of bert\n","            - bert_variant (str): BERT variant to use\n","        \"\"\"\n","        super(BertMultiChoiceClassifierModel, self).__init__()\n","        self.bert_layer = None\n","        self.mlp_layer = None\n","        self.log_softmax_layer = None\n","\n","        # YOUR CODE HERE\n","        self.bert_layer = BertModel.from_pretrained(bert_variant)\n","        self.mlp_layer = nn.Sequential(\n","            nn.Linear(d_hidden, d_hidden),\n","            nn.ReLU(),\n","            nn.Linear(d_hidden, 1)\n","        )\n","        self.log_softmax_layer = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, input_ids_dict, attn_mask_dict):\n","        \"\"\"\n","        Forward Passes the inputs through the network and obtains the prediction\n","\n","        Inputs:\n","            - input_ids_dict (dict(str,torch.tensor)): A dictionary containing input_ids corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n","                                        representing the sequence of token ids\n","            - attn_mask_dict (dict(str,torch.tensor)): A dictionary containing attention mask corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n","\n","        Returns:\n","          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n","\n","\n","        Hints:\n","            1. Recall which of the outputs from BertModel is appropriate for the sentence classification task and how to access it.\n","            2. `torch.cat` might come in handy before performing softmax\n","        \"\"\"\n","        output = None\n","        key_outs = []\n","        # YOUR CODE HERE\n","        for key in input_ids_dict.keys():\n","            bert_output = self.bert_layer(input_ids_dict[key], attention_mask = attn_mask_dict[key])\n","            pooler_output = bert_output.pooler_output\n","            mlp_output = self.mlp_layer(pooler_output)\n","            key_outs.append(mlp_output)\n","\n","        output = self.log_softmax_layer(torch.cat(key_outs, dim=-1))\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"id":"1c65d7c7","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"35f231082355ea3357feed7ad243b08a","grade":true,"grade_id":"cell-cf9b5db5de53eeac","locked":true,"points":3,"schema_version":3,"solution":false,"task":false},"id":"1c65d7c7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f988edd-1847-4249-bca5-dd6f5a30f39f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Cases!\n","Sample Test Case 1\n","Model Output: [[-1.1189675 -1.0885006 -1.0886753]\n"," [-1.1045516 -1.0834142 -1.108049 ]\n"," [-1.1027124 -1.0822923 -1.1110513]\n"," [-1.1008494 -1.0936637 -1.1013424]\n"," [-1.0921423 -1.0974909 -1.1062545]\n"," [-1.0798944 -1.1088551 -1.1073539]\n"," [-1.1030428 -1.0939085 -1.0989064]\n"," [-1.0971036 -1.0970919 -1.1016482]\n"," [-1.1319212 -1.082568  -1.0821619]\n"," [-1.0961349 -1.1014836 -1.0982256]\n"," [-1.0979306 -1.0836825 -1.1144607]\n"," [-1.1034716 -1.0959275 -1.0964555]\n"," [-1.101945  -1.0958116 -1.0980897]\n"," [-1.1050864 -1.0986388 -1.0921534]\n"," [-1.1013197 -1.082134  -1.1126211]\n"," [-1.102798  -1.0906713 -1.1024151]]\n","Expected Output: [[-1.1189675 -1.0885007 -1.0886753]\n"," [-1.1045516 -1.0834142 -1.108049 ]\n"," [-1.1027125 -1.0822924 -1.1110513]\n"," [-1.1008494 -1.0936636 -1.1013424]\n"," [-1.0921422 -1.0974907 -1.1062546]\n"," [-1.0798943 -1.1088552 -1.1073538]\n"," [-1.1030427 -1.0939085 -1.0989065]\n"," [-1.0971034 -1.097092  -1.1016482]\n"," [-1.131921  -1.0825679 -1.0821619]\n"," [-1.0961349 -1.1014836 -1.0982255]\n"," [-1.0979307 -1.0836827 -1.1144608]\n"," [-1.1034715 -1.0959275 -1.0964555]\n"," [-1.1019452 -1.0958116 -1.0980899]\n"," [-1.1050864 -1.0986389 -1.0921533]\n"," [-1.1013198 -1.0821339 -1.112621 ]\n"," [-1.1027979 -1.0906712 -1.1024152]]\n","Test Case Passed! :)\n","******************************\n","\n","Sample Test Case 2\n","Model Output: [[-1.100536  -1.1009303 -1.094384 ]\n"," [-1.0732511 -1.1178818 -1.1052345]\n"," [-1.1025076 -1.094363  -1.098983 ]\n"," [-1.1236264 -1.1056153 -1.0674216]\n"," [-1.0999552 -1.1014045 -1.0944903]\n"," [-1.0953273 -1.0959655 -1.1045707]\n"," [-1.10844   -1.0971686 -1.0903118]\n"," [-1.0993489 -1.1130905 -1.0836148]\n"," [-1.1031718 -1.0897288 -1.1029955]\n"," [-1.0929244 -1.1077558 -1.0952207]\n"," [-1.0995094 -1.0998485 -1.0964826]\n"," [-1.1419643 -1.1081928 -1.0479566]\n"," [-1.1052556 -1.0851237 -1.1055952]\n"," [-1.0840428 -1.1084775 -1.1034834]\n"," [-1.0872697 -1.1025085 -1.1061593]\n"," [-1.1060572 -1.0939908 -1.0958309]]\n","Expected Output: [[-1.1005359 -1.1009303 -1.094384 ]\n"," [-1.073251  -1.1178819 -1.1052346]\n"," [-1.1025076 -1.094363  -1.098983 ]\n"," [-1.1236262 -1.1056151 -1.0674216]\n"," [-1.0999551 -1.1014045 -1.0944905]\n"," [-1.0953273 -1.0959654 -1.1045709]\n"," [-1.1084402 -1.0971687 -1.0903118]\n"," [-1.099349  -1.1130908 -1.0836148]\n"," [-1.1031718 -1.0897288 -1.1029954]\n"," [-1.0929244 -1.1077557 -1.0952206]\n"," [-1.0995092 -1.0998485 -1.0964826]\n"," [-1.1419646 -1.1081928 -1.0479565]\n"," [-1.1052557 -1.0851235 -1.1055952]\n"," [-1.0840428 -1.1084775 -1.1034834]\n"," [-1.0872697 -1.1025085 -1.1061592]\n"," [-1.1060572 -1.0939908 -1.095831 ]]\n","Test Case Passed! :)\n","******************************\n","\n"]}],"source":["print(f\"Running Sample Test Cases!\")\n","torch.manual_seed(42)\n","model = BertMultiChoiceClassifierModel()\n","\n","print(\"Sample Test Case 1\")\n","batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n","bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n","expected_bert_out = np.array([[-1.1189675, -1.0885007, -1.0886753],\n","                            [-1.1045516, -1.0834142, -1.108049 ],\n","                            [-1.1027125, -1.0822924, -1.1110513],\n","                            [-1.1008494, -1.0936636, -1.1013424],\n","                            [-1.0921422, -1.0974907, -1.1062546],\n","                            [-1.0798943, -1.1088552, -1.1073538],\n","                            [-1.1030427, -1.0939085, -1.0989065],\n","                            [-1.0971034, -1.097092 , -1.1016482],\n","                            [-1.131921 , -1.0825679, -1.0821619],\n","                            [-1.0961349, -1.1014836, -1.0982255],\n","                            [-1.0979307, -1.0836827, -1.1144608],\n","                            [-1.1034715, -1.0959275, -1.0964555],\n","                            [-1.1019452, -1.0958116, -1.0980899],\n","                            [-1.1050864, -1.0986389, -1.0921533],\n","                            [-1.1013198, -1.0821339, -1.112621 ],\n","                            [-1.1027979, -1.0906712, -1.1024152]],)\n","print(f\"Model Output: {bert_out}\")\n","print(f\"Expected Output: {expected_bert_out}\")\n","\n","assert bert_out.shape == expected_bert_out.shape\n","assert np.allclose(bert_out, expected_bert_out, 1e-4)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")\n","\n","print(\"Sample Test Case 2\")\n","batch_input_ids, batch_attn_mask, batch_labels = next(iter(dev_loader))\n","bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n","expected_bert_out = np.array([[-1.1005359, -1.1009303, -1.094384 ],\n","                            [-1.073251 , -1.1178819, -1.1052346],\n","                            [-1.1025076, -1.094363 , -1.098983 ],\n","                            [-1.1236262, -1.1056151, -1.0674216],\n","                            [-1.0999551, -1.1014045, -1.0944905],\n","                            [-1.0953273, -1.0959654, -1.1045709],\n","                            [-1.1084402, -1.0971687, -1.0903118],\n","                            [-1.099349 , -1.1130908, -1.0836148],\n","                            [-1.1031718, -1.0897288, -1.1029954],\n","                            [-1.0929244, -1.1077557, -1.0952206],\n","                            [-1.0995092, -1.0998485, -1.0964826],\n","                            [-1.1419646, -1.1081928, -1.0479565],\n","                            [-1.1052557, -1.0851235, -1.1055952],\n","                            [-1.0840428, -1.1084775, -1.1034834],\n","                            [-1.0872697, -1.1025085, -1.1061592],\n","                            [-1.1060572, -1.0939908, -1.095831 ]])\n","print(f\"Model Output: {bert_out}\")\n","print(f\"Expected Output: {expected_bert_out}\")\n","\n","assert bert_out.shape == expected_bert_out.shape\n","assert np.allclose(bert_out, expected_bert_out, 1e-4)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")"]},{"cell_type":"markdown","id":"f38f523b","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"b96506abcb89deafbdc245e7e22a0509","grade":false,"grade_id":"cell-d26d1e16847283ff","locked":true,"schema_version":3,"solution":false,"task":false},"id":"f38f523b"},"source":["### Task 2.2: Training and Evaluating the Model\n","\n","Now that we have implemented the custom Dataset and a BERT based classifier model, we can start training and evaluating the model. This time we will modify the training loop slightly. At the end of each training epoch we will now evaluate on the validation data and check the accuracy. Based on this we will select the best model across the epochs that obtains highest validation accuracy. You will need to implement the `train` and `evaluate` functions below."]},{"cell_type":"code","execution_count":null,"id":"af1e43b1","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"fce28e32786d4f34b17a1826f752e3a2","grade":true,"grade_id":"cell-37de7065d6cb7392","locked":false,"points":3,"schema_version":3,"solution":true,"task":false},"id":"af1e43b1"},"outputs":[],"source":["def evaluate(model, test_dataloader, device = \"cpu\"):\n","    \"\"\"\n","    Evaluates `model` on test dataset\n","\n","    Inputs:\n","        - model (BertMultiChoiceClassifierModel): BERT based multiple choice classifier model to be evaluated\n","        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n","\n","    Returns:\n","        - accuracy (float): Average accuracy over the test dataset\n","    \"\"\"\n","\n","    model.eval()\n","    model = model.to(device)\n","    accuracy = 0\n","\n","\n","    model = model.to(device)\n","    with torch.no_grad():\n","        for test_batch in test_dataloader:\n","\n","            # Read the batch from dataloader\n","            input_ids_dict, attn_mask_dict, labels = test_batch\n","\n","            # Send all values of dicts to device\n","            for key in input_ids_dict.keys():\n","                input_ids_dict[key] = input_ids_dict[key].to(device)\n","                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n","            labels = labels.float().to(device)\n","\n","            # Step 1: Compute model's prediction on the test batch (Note here you need to get the final prediction from the model's output)\n","            preds = None\n","            # YOUR CODE HERE\n","            preds = model(input_ids_dict, attn_mask_dict)\n","\n","            # Step 2: then compute accuracy and store it in batch_accuracy\n","            batch_accuracy = 0\n","            # YOUR CODE HERE - Referred to Assignment 1\n","            preds = torch.argmax(preds, dim=1)\n","            pred_accuracy = torch.sum(preds == labels).item()\n","            batch_accuracy = pred_accuracy / len(labels)\n","\n","            accuracy += batch_accuracy\n","\n","    accuracy = accuracy / len(test_dataloader)\n","    return accuracy\n","\n","\n","def train(model, train_dataloader, val_dataloader,\n","          lr = 1e-5, num_epochs = 3,\n","          device = \"cpu\"):\n","    \"\"\"\n","    Runs the training loop. Define the loss function as BCELoss like the last tine\n","    and optimizer as Adam and traine for `num_epochs` epochs.\n","\n","    Inputs:\n","        - model (BertMultiChoiceClassifierModel): BERT based classifer model to be trained\n","        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n","        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n","        - lr (float): The learning rate for the optimizer\n","        - num_epochs (int): Number of epochs to train the model for.\n","        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n","\n","    Returns:\n","        - best_model (BertMultiChoiceClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n","        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n","    \"\"\"\n","    epoch_loss = 0\n","    model = model.to(device)\n","\n","    best_val_accuracy = float(\"-inf\")\n","    best_model = None\n","\n","    # 1. Define Loss function and optimizer\n","    loss_fn = None\n","    optimizer = None\n","    # YOUR CODE HERE\n","    loss_fn = nn.NLLLoss()\n","    optimizer = Adam(model.parameters(), lr=lr)\n","\n","    # Iterate over `num_epochs`\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0 # We can use this to keep track of how the loss value changes as we train the model.\n","        # Iterate over each batch using the `train_dataloader`\n","        for train_batch in tqdm(train_dataloader):\n","\n","            # Zero out any gradients stored in the previous steps\n","            optimizer.zero_grad()\n","\n","            # Read the batch from dataloader\n","            input_ids_dict, attn_mask_dict, labels = train_batch\n","\n","            # Send all values of dicts to device\n","            for key in input_ids_dict.keys():\n","                input_ids_dict[key] = input_ids_dict[key].to(device)\n","                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n","            labels = labels.to(device)\n","\n","            # Step 3: Feed the input features to the model to get outputs log-probabilities\n","            model_outs = None\n","            # YOUR CODE HERE\n","            model_outs = model(input_ids_dict, attn_mask_dict)\n","\n","            # Step 4: Compute the loss and perform backward pass\n","            loss = None\n","            # YOUR CODE HERE\n","            loss = loss_fn(model_outs, labels)\n","            loss.backward()\n","\n","            # Step 5: Take optimizer step\n","            # YOUR CODE HERE\n","            optimizer.step()\n","\n","            # Store loss value for tracking\n","            epoch_loss += loss.item()\n","\n","        epoch_loss = epoch_loss / len(train_dataloader)\n","        # Step 6. Evaluate on validation data by calling `evaluate` and store the validation accuracy in `val_accurracy`\n","        val_accuracy = 0\n","        # YOUR CODE HERE\n","        val_accuracy = evaluate(model, val_dataloader, device)\n","\n","        # Model selection\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            best_model = copy.deepcopy(model) # Create a copy of model\n","\n","        print(f\"Epoch {epoch} completed | Average Training Loss: {epoch_loss} | Validation Accuracy: {val_accuracy}\")\n","\n","    return best_model, best_val_accuracy"]},{"cell_type":"code","execution_count":null,"id":"b408695c","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6e8f472b79d52b3b88d3ea6a7b0b6e74","grade":true,"grade_id":"cell-f17fa8b3f55ab382","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"b408695c","colab":{"base_uri":"https://localhost:8080/","height":316,"referenced_widgets":["b327d02baf7f42fcaed457593e29d0ec","a02aab0fbfe94f2f8e4e89c5f29c7788","4901ad1caca54fc39d9a66094deb0f7e","49bf0bb4bd824f24b9a65d9c948001e3","fa1fd1c13a1d4696bf1ac1042812d69e","4b83455666f74b279074af648517e728","13bdbaefc174461cb77ff7e479c8aa4c","4ed15983d39c44cb873894b3dac9eb4b","e257b0b885f048f0bda6fd748e1b3e40","8eb5731eb2ad48b7947786a807e05178","15b462fb7be64a1c8bd99fb10988aa22","a91243a797d241fc8c0d695e45224c5e","c883df886b5044c0bfa4d933f6192d96","e0c248567e0146f2979f97ca0f64ce59","1791a7a9e413460b85c3de34aca46fb2","c5f96fea005f4c33a2b2d64eb8168f88","b508f624716242d39884adb29e2a3cec","c9ec716278d440a08313047a5bc221ca","e3ccc652089541bda03d027938f2662d","fc0a03dd40174b338d83d32251232b12","210e26ff990f4cc08db2c9b9e42efba7","702a87d037a4437fad307f2eeaa0ddfe","1be91cbd7e9b411ca815c84711a036bd","4505fb551f9e40e697214c6d6d6a98ad","933c34a6a0874c378e0df0e464a2dc93","ee793521388447b8b1781b8a5b13499b","ccc98c91d8a048799d394a6641cbf919","91f8699a6f0146089de3e582fb087f98","08c27bd660a244fe92df6cd80b2a95f9","6f7c92dec8c24094ada7c8474143e4fe","91d2ece2de494f2884699ff3b18947ea","beba8c7115f94819a29130de39ac83e7","e6dd2b31fca44995973a3a855e533704","646b36a6ac2f46579cc389ceeacb67dc","58013b573e174c9889a630ab4ffd8056","1d38a8b14ebe4fe9b296dbcbfb60d3ca","23f31cd7e18347debfd8837e78fb4a31","466aa311d985481282d2d9153b7773ae","a94f44e2ede34f6cae0312cc6b99c021","f8450f84e90f4a238aa023bca10a5d88","e7b42923d904486ebe69c70a6eb47fe6","a3b072f786c74b06898dce95d8fafea8","e347aeea0a5a4072902e2370cc6b4754","8fc38796e3744f6380925c739dc21392","46ecd1f4965a4d7d97705e20d1b02a88","ae76d724a1204a20a913684ad6fc7728","6f81311c23cb4bd69b5d2ad08c45e7fd","d00e18d781c04c67be0f750198a53153","423d8c37b0d544cfbfb0ac5bd4c490b9","c415332927894606aeca98c97f9c7adc","f7de15280fe04b27bc844ed935fa020e","66e0effb96544572a5d4aefd571edfb0","9d77d85ffb7a4e10ae22bc0eb8937dcd","5b32ff46982a4c8da44d37e18495b5f0","741844aadd9b47b8ba86e75e92b8f76d"]},"outputId":"fa569efb-0e44-467f-8995-12e20b745765"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training on 100 data points for sanity check\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b327d02baf7f42fcaed457593e29d0ec"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 completed | Average Training Loss: 1.099404911994934 | Validation Accuracy: 0.78\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a91243a797d241fc8c0d695e45224c5e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1 completed | Average Training Loss: 1.005476462841034 | Validation Accuracy: 0.88\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be91cbd7e9b411ca815c84711a036bd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 2 completed | Average Training Loss: 0.5784622395038604 | Validation Accuracy: 0.96\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"646b36a6ac2f46579cc389ceeacb67dc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 3 completed | Average Training Loss: 0.34795909315347673 | Validation Accuracy: 1.0\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ecd1f4965a4d7d97705e20d1b02a88"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 4 completed | Average Training Loss: 0.10256962414830922 | Validation Accuracy: 1.0\n","Best Validation Accuracy: 1.0\n","Expected Best Validation Accuracy: 1.0\n"]}],"source":["torch.manual_seed(42)\n","print(\"Training on 100 data points for sanity check\")\n","sample_data = train_data[:100]\n","sample_labels = train_labels[:100]\n","sample_dataset = SIQABertDataset(sample_data, sample_labels)\n","sample_dataloader = DataLoader(sample_dataset, batch_size=4, collate_fn=partial(collate_fn, sample_dataset.tokenizer))\n","\n","model = BertMultiChoiceClassifierModel()\n","best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, num_epochs = 5, device = \"cuda\")\n","print(f\"Best Validation Accuracy: {best_val_acc}\")\n","print(f\"Expected Best Validation Accuracy: {1.0}\")"]},{"cell_type":"markdown","id":"e958b3c7","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"dbd0f3b1ded747e9f489e15a0fd7dd8e","grade":false,"grade_id":"cell-e98f8c2ba73c862e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"e958b3c7"},"source":[" You can expect the validation accuracy of 1.0 by the end of training. This is so high because we trained on just 100 examples and just use those for validation for a sanity check. This is often done to debug the model and training loop. Let's now train on the entire dataset. This can take some time approximately 50 minutes per epoch, since we are fine-tuning all the 12 layers of BERT."]},{"cell_type":"code","execution_count":null,"id":"298b6cf0","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"19c69cac6d134da18698445835172a21","grade":false,"grade_id":"cell-33834a90b1557f37","locked":true,"schema_version":3,"solution":false,"task":false},"id":"298b6cf0","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["7e0d8e989a6044a0b93fd167a3ffbbe3","0648004162d94abc843f7486854b392d","2307d5cb66194b2c84205bf0ed962319","e1bf3ced364a4c069fcddb562b50f503","5f2e850d94b04af9919845917477b54e","87919c3a08f149a29eee1e98a6f87cf3","f517455fe2af40d498d35237530a1eb4","250e76d381e5461db1228db2e1f91e9b","ab42a13168924ccba8466fa0b498f3c0","c856586429174e4887288b2e6e63c782","e8d4799ff405459488a66320a66f2474","18d1752139da4f4bb996e31feebea3bb","be563b9d6c434d06b9f6768a1652950c","c85b995caae943ae8058f76afd80a0be","d768a3406bd24e4492e6232f50eca3ac","0150d1941b574223a43dccb8d368d79f","4ba36643162c474f85eafaa9a02dc28d","af9b9ecca17a430e8e8105b8fed8bf6e","11ef94689d3048eebea859a16d4bd220","776a5450ca77420797974f7688f6f97f","600a53f6f06d47d89a88c4aa1253688e","ef662e1ed5824ef2b0334d19181f7a4f"]},"outputId":"54393a69-3afc-485d-d466-7db060974ce1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2089 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0d8e989a6044a0b93fd167a3ffbbe3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 completed | Average Training Loss: 0.6980315272875982 | Validation Accuracy: 0.6102642276422764\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2089 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d1752139da4f4bb996e31feebea3bb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1 completed | Average Training Loss: 0.4188179671693042 | Validation Accuracy: 0.6072154471544715\n"]}],"source":["model = BertMultiChoiceClassifierModel()\n","best_model, best_val_acc = train(model, train_loader, dev_loader, num_epochs = 2, device = \"cuda\")"]},{"cell_type":"markdown","id":"a397015f","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"48068cd407a033e3d3a83d6f5ffc416f","grade":false,"grade_id":"cell-eb742cd2053baea4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"a397015f"},"source":["You should expect about ~61% validation accuracy (random classifier will have an accuracy of 33%), which is around what's reported in the SocialIQA paper. Note that this is a much more complex task than the news classification that we had in the last lab. You can further improve the performance by using bigger models like bert-base-large or roberta-large."]},{"cell_type":"markdown","id":"95b18060","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"fb7eeb52eab4c3dee032a34d8cab16a8","grade":false,"grade_id":"cell-92df0b4e94279d7b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"95b18060"},"source":["Now that we have a model ready for the task, we can save it on disk, so we can use it later (This will come handy for Assignment2)"]},{"cell_type":"code","execution_count":null,"id":"c3168433","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"3840681b544c16526676ad44bfbbf4d7","grade":false,"grade_id":"cell-58bf127a74f43114","locked":true,"schema_version":3,"solution":false,"task":false},"id":"c3168433"},"outputs":[],"source":["# Save the best model\n","save_dir = \"models/siqa_bert-base-uncased/\"\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","\n","torch.save(best_model.state_dict(), f\"{save_dir}/model.pt\")"]},{"cell_type":"markdown","id":"82f7ea51","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3a3028590f28ef4abb1bba217f555232","grade":false,"grade_id":"cell-28db2cd28da09990","locked":true,"schema_version":3,"solution":false,"task":false},"id":"82f7ea51"},"source":["### Task 2.3: Making Predictions from scratch\n","\n","Similar to assignment 1, implement the function `predict_siqa` that takes as input the context, question and answers and runs them through the BERT classifier model to obtain the prediction."]},{"cell_type":"code","execution_count":null,"id":"639651fe","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"f7d2ad4a45affa789ed2eab66e144df8","grade":false,"grade_id":"cell-8a815364723b18a7","locked":false,"schema_version":3,"solution":true,"task":false},"id":"639651fe"},"outputs":[],"source":["def predict_text(siqa_instance, model, tokenizer,device = \"cpu\"):\n","    \"\"\"\n","    Predicts the correct answer for a piece of a Social IQA instance using the BERT classifier model\n","\n","    Inputs:\n","        - siqa_instance (dict(str, str)): An SIQA instance containing the context, question and the three answer choices.\n","        - model (BertMultiChoiceClassifierModel): Fine-tuned BERT based classifer model\n","        - tokenizer (BertTokenizer): Pre-trained BERT tokenizer\n","    Returns:\n","        - pred_label (float): Predicted answer for `siqa_instance`\n","    \"\"\"\n","\n","    model = model.to(device)\n","    model.eval()\n","\n","    pred_label = None\n","\n","    input_ids_dict = None\n","    attn_mask_dict = None\n","    # Step 1: Tokenize the [sentence, question, answer] triplet using the tokenizer and create input_ids_dict and attn_mask_dict, as done in the Dataset class\n","    # (Don't forget to convert the lists to tensors, torch.Tensor() can come handy or just use return_tensors = \"pt\" while calling the tokenizer)\n","    # YOUR CODE HERE\n","    context = siqa_instance[\"context\"]\n","    question = siqa_instance[\"question\"]\n","    answerA = siqa_instance[\"answerA\"]\n","    answerB = siqa_instance[\"answerB\"]\n","    answerC = siqa_instance[\"answerC\"]\n","\n","    tokenized_input_dict = {\"A\": None, \"B\": None, \"C\": None}\n","\n","    cqaA = context + tokenizer.sep_token + question + tokenizer.sep_token + answerA\n","    cqaB = context + tokenizer.sep_token + question + tokenizer.sep_token + answerB\n","    cqaC = context + tokenizer.sep_token + question + tokenizer.sep_token + answerC\n","\n","    tokenized_input_dict[\"A\"] = tokenizer(cqaA, return_tensors=\"pt\")\n","    tokenized_input_dict[\"B\"] = tokenizer(cqaB, return_tensors=\"pt\")\n","    tokenized_input_dict[\"C\"] = tokenizer(cqaC, return_tensors=\"pt\")\n","\n","\n","    input_ids_dict = {key: tokenized_input_dict[key][\"input_ids\"].to(device) for key in tokenized_input_dict.keys()}\n","    attn_mask_dict = {key: tokenized_input_dict[key][\"attention_mask\"].to(device) for key in tokenized_input_dict.keys()}\n","\n","\n","    # Step 2: Feed the input_ids_dict and attn_mask_dict to the model and get the final predictions\n","    # (Don't forget torch.no_grad())\n","    pred_label = None\n","    # YOUR CODE HERE\n","    with torch.no_grad():\n","        pred_label = model(input_ids_dict, attn_mask_dict)\n","        pred_label = torch.argmax(pred_label, dim=1).item()\n","\n","    # Step 3: Make the predicted human readable i.e. convert 0 to A, 1 to B and 2 to C\n","    pred_label_hr = None\n","    # YOUR CODE HERE\n","    mapping = {0: \"A\", 1: \"B\", 2: \"C\"}\n","    pred_label_hr = mapping[pred_label]\n","\n","    return pred_label_hr"]},{"cell_type":"code","execution_count":null,"id":"b79990de","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"4e2ea8703c05b6e64a2fd3574ac83692","grade":false,"grade_id":"cell-2a404905599a658d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b79990de","colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["2cf3f65bffaa4c4b8511270c99e3e6d2","073470176cd144b7ae7a9c3936354565","bf5984737d924467aed74260d49ff52b","bcba451548154fc5a2b06c9a964cd0c6","4c0d2e1ad749481abb30b56eb5987edc","f5fa654a7f3b48b3b9f271e89359d683","0a7c7a7c12664b4ba2f7e28a9b27f6b6","e2d1b67dc79e437c9093a6f152a099bd","bed15f0c6cad473f8f41b008ac9afb56","7b1abd62bfa74b0e9e43b42ef3739140","72269ca19657419aaf96910b461ac691"]},"outputId":"56ec919d-b60b-4c8f-8638-147360fabaf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1954 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf3f65bffaa4c4b8511270c99e3e6d2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Accuracy by calling `predict_text`: 0.6074718526100307\n","Expected Accuracy: 0.6102642276422764\n","Test Case Passed! :)\n","******************************\n","\n"]}],"source":["print(\"Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\")\n","preds = [\n","    predict_text(siqa_instance, best_model, bert_tokenizer, device = \"cuda\")\n","    for siqa_instance in tqdm(dev_data)\n","]\n","test_case_accuracy = (np.array(preds) == np.array(dev_labels)).mean()\n","print(f\"Accuracy by calling `predict_text`: {test_case_accuracy}\")\n","print(f\"Expected Accuracy: {best_val_acc}\")\n","\n","assert np.allclose(test_case_accuracy, best_val_acc, 1e-2)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")"]},{"cell_type":"code","execution_count":null,"id":"7c969000","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5236adbcc6dc50b20fd6f56c133a3ee0","grade":true,"grade_id":"cell-00c5243a8d59f404","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"7c969000","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dab9e2ba-cfd1-4b18-8661-700b8aabcb7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["{   'context': \"Tracy didn't go home that evening and resisted Riley's \"\n","               'attacks.',\n","    'question': 'What does Tracy need to do before this?',\n","    'answerA': 'make a new plan',\n","    'answerB': 'Go home and see Riley',\n","    'answerC': 'Find somewhere to go'}\n","Predicted Label: C\n","Gold Label: C\n","**********************************\n","\n","{   'context': 'Robin left food out for the animals in her backyard to come '\n","               'and enjoy.',\n","    'question': 'What will Robin want to do next?',\n","    'answerA': 'chase the animals away',\n","    'answerB': 'watch the animals eat',\n","    'answerC': 'go out in the backyard'}\n","Predicted Label: B\n","Gold Label: B\n","**********************************\n","\n","{   'context': 'As usual, Aubrey went to the park but this morning, he met a '\n","               'stranger at the park who jogged with him.',\n","    'question': 'What will Others want to do next?',\n","    'answerA': 'win against Aubrey',\n","    'answerB': 'have a nice jog',\n","    'answerC': 'go to eat'}\n","Predicted Label: B\n","Gold Label: B\n","**********************************\n","\n"]}],"source":["idx = 0\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","print(\"**********************************\\n\")\n","\n","idx = 100\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","print(\"**********************************\\n\")\n","\n","\n","idx = 200\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","\n","print(\"**********************************\\n\")"]},{"cell_type":"code","execution_count":null,"id":"5ef516c5","metadata":{"id":"5ef516c5"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5524eb95b9aa41ecb89d492015b94b6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10c8f4658c62444c93f016a41a7d12d3","IPY_MODEL_99e375f78e56458eb4408c24ccd13638","IPY_MODEL_4b3a02382f514b6fb3719e38e86fa6ae"],"layout":"IPY_MODEL_a968f3539bdf4f6e9b08d904068f139c"}},"10c8f4658c62444c93f016a41a7d12d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cc0c67d0259471e8861d3b02be04706","placeholder":"â€‹","style":"IPY_MODEL_92fab9190f4c425e850a96367ff284dc","value":"tokenizer_config.json:â€‡100%"}},"99e375f78e56458eb4408c24ccd13638":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d26ce4149f794600b67fd72c97ec5730","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f519fb508154a19bd269a62898fca79","value":48}},"4b3a02382f514b6fb3719e38e86fa6ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42ef31ce7244430d8e07c16e858ab0db","placeholder":"â€‹","style":"IPY_MODEL_72873faab0494705818692250ed5a80a","value":"â€‡48.0/48.0â€‡[00:00&lt;00:00,â€‡2.30kB/s]"}},"a968f3539bdf4f6e9b08d904068f139c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cc0c67d0259471e8861d3b02be04706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92fab9190f4c425e850a96367ff284dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d26ce4149f794600b67fd72c97ec5730":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f519fb508154a19bd269a62898fca79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42ef31ce7244430d8e07c16e858ab0db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72873faab0494705818692250ed5a80a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03402e6fc94f4980a998c94b1da88254":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2523e6214214e108b3e6cff5ebfa3f5","IPY_MODEL_e03bdf8dbd6d4598acc33078c5bb6424","IPY_MODEL_88aaf87c98da4a6686a7ed1edad3e4e2"],"layout":"IPY_MODEL_bb7e609b679b410697b3af3617b2a6ec"}},"f2523e6214214e108b3e6cff5ebfa3f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad006ea701f7441a9cc16682c6987147","placeholder":"â€‹","style":"IPY_MODEL_122fcf6f96324d77a73acbc0a6fc3cce","value":"vocab.txt:â€‡100%"}},"e03bdf8dbd6d4598acc33078c5bb6424":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_532886af2c644141b185fb55d4bcfe52","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14a140f56fd34f008b3662c8db44c90c","value":231508}},"88aaf87c98da4a6686a7ed1edad3e4e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd3e63c937da4e57bcc5c3e03b26cd4e","placeholder":"â€‹","style":"IPY_MODEL_89ecd1cb5d2440e79a85898c34ade2f0","value":"â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡2.07MB/s]"}},"bb7e609b679b410697b3af3617b2a6ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad006ea701f7441a9cc16682c6987147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"122fcf6f96324d77a73acbc0a6fc3cce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"532886af2c644141b185fb55d4bcfe52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14a140f56fd34f008b3662c8db44c90c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd3e63c937da4e57bcc5c3e03b26cd4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ecd1cb5d2440e79a85898c34ade2f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e34594c4aaa64044935ecf2a807a1a2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a80d7dc903d47ba9b2d51623166eafe","IPY_MODEL_a3ebe4648937466ebdced1126c4af567","IPY_MODEL_b5ae0ee0d9da48feb8e78071753956c7"],"layout":"IPY_MODEL_3a856ff7cf7e47508eb72ca1583f9b88"}},"5a80d7dc903d47ba9b2d51623166eafe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5075d03df46e425585e106ad50fbde90","placeholder":"â€‹","style":"IPY_MODEL_c563b1a12635423ba03170b54739bd29","value":"tokenizer.json:â€‡100%"}},"a3ebe4648937466ebdced1126c4af567":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c60027d42c8a4175a163c58c518afb9f","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a8be88108f648108c0fa5e715167d35","value":466062}},"b5ae0ee0d9da48feb8e78071753956c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8edf41106ec846cbb0596a3e3bccf3cc","placeholder":"â€‹","style":"IPY_MODEL_cfadc104db334d4891f0fe0bb33feba1","value":"â€‡466k/466kâ€‡[00:00&lt;00:00,â€‡2.71MB/s]"}},"3a856ff7cf7e47508eb72ca1583f9b88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5075d03df46e425585e106ad50fbde90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c563b1a12635423ba03170b54739bd29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c60027d42c8a4175a163c58c518afb9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a8be88108f648108c0fa5e715167d35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8edf41106ec846cbb0596a3e3bccf3cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfadc104db334d4891f0fe0bb33feba1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da3de40dd9ce452b872a7a6d2ede989e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b276266af2940b4a064cc95f6096062","IPY_MODEL_f952c79c9ceb4f8b880cc5e1d989c7cf","IPY_MODEL_58b7b597e4904269b83212e83a9971b6"],"layout":"IPY_MODEL_282fbecafeed41a99530dbad9efd4f2b"}},"3b276266af2940b4a064cc95f6096062":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f7aab27fe2340b0a6db8a2c1a7f78de","placeholder":"â€‹","style":"IPY_MODEL_08a228dd7b4e49738efcfe25458143bd","value":"config.json:â€‡100%"}},"f952c79c9ceb4f8b880cc5e1d989c7cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e56be8c8ab8a4975b9311922cc8d41e7","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d283e8008d3e45beaa99e1f4e2ce30e1","value":570}},"58b7b597e4904269b83212e83a9971b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c5aae6c1e694764b7caf2f81955c232","placeholder":"â€‹","style":"IPY_MODEL_0a60bb90de014cf9b6ebb42c97c61e8d","value":"â€‡570/570â€‡[00:00&lt;00:00,â€‡36.6kB/s]"}},"282fbecafeed41a99530dbad9efd4f2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f7aab27fe2340b0a6db8a2c1a7f78de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08a228dd7b4e49738efcfe25458143bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e56be8c8ab8a4975b9311922cc8d41e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d283e8008d3e45beaa99e1f4e2ce30e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c5aae6c1e694764b7caf2f81955c232":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a60bb90de014cf9b6ebb42c97c61e8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f30874aaa6942319220a30c2e33186e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f84e37c7d08e48088f212400c7c7ac17","IPY_MODEL_7620707e40ef43858e0b1c93b3e55ec5","IPY_MODEL_ca881374113b4f2896e3edab915f91f6"],"layout":"IPY_MODEL_c7781536b8d6407c8b5a13897d2b5a3e"}},"f84e37c7d08e48088f212400c7c7ac17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6737485405ab4adb97f6ecf152e42810","placeholder":"â€‹","style":"IPY_MODEL_d3f0c313fccf47aabd260057790e0a2f","value":"tokenizer_config.json:â€‡100%"}},"7620707e40ef43858e0b1c93b3e55ec5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c50db090dcb249ea9fe2eb1599d77b3d","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a8a8d9279bd473080b786f1e93dcd1f","value":49}},"ca881374113b4f2896e3edab915f91f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dab1f2446e142f8a58aebf2ddedc874","placeholder":"â€‹","style":"IPY_MODEL_ab244ffeaed14a56be4edbce1c945949","value":"â€‡49.0/49.0â€‡[00:00&lt;00:00,â€‡4.06kB/s]"}},"c7781536b8d6407c8b5a13897d2b5a3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6737485405ab4adb97f6ecf152e42810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3f0c313fccf47aabd260057790e0a2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c50db090dcb249ea9fe2eb1599d77b3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a8a8d9279bd473080b786f1e93dcd1f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2dab1f2446e142f8a58aebf2ddedc874":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab244ffeaed14a56be4edbce1c945949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb0b6ae457234ac1b2f4995dd3ceb9ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d789df449461469abf2b77420aad6dbb","IPY_MODEL_d0858acc88654cd0a4948fb07717e64c","IPY_MODEL_378868b731db41eebc46435264ff7a02"],"layout":"IPY_MODEL_a7ed509313eb46cc983238c252af547e"}},"d789df449461469abf2b77420aad6dbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10308e7f0cf945d7aafebba0e3cd3c6b","placeholder":"â€‹","style":"IPY_MODEL_491f99340bbb4a828626276f6d8fc296","value":"vocab.txt:â€‡100%"}},"d0858acc88654cd0a4948fb07717e64c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d14aa5a98b84c51a5fd6892627ad160","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef953583191745c1aa38bcb519b0a09a","value":213450}},"378868b731db41eebc46435264ff7a02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a822f6faa0ef4740b3ae234bbc6d937b","placeholder":"â€‹","style":"IPY_MODEL_ea5c5ea247d746039bc558c03858b595","value":"â€‡213k/213kâ€‡[00:00&lt;00:00,â€‡3.66MB/s]"}},"a7ed509313eb46cc983238c252af547e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10308e7f0cf945d7aafebba0e3cd3c6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"491f99340bbb4a828626276f6d8fc296":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d14aa5a98b84c51a5fd6892627ad160":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef953583191745c1aa38bcb519b0a09a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a822f6faa0ef4740b3ae234bbc6d937b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea5c5ea247d746039bc558c03858b595":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56cd7cb15fb240c9af2b32a138c43d0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64021529311a475ba7c0e40044dff8ac","IPY_MODEL_d1e89e4b03d145118c088d45943c3cd2","IPY_MODEL_b7e2ab564f534e959c8c098a15fe1549"],"layout":"IPY_MODEL_802ad382721d4d05ab52c86e2588c153"}},"64021529311a475ba7c0e40044dff8ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9302baabddd143adbded88fbc7baa92c","placeholder":"â€‹","style":"IPY_MODEL_de51a9c1ea864bfbaf2c4a74619d0d57","value":"tokenizer.json:â€‡100%"}},"d1e89e4b03d145118c088d45943c3cd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c9861d208f04ab0879abd24eca92b3d","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e912b8f8298048798d8b6b764ce1be08","value":435797}},"b7e2ab564f534e959c8c098a15fe1549":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55b12731c6b240f58823cbf124d095de","placeholder":"â€‹","style":"IPY_MODEL_71a7074524874e3f8d0b208ed007b5c1","value":"â€‡436k/436kâ€‡[00:00&lt;00:00,â€‡2.56MB/s]"}},"802ad382721d4d05ab52c86e2588c153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9302baabddd143adbded88fbc7baa92c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de51a9c1ea864bfbaf2c4a74619d0d57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c9861d208f04ab0879abd24eca92b3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e912b8f8298048798d8b6b764ce1be08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55b12731c6b240f58823cbf124d095de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71a7074524874e3f8d0b208ed007b5c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90deb860c51c4915b019ab6e33b95e18":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e2ad4416ece476284259646eb55cb81","IPY_MODEL_b07044bf05e8409cb483c0e4c3a860ae","IPY_MODEL_fe3580db28c84353b8d30af618bb04d0"],"layout":"IPY_MODEL_89bdf4d2134345fda0935f170ac7b4d7"}},"6e2ad4416ece476284259646eb55cb81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceba11a4889c46bf92ed4788d3f086d2","placeholder":"â€‹","style":"IPY_MODEL_0b3dbb7640be433aa09062a12701ad5c","value":"config.json:â€‡100%"}},"b07044bf05e8409cb483c0e4c3a860ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0da754374ff4f1887d8f4fea680245b","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_984d8923007840599e50ba9e8a5802f6","value":570}},"fe3580db28c84353b8d30af618bb04d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59b0af5c251146a5ad759726b1016717","placeholder":"â€‹","style":"IPY_MODEL_325a1bde9fe44a4e8accf2763d653648","value":"â€‡570/570â€‡[00:00&lt;00:00,â€‡30.8kB/s]"}},"89bdf4d2134345fda0935f170ac7b4d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceba11a4889c46bf92ed4788d3f086d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b3dbb7640be433aa09062a12701ad5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0da754374ff4f1887d8f4fea680245b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"984d8923007840599e50ba9e8a5802f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59b0af5c251146a5ad759726b1016717":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"325a1bde9fe44a4e8accf2763d653648":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d6e695936b14c2eb56a9d6d3810c92e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25e1fd955cea4ae8b32d4da6421af3f9","IPY_MODEL_fd35e7e6e95041888eca1feb43d9a403","IPY_MODEL_6ea8584d164742a5b69aad5c9a6a06ea"],"layout":"IPY_MODEL_bef3a48f2b5f4805940d7e69575f8847"}},"25e1fd955cea4ae8b32d4da6421af3f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1544a6a865d4ca0905c49b697e80f9b","placeholder":"â€‹","style":"IPY_MODEL_91b65a8139f8432b9d4f00555646820e","value":"model.safetensors:â€‡100%"}},"fd35e7e6e95041888eca1feb43d9a403":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_722c87cd167e4479bfba9fdcefaddf9a","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5baa581239164df9bdba9fd4c4ea2673","value":440449768}},"6ea8584d164742a5b69aad5c9a6a06ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc47abbfd1de4770883a1aea21c97f22","placeholder":"â€‹","style":"IPY_MODEL_6ad8a2b91f9745daaf4a328bb58884fa","value":"â€‡440M/440Mâ€‡[00:03&lt;00:00,â€‡139MB/s]"}},"bef3a48f2b5f4805940d7e69575f8847":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1544a6a865d4ca0905c49b697e80f9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91b65a8139f8432b9d4f00555646820e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"722c87cd167e4479bfba9fdcefaddf9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5baa581239164df9bdba9fd4c4ea2673":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc47abbfd1de4770883a1aea21c97f22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ad8a2b91f9745daaf4a328bb58884fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b327d02baf7f42fcaed457593e29d0ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a02aab0fbfe94f2f8e4e89c5f29c7788","IPY_MODEL_4901ad1caca54fc39d9a66094deb0f7e","IPY_MODEL_49bf0bb4bd824f24b9a65d9c948001e3"],"layout":"IPY_MODEL_fa1fd1c13a1d4696bf1ac1042812d69e"}},"a02aab0fbfe94f2f8e4e89c5f29c7788":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b83455666f74b279074af648517e728","placeholder":"â€‹","style":"IPY_MODEL_13bdbaefc174461cb77ff7e479c8aa4c","value":"100%"}},"4901ad1caca54fc39d9a66094deb0f7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ed15983d39c44cb873894b3dac9eb4b","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e257b0b885f048f0bda6fd748e1b3e40","value":25}},"49bf0bb4bd824f24b9a65d9c948001e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8eb5731eb2ad48b7947786a807e05178","placeholder":"â€‹","style":"IPY_MODEL_15b462fb7be64a1c8bd99fb10988aa22","value":"â€‡25/25â€‡[00:05&lt;00:00,â€‡â€‡6.41it/s]"}},"fa1fd1c13a1d4696bf1ac1042812d69e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b83455666f74b279074af648517e728":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13bdbaefc174461cb77ff7e479c8aa4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ed15983d39c44cb873894b3dac9eb4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e257b0b885f048f0bda6fd748e1b3e40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8eb5731eb2ad48b7947786a807e05178":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15b462fb7be64a1c8bd99fb10988aa22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a91243a797d241fc8c0d695e45224c5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c883df886b5044c0bfa4d933f6192d96","IPY_MODEL_e0c248567e0146f2979f97ca0f64ce59","IPY_MODEL_1791a7a9e413460b85c3de34aca46fb2"],"layout":"IPY_MODEL_c5f96fea005f4c33a2b2d64eb8168f88"}},"c883df886b5044c0bfa4d933f6192d96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b508f624716242d39884adb29e2a3cec","placeholder":"â€‹","style":"IPY_MODEL_c9ec716278d440a08313047a5bc221ca","value":"100%"}},"e0c248567e0146f2979f97ca0f64ce59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3ccc652089541bda03d027938f2662d","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc0a03dd40174b338d83d32251232b12","value":25}},"1791a7a9e413460b85c3de34aca46fb2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_210e26ff990f4cc08db2c9b9e42efba7","placeholder":"â€‹","style":"IPY_MODEL_702a87d037a4437fad307f2eeaa0ddfe","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.55it/s]"}},"c5f96fea005f4c33a2b2d64eb8168f88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b508f624716242d39884adb29e2a3cec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9ec716278d440a08313047a5bc221ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3ccc652089541bda03d027938f2662d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc0a03dd40174b338d83d32251232b12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"210e26ff990f4cc08db2c9b9e42efba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"702a87d037a4437fad307f2eeaa0ddfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1be91cbd7e9b411ca815c84711a036bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4505fb551f9e40e697214c6d6d6a98ad","IPY_MODEL_933c34a6a0874c378e0df0e464a2dc93","IPY_MODEL_ee793521388447b8b1781b8a5b13499b"],"layout":"IPY_MODEL_ccc98c91d8a048799d394a6641cbf919"}},"4505fb551f9e40e697214c6d6d6a98ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91f8699a6f0146089de3e582fb087f98","placeholder":"â€‹","style":"IPY_MODEL_08c27bd660a244fe92df6cd80b2a95f9","value":"100%"}},"933c34a6a0874c378e0df0e464a2dc93":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f7c92dec8c24094ada7c8474143e4fe","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91d2ece2de494f2884699ff3b18947ea","value":25}},"ee793521388447b8b1781b8a5b13499b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_beba8c7115f94819a29130de39ac83e7","placeholder":"â€‹","style":"IPY_MODEL_e6dd2b31fca44995973a3a855e533704","value":"â€‡25/25â€‡[00:04&lt;00:00,â€‡â€‡6.33it/s]"}},"ccc98c91d8a048799d394a6641cbf919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91f8699a6f0146089de3e582fb087f98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c27bd660a244fe92df6cd80b2a95f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f7c92dec8c24094ada7c8474143e4fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d2ece2de494f2884699ff3b18947ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"beba8c7115f94819a29130de39ac83e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6dd2b31fca44995973a3a855e533704":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"646b36a6ac2f46579cc389ceeacb67dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58013b573e174c9889a630ab4ffd8056","IPY_MODEL_1d38a8b14ebe4fe9b296dbcbfb60d3ca","IPY_MODEL_23f31cd7e18347debfd8837e78fb4a31"],"layout":"IPY_MODEL_466aa311d985481282d2d9153b7773ae"}},"58013b573e174c9889a630ab4ffd8056":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a94f44e2ede34f6cae0312cc6b99c021","placeholder":"â€‹","style":"IPY_MODEL_f8450f84e90f4a238aa023bca10a5d88","value":"100%"}},"1d38a8b14ebe4fe9b296dbcbfb60d3ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7b42923d904486ebe69c70a6eb47fe6","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a3b072f786c74b06898dce95d8fafea8","value":25}},"23f31cd7e18347debfd8837e78fb4a31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e347aeea0a5a4072902e2370cc6b4754","placeholder":"â€‹","style":"IPY_MODEL_8fc38796e3744f6380925c739dc21392","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.40it/s]"}},"466aa311d985481282d2d9153b7773ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a94f44e2ede34f6cae0312cc6b99c021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8450f84e90f4a238aa023bca10a5d88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7b42923d904486ebe69c70a6eb47fe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3b072f786c74b06898dce95d8fafea8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e347aeea0a5a4072902e2370cc6b4754":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fc38796e3744f6380925c739dc21392":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46ecd1f4965a4d7d97705e20d1b02a88":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae76d724a1204a20a913684ad6fc7728","IPY_MODEL_6f81311c23cb4bd69b5d2ad08c45e7fd","IPY_MODEL_d00e18d781c04c67be0f750198a53153"],"layout":"IPY_MODEL_423d8c37b0d544cfbfb0ac5bd4c490b9"}},"ae76d724a1204a20a913684ad6fc7728":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c415332927894606aeca98c97f9c7adc","placeholder":"â€‹","style":"IPY_MODEL_f7de15280fe04b27bc844ed935fa020e","value":"100%"}},"6f81311c23cb4bd69b5d2ad08c45e7fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66e0effb96544572a5d4aefd571edfb0","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d77d85ffb7a4e10ae22bc0eb8937dcd","value":25}},"d00e18d781c04c67be0f750198a53153":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b32ff46982a4c8da44d37e18495b5f0","placeholder":"â€‹","style":"IPY_MODEL_741844aadd9b47b8ba86e75e92b8f76d","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.23it/s]"}},"423d8c37b0d544cfbfb0ac5bd4c490b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c415332927894606aeca98c97f9c7adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7de15280fe04b27bc844ed935fa020e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66e0effb96544572a5d4aefd571edfb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d77d85ffb7a4e10ae22bc0eb8937dcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b32ff46982a4c8da44d37e18495b5f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"741844aadd9b47b8ba86e75e92b8f76d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e0d8e989a6044a0b93fd167a3ffbbe3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0648004162d94abc843f7486854b392d","IPY_MODEL_2307d5cb66194b2c84205bf0ed962319","IPY_MODEL_e1bf3ced364a4c069fcddb562b50f503"],"layout":"IPY_MODEL_5f2e850d94b04af9919845917477b54e"}},"0648004162d94abc843f7486854b392d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87919c3a08f149a29eee1e98a6f87cf3","placeholder":"â€‹","style":"IPY_MODEL_f517455fe2af40d498d35237530a1eb4","value":"100%"}},"2307d5cb66194b2c84205bf0ed962319":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_250e76d381e5461db1228db2e1f91e9b","max":2089,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab42a13168924ccba8466fa0b498f3c0","value":2089}},"e1bf3ced364a4c069fcddb562b50f503":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c856586429174e4887288b2e6e63c782","placeholder":"â€‹","style":"IPY_MODEL_e8d4799ff405459488a66320a66f2474","value":"â€‡2089/2089â€‡[14:24&lt;00:00,â€‡â€‡3.20it/s]"}},"5f2e850d94b04af9919845917477b54e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87919c3a08f149a29eee1e98a6f87cf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f517455fe2af40d498d35237530a1eb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"250e76d381e5461db1228db2e1f91e9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab42a13168924ccba8466fa0b498f3c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c856586429174e4887288b2e6e63c782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8d4799ff405459488a66320a66f2474":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18d1752139da4f4bb996e31feebea3bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be563b9d6c434d06b9f6768a1652950c","IPY_MODEL_c85b995caae943ae8058f76afd80a0be","IPY_MODEL_d768a3406bd24e4492e6232f50eca3ac"],"layout":"IPY_MODEL_0150d1941b574223a43dccb8d368d79f"}},"be563b9d6c434d06b9f6768a1652950c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ba36643162c474f85eafaa9a02dc28d","placeholder":"â€‹","style":"IPY_MODEL_af9b9ecca17a430e8e8105b8fed8bf6e","value":"100%"}},"c85b995caae943ae8058f76afd80a0be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11ef94689d3048eebea859a16d4bd220","max":2089,"min":0,"orientation":"horizontal","style":"IPY_MODEL_776a5450ca77420797974f7688f6f97f","value":2089}},"d768a3406bd24e4492e6232f50eca3ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_600a53f6f06d47d89a88c4aa1253688e","placeholder":"â€‹","style":"IPY_MODEL_ef662e1ed5824ef2b0334d19181f7a4f","value":"â€‡2089/2089â€‡[14:21&lt;00:00,â€‡â€‡3.21it/s]"}},"0150d1941b574223a43dccb8d368d79f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ba36643162c474f85eafaa9a02dc28d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af9b9ecca17a430e8e8105b8fed8bf6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11ef94689d3048eebea859a16d4bd220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"776a5450ca77420797974f7688f6f97f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"600a53f6f06d47d89a88c4aa1253688e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef662e1ed5824ef2b0334d19181f7a4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cf3f65bffaa4c4b8511270c99e3e6d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_073470176cd144b7ae7a9c3936354565","IPY_MODEL_bf5984737d924467aed74260d49ff52b","IPY_MODEL_bcba451548154fc5a2b06c9a964cd0c6"],"layout":"IPY_MODEL_4c0d2e1ad749481abb30b56eb5987edc"}},"073470176cd144b7ae7a9c3936354565":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5fa654a7f3b48b3b9f271e89359d683","placeholder":"â€‹","style":"IPY_MODEL_0a7c7a7c12664b4ba2f7e28a9b27f6b6","value":"100%"}},"bf5984737d924467aed74260d49ff52b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2d1b67dc79e437c9093a6f152a099bd","max":1954,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bed15f0c6cad473f8f41b008ac9afb56","value":1954}},"bcba451548154fc5a2b06c9a964cd0c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b1abd62bfa74b0e9e43b42ef3739140","placeholder":"â€‹","style":"IPY_MODEL_72269ca19657419aaf96910b461ac691","value":"â€‡1954/1954â€‡[01:16&lt;00:00,â€‡18.13it/s]"}},"4c0d2e1ad749481abb30b56eb5987edc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5fa654a7f3b48b3b9f271e89359d683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a7c7a7c12664b4ba2f7e28a9b27f6b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2d1b67dc79e437c9093a6f152a099bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bed15f0c6cad473f8f41b008ac9afb56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b1abd62bfa74b0e9e43b42ef3739140":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72269ca19657419aaf96910b461ac691":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}